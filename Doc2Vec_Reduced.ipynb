{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxy\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec,LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zxy\\Anaconda3\\lib\\site-packages\\gensim-3.4.0-py3.6-win-amd64.egg\\gensim\\models\\doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.43783331,  2.06801486,  0.53648782,  0.43270174, -0.98655158,\n",
       "        1.65624988, -0.06916815, -0.73676074, -0.4503608 ,  0.32821459,\n",
       "        0.9090364 ,  0.23109597,  0.78079277, -0.40345684,  0.14013301,\n",
       "        0.65005356, -0.18434927,  0.58185804,  0.11211506, -1.18564868,\n",
       "       -0.41236639, -0.07018057, -0.92799979,  0.9965834 ,  0.53326535], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "\n",
    "mer=pickle.load(open('cc_merged_0429.pkl',\"rb\"))\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Load data\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(mer.txt):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 25, window = 10, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "arr=np.zeros((8434,25))\n",
    "\n",
    "for i in range(len(docs)):\n",
    "    arr[i]=model.docvecs[i]\n",
    "for i in range(1,26):\n",
    "    mer[str(i)]=None\n",
    "\n",
    "col=[str(x) for x in list(range(1,26))]\n",
    "\n",
    "mer[col]=pd.DataFrame(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mer.to_pickle('documentvec25.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAD0CAYAAABtjRZ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nogT2/wF0YAMZT6blPgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1def3469a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = model[model.wv.vocab]\n",
    "X=model[list(range(8434))]\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Read in data\n",
    "rf=pd.read_csv('bio_txt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'index',\n",
       " 'x_dem_dm',\n",
       " 'x_republican_dm',\n",
       " 'x_instate_ba_dm',\n",
       " 'x_elev_dm',\n",
       " 'x_unity_dm',\n",
       " 'x_aba_dm',\n",
       " 'x_crossa_dm',\n",
       " 'x_pfedjdge_dm',\n",
       " 'x_pindreg1_dm',\n",
       " 'x_plawprof_dm',\n",
       " 'x_pscab_dm',\n",
       " 'x_pcab_dm',\n",
       " 'x_pusa_dm',\n",
       " 'x_pssenate_dm',\n",
       " 'x_paag_dm',\n",
       " 'x_psp_dm',\n",
       " 'x_pslc_dm',\n",
       " 'x_pssc_dm',\n",
       " 'x_pshouse_dm',\n",
       " 'x_psg_dm',\n",
       " 'x_psgo_dm',\n",
       " 'x_psenate_dm',\n",
       " 'x_psatty_dm',\n",
       " 'x_pprivate_dm',\n",
       " 'x_pmayor_dm',\n",
       " 'x_plocct_dm',\n",
       " 'x_phouse_dm',\n",
       " 'x_pgov_dm',\n",
       " 'x_pda_dm',\n",
       " 'x_pcc_dm',\n",
       " 'x_pccoun_dm',\n",
       " 'x_pausa_dm',\n",
       " 'x_pasatty_dm',\n",
       " 'x_pag_dm',\n",
       " 'x_pada_dm',\n",
       " 'x_pgovt_dm',\n",
       " 'x_llm_sjd_dm',\n",
       " 'x_protestant_dm',\n",
       " 'x_evangelical_dm',\n",
       " 'x_mainline_dm',\n",
       " 'x_noreligion_dm',\n",
       " 'x_catholic_dm',\n",
       " 'x_jewish_dm',\n",
       " 'x_black_dm',\n",
       " 'x_nonwhite_dm',\n",
       " 'x_female_dm',\n",
       " 'x_jd_public_dm',\n",
       " 'x_ba_public_dm',\n",
       " 'x_b10s_dm',\n",
       " 'x_b20s_dm',\n",
       " 'x_b30s_dm',\n",
       " 'x_b40s_dm',\n",
       " 'x_b50s_dm',\n",
       " 'x_pbank_dm',\n",
       " 'x_pmag_dm',\n",
       " 'x_ageon40s_dm',\n",
       " 'x_ageon50s_dm',\n",
       " 'x_ageon60s_dm',\n",
       " 'x_ageon40orless_dm',\n",
       " 'x_ageon70ormore_dm',\n",
       " 'x_pago_dm',\n",
       " 'x_apptoter_dm',\n",
       " 'x_term_dm',\n",
       " 'x_circuit_dm',\n",
       " 'x_hdem_dm',\n",
       " 'x_hrep_dm',\n",
       " 'x_sdem_dm',\n",
       " 'x_srep_dm',\n",
       " 'x_hother_dm',\n",
       " 'x_sother_dm',\n",
       " 'x_agecommi_dm',\n",
       " 'res',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '10',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " '21',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " 'length_3m_dif',\n",
       " 'Res_binary',\n",
       " '0_hat',\n",
       " '1_hat',\n",
       " '2_hat',\n",
       " '3_hat',\n",
       " '4_hat',\n",
       " '5_hat',\n",
       " '6_hat',\n",
       " '7_hat',\n",
       " '8_hat',\n",
       " '9_hat',\n",
       " '10_hat',\n",
       " '11_hat',\n",
       " '12_hat',\n",
       " '13_hat',\n",
       " '14_hat',\n",
       " '15_hat',\n",
       " '16_hat',\n",
       " '17_hat',\n",
       " '18_hat',\n",
       " '19_hat',\n",
       " '20_hat',\n",
       " '21_hat',\n",
       " '22_hat',\n",
       " '23_hat',\n",
       " '24_hat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Select predictor variable and target variable\n",
    "cols=[col for col in rf if col.startswith('x_')]\n",
    "X=rf[cols]\n",
    "Y=rf.length_3m_dif\n",
    "\n",
    "##Rescale to Normalize\n",
    "n_X=(X-X.min())/(X.max()-X.min())\n",
    "n_y=(Y-Y.min())/(Y.max()-Y.min())\n",
    "\n",
    "##Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(n_X, n_y, test_size=0.25, random_state=42)\n",
    "\n",
    "##Add a Bias Variable\n",
    "X_train=np.hstack((X_train,np.ones((X_train.shape[0],1))))\n",
    "X_test=np.hstack((X_test,np.ones((X_test.shape[0],1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "x_train = torch.from_numpy(np.nan_to_num(X_train))\n",
    "y_train = torch.from_numpy(y_train.values)\n",
    "x_test = torch.from_numpy(np.nan_to_num(X_test))\n",
    "y_test = torch.from_numpy(y_test.values)\n",
    "x_test = Variable(x_test.float())\n",
    "y_test = Variable(y_test.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden=torch.nn.Linear(n_features,n_hidden) \n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.predict=torch.nn.Linear(n_hidden,1) \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.hidden(x)) \n",
    "        x=self.hidden2(x)\n",
    "        x=self.predict(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net(72, 50, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.01)\n",
    "loss_func=torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "dataset_train = Data.TensorDataset(data_tensor = x_train, target_tensor = y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = dataset_train,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Step:  0  training loss:  0.8170713186264038\n",
      "Epoch: 0  Step:  1  training loss:  0.7293936014175415\n",
      "Epoch: 0  Step:  2  training loss:  0.6572001576423645\n",
      "Epoch: 0  Step:  3  training loss:  0.578026533126831\n",
      "Epoch: 0  Step:  4  training loss:  0.5130570530891418\n",
      "Epoch: 0  Step:  5  training loss:  0.45738497376441956\n",
      "Epoch: 0  Step:  6  training loss:  0.40658116340637207\n",
      "Epoch: 0  Step:  7  training loss:  0.3501603305339813\n",
      "Epoch: 0  Step:  8  training loss:  0.3137766122817993\n",
      "Epoch: 0  Step:  9  training loss:  0.2848190665245056\n",
      "Epoch: 0  Step:  10  training loss:  0.24920740723609924\n",
      "Epoch: 0  Step:  11  training loss:  0.21098659932613373\n",
      "Epoch: 0  Step:  12  training loss:  0.1983512043952942\n",
      "Epoch: 0  Step:  13  training loss:  0.16883903741836548\n",
      "Epoch: 0  Step:  14  training loss:  0.1468013972043991\n",
      "Epoch: 0  Step:  15  training loss:  0.12663093209266663\n",
      "Epoch: 0  Step:  16  training loss:  0.11510252207517624\n",
      "Epoch: 0  Step:  17  training loss:  0.09994591027498245\n",
      "Epoch: 0  Step:  18  training loss:  0.08125774562358856\n",
      "Epoch: 0  Step:  19  training loss:  0.07464675605297089\n",
      "Epoch: 0  Step:  20  training loss:  0.06004960462450981\n",
      "Epoch: 0  Step:  21  training loss:  0.05457450821995735\n",
      "Epoch: 0  Step:  22  training loss:  0.04434874653816223\n",
      "Epoch: 0  Step:  23  training loss:  0.042604923248291016\n",
      "Epoch: 0  Step:  24  training loss:  0.03330984339118004\n",
      "Epoch: 0  Step:  25  training loss:  0.02768365852534771\n",
      "Epoch: 0  Step:  26  training loss:  0.0251042228192091\n",
      "Epoch: 0  Step:  27  training loss:  0.021618904545903206\n",
      "Epoch: 0  Step:  28  training loss:  0.019498692825436592\n",
      "Epoch: 0  Step:  29  training loss:  0.020027460530400276\n",
      "Epoch: 0  Step:  30  training loss:  0.016302375122904778\n",
      "Epoch: 0  Step:  31  training loss:  0.018303321674466133\n",
      "Epoch: 0  Step:  32  training loss:  0.012052017264068127\n",
      "Epoch: 0  Step:  33  training loss:  0.008830680511891842\n",
      "Epoch: 0  Step:  34  training loss:  0.0086815832182765\n",
      "Epoch: 0  Step:  35  training loss:  0.006034272722899914\n",
      "Epoch: 0  Step:  36  training loss:  0.005493823438882828\n",
      "Epoch: 0  Step:  37  training loss:  0.005838177166879177\n",
      "Epoch: 0  Step:  38  training loss:  0.005294236354529858\n",
      "Epoch: 0  Step:  39  training loss:  0.00420968234539032\n",
      "Epoch: 0  Step:  40  training loss:  0.00664232112467289\n",
      "Epoch: 0  Step:  41  training loss:  0.004553052596747875\n",
      "Epoch: 0  Step:  42  training loss:  0.0035171217750757933\n",
      "Epoch: 0  Step:  43  training loss:  0.00356478663161397\n",
      "Epoch: 0  Step:  44  training loss:  0.004152500536292791\n",
      "Epoch: 0  Step:  45  training loss:  0.004181393887847662\n",
      "Epoch: 0  Step:  46  training loss:  0.0029647990595549345\n",
      "Epoch: 0  Step:  47  training loss:  0.003553061280399561\n",
      "Epoch: 0  Step:  48  training loss:  0.002828446216881275\n",
      "Epoch: 0  Step:  49  training loss:  0.0026537799276411533\n",
      "Epoch: 0  Step:  50  training loss:  0.0031388106290251017\n",
      "Epoch: 0  Step:  51  training loss:  0.0025882062036544085\n",
      "Epoch: 0  Step:  52  training loss:  0.002828839235007763\n",
      "Epoch: 0  Step:  53  training loss:  0.002190411789342761\n",
      "Epoch: 0  Step:  54  training loss:  0.003756193909794092\n",
      "Epoch: 0  Step:  55  training loss:  0.0030181207694113255\n",
      "Epoch: 1  Step:  0  training loss:  0.0029018851928412914\n",
      "Epoch: 1  Step:  1  training loss:  0.0026019953656941652\n",
      "Epoch: 1  Step:  2  training loss:  0.0034171841107308865\n",
      "Epoch: 1  Step:  3  training loss:  0.002167403930798173\n",
      "Epoch: 1  Step:  4  training loss:  0.0019072911236435175\n",
      "Epoch: 1  Step:  5  training loss:  0.002299079205840826\n",
      "Epoch: 1  Step:  6  training loss:  0.005073206033557653\n",
      "Epoch: 1  Step:  7  training loss:  0.0021610958501696587\n",
      "Epoch: 1  Step:  8  training loss:  0.0033881976269185543\n",
      "Epoch: 1  Step:  9  training loss:  0.00294741103425622\n",
      "Epoch: 1  Step:  10  training loss:  0.0033294609747827053\n",
      "Epoch: 1  Step:  11  training loss:  0.0018051635706797242\n",
      "Epoch: 1  Step:  12  training loss:  0.002502934541553259\n",
      "Epoch: 1  Step:  13  training loss:  0.002477093134075403\n",
      "Epoch: 1  Step:  14  training loss:  0.0020289525855332613\n",
      "Epoch: 1  Step:  15  training loss:  0.0033590912353247404\n",
      "Epoch: 1  Step:  16  training loss:  0.0025496629532426596\n",
      "Epoch: 1  Step:  17  training loss:  0.0023490453604608774\n",
      "Epoch: 1  Step:  18  training loss:  0.0032421655487269163\n",
      "Epoch: 1  Step:  19  training loss:  0.001977213192731142\n",
      "Epoch: 1  Step:  20  training loss:  0.002705387072637677\n",
      "Epoch: 1  Step:  21  training loss:  0.003092660801485181\n",
      "Epoch: 1  Step:  22  training loss:  0.0024662003852427006\n",
      "Epoch: 1  Step:  23  training loss:  0.008530955761671066\n",
      "Epoch: 1  Step:  24  training loss:  0.002719615353271365\n",
      "Epoch: 1  Step:  25  training loss:  0.0020350199192762375\n",
      "Epoch: 1  Step:  26  training loss:  0.003570257918909192\n",
      "Epoch: 1  Step:  27  training loss:  0.0030818399973213673\n",
      "Epoch: 1  Step:  28  training loss:  0.002295928541570902\n",
      "Epoch: 1  Step:  29  training loss:  0.0024332155007869005\n",
      "Epoch: 1  Step:  30  training loss:  0.0027667037211358547\n",
      "Epoch: 1  Step:  31  training loss:  0.003029165556654334\n",
      "Epoch: 1  Step:  32  training loss:  0.005916156340390444\n",
      "Epoch: 1  Step:  33  training loss:  0.002526593394577503\n",
      "Epoch: 1  Step:  34  training loss:  0.0028469448443502188\n",
      "Epoch: 1  Step:  35  training loss:  0.0034235981293022633\n",
      "Epoch: 1  Step:  36  training loss:  0.00268648867495358\n",
      "Epoch: 1  Step:  37  training loss:  0.0036525516770780087\n",
      "Epoch: 1  Step:  38  training loss:  0.003238269593566656\n",
      "Epoch: 1  Step:  39  training loss:  0.0028076311573386192\n",
      "Epoch: 1  Step:  40  training loss:  0.0029897114727646112\n",
      "Epoch: 1  Step:  41  training loss:  0.002548519754782319\n",
      "Epoch: 1  Step:  42  training loss:  0.0019768001511693\n",
      "Epoch: 1  Step:  43  training loss:  0.0030534344259649515\n",
      "Epoch: 1  Step:  44  training loss:  0.0031229909509420395\n",
      "Epoch: 1  Step:  45  training loss:  0.0025311452336609364\n",
      "Epoch: 1  Step:  46  training loss:  0.0033094000536948442\n",
      "Epoch: 1  Step:  47  training loss:  0.0026346181984990835\n",
      "Epoch: 1  Step:  48  training loss:  0.002358395606279373\n",
      "Epoch: 1  Step:  49  training loss:  0.002898584119975567\n",
      "Epoch: 1  Step:  50  training loss:  0.0019526082323864102\n",
      "Epoch: 1  Step:  51  training loss:  0.002186813158914447\n",
      "Epoch: 1  Step:  52  training loss:  0.002700928132981062\n",
      "Epoch: 1  Step:  53  training loss:  0.0031226021237671375\n",
      "Epoch: 1  Step:  54  training loss:  0.0032627873588353395\n",
      "Epoch: 1  Step:  55  training loss:  0.0018260262440890074\n",
      "Epoch: 2  Step:  0  training loss:  0.0027456434909254313\n",
      "Epoch: 2  Step:  1  training loss:  0.0037446466740220785\n",
      "Epoch: 2  Step:  2  training loss:  0.0030173903796821833\n",
      "Epoch: 2  Step:  3  training loss:  0.002446824684739113\n",
      "Epoch: 2  Step:  4  training loss:  0.0021455136593431234\n",
      "Epoch: 2  Step:  5  training loss:  0.00339692085981369\n",
      "Epoch: 2  Step:  6  training loss:  0.0027472274377942085\n",
      "Epoch: 2  Step:  7  training loss:  0.0029710999224334955\n",
      "Epoch: 2  Step:  8  training loss:  0.003164333291351795\n",
      "Epoch: 2  Step:  9  training loss:  0.00260922615416348\n",
      "Epoch: 2  Step:  10  training loss:  0.0026027641724795103\n",
      "Epoch: 2  Step:  11  training loss:  0.0030067001935094595\n",
      "Epoch: 2  Step:  12  training loss:  0.0022823414765298367\n",
      "Epoch: 2  Step:  13  training loss:  0.00275636138394475\n",
      "Epoch: 2  Step:  14  training loss:  0.0030424147844314575\n",
      "Epoch: 2  Step:  15  training loss:  0.002946834545582533\n",
      "Epoch: 2  Step:  16  training loss:  0.0017815340543165803\n",
      "Epoch: 2  Step:  17  training loss:  0.0032220245338976383\n",
      "Epoch: 2  Step:  18  training loss:  0.0026741891633719206\n",
      "Epoch: 2  Step:  19  training loss:  0.0026398205664008856\n",
      "Epoch: 2  Step:  20  training loss:  0.002914492739364505\n",
      "Epoch: 2  Step:  21  training loss:  0.002391452668234706\n",
      "Epoch: 2  Step:  22  training loss:  0.0025856178253889084\n",
      "Epoch: 2  Step:  23  training loss:  0.0025119183119386435\n",
      "Epoch: 2  Step:  24  training loss:  0.003314074594527483\n",
      "Epoch: 2  Step:  25  training loss:  0.0024214000441133976\n",
      "Epoch: 2  Step:  26  training loss:  0.003772472031414509\n",
      "Epoch: 2  Step:  27  training loss:  0.0023272354155778885\n",
      "Epoch: 2  Step:  28  training loss:  0.0019887348171323538\n",
      "Epoch: 2  Step:  29  training loss:  0.006037058774381876\n",
      "Epoch: 2  Step:  30  training loss:  0.0022293939255177975\n",
      "Epoch: 2  Step:  31  training loss:  0.0019620510283857584\n",
      "Epoch: 2  Step:  32  training loss:  0.00252744322642684\n",
      "Epoch: 2  Step:  33  training loss:  0.0029871680308133364\n",
      "Epoch: 2  Step:  34  training loss:  0.0032482165843248367\n",
      "Epoch: 2  Step:  35  training loss:  0.002873916644603014\n",
      "Epoch: 2  Step:  36  training loss:  0.0018225839594379067\n",
      "Epoch: 2  Step:  37  training loss:  0.002085617044940591\n",
      "Epoch: 2  Step:  38  training loss:  0.008924189023673534\n",
      "Epoch: 2  Step:  39  training loss:  0.002724984660744667\n",
      "Epoch: 2  Step:  40  training loss:  0.0028759485576301813\n",
      "Epoch: 2  Step:  41  training loss:  0.0023116017691791058\n",
      "Epoch: 2  Step:  42  training loss:  0.003004897153005004\n",
      "Epoch: 2  Step:  43  training loss:  0.002453627996146679\n",
      "Epoch: 2  Step:  44  training loss:  0.0022596961352974176\n",
      "Epoch: 2  Step:  45  training loss:  0.002902654465287924\n",
      "Epoch: 2  Step:  46  training loss:  0.0024252748116850853\n",
      "Epoch: 2  Step:  47  training loss:  0.002060175174847245\n",
      "Epoch: 2  Step:  48  training loss:  0.005816209129989147\n",
      "Epoch: 2  Step:  49  training loss:  0.002653315430507064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  Step:  50  training loss:  0.0019039011094719172\n",
      "Epoch: 2  Step:  51  training loss:  0.0019283993169665337\n",
      "Epoch: 2  Step:  52  training loss:  0.0036831723991781473\n",
      "Epoch: 2  Step:  53  training loss:  0.0019669223111122847\n",
      "Epoch: 2  Step:  54  training loss:  0.0038354701828211546\n",
      "Epoch: 2  Step:  55  training loss:  0.0025010849349200726\n",
      "Epoch: 3  Step:  0  training loss:  0.002339285798370838\n",
      "Epoch: 3  Step:  1  training loss:  0.002536910818889737\n",
      "Epoch: 3  Step:  2  training loss:  0.002116608899086714\n",
      "Epoch: 3  Step:  3  training loss:  0.002417808398604393\n",
      "Epoch: 3  Step:  4  training loss:  0.0023977227974683046\n",
      "Epoch: 3  Step:  5  training loss:  0.0030602100305259228\n",
      "Epoch: 3  Step:  6  training loss:  0.002695603994652629\n",
      "Epoch: 3  Step:  7  training loss:  0.002660982543602586\n",
      "Epoch: 3  Step:  8  training loss:  0.009179593063890934\n",
      "Epoch: 3  Step:  9  training loss:  0.0026477212086319923\n",
      "Epoch: 3  Step:  10  training loss:  0.0021847859025001526\n",
      "Epoch: 3  Step:  11  training loss:  0.002206539735198021\n",
      "Epoch: 3  Step:  12  training loss:  0.003922978416085243\n",
      "Epoch: 3  Step:  13  training loss:  0.00358340865932405\n",
      "Epoch: 3  Step:  14  training loss:  0.002072571776807308\n",
      "Epoch: 3  Step:  15  training loss:  0.0028240508399903774\n",
      "Epoch: 3  Step:  16  training loss:  0.002277946565300226\n",
      "Epoch: 3  Step:  17  training loss:  0.003318224800750613\n",
      "Epoch: 3  Step:  18  training loss:  0.002310494426637888\n",
      "Epoch: 3  Step:  19  training loss:  0.0033683490473777056\n",
      "Epoch: 3  Step:  20  training loss:  0.0022231333423405886\n",
      "Epoch: 3  Step:  21  training loss:  0.005617556162178516\n",
      "Epoch: 3  Step:  22  training loss:  0.002784643555060029\n",
      "Epoch: 3  Step:  23  training loss:  0.002600808162242174\n",
      "Epoch: 3  Step:  24  training loss:  0.0031390953809022903\n",
      "Epoch: 3  Step:  25  training loss:  0.002649936592206359\n",
      "Epoch: 3  Step:  26  training loss:  0.0020068292506039143\n",
      "Epoch: 3  Step:  27  training loss:  0.003877979004755616\n",
      "Epoch: 3  Step:  28  training loss:  0.002421972341835499\n",
      "Epoch: 3  Step:  29  training loss:  0.00251921359449625\n",
      "Epoch: 3  Step:  30  training loss:  0.001849754131399095\n",
      "Epoch: 3  Step:  31  training loss:  0.002651097485795617\n",
      "Epoch: 3  Step:  32  training loss:  0.0027410066686570644\n",
      "Epoch: 3  Step:  33  training loss:  0.0033483721781522036\n",
      "Epoch: 3  Step:  34  training loss:  0.002311769872903824\n",
      "Epoch: 3  Step:  35  training loss:  0.00322852679528296\n",
      "Epoch: 3  Step:  36  training loss:  0.002053073840215802\n",
      "Epoch: 3  Step:  37  training loss:  0.005449230782687664\n",
      "Epoch: 3  Step:  38  training loss:  0.0034816425759345293\n",
      "Epoch: 3  Step:  39  training loss:  0.0025528636761009693\n",
      "Epoch: 3  Step:  40  training loss:  0.001852425280958414\n",
      "Epoch: 3  Step:  41  training loss:  0.003101133508607745\n",
      "Epoch: 3  Step:  42  training loss:  0.003083866788074374\n",
      "Epoch: 3  Step:  43  training loss:  0.0027543785981833935\n",
      "Epoch: 3  Step:  44  training loss:  0.003629769664257765\n",
      "Epoch: 3  Step:  45  training loss:  0.0024667491670697927\n",
      "Epoch: 3  Step:  46  training loss:  0.002209464553743601\n",
      "Epoch: 3  Step:  47  training loss:  0.002421541837975383\n",
      "Epoch: 3  Step:  48  training loss:  0.002074338961392641\n",
      "Epoch: 3  Step:  49  training loss:  0.002750835381448269\n",
      "Epoch: 3  Step:  50  training loss:  0.0023959132377058268\n",
      "Epoch: 3  Step:  51  training loss:  0.002991398796439171\n",
      "Epoch: 3  Step:  52  training loss:  0.002529749646782875\n",
      "Epoch: 3  Step:  53  training loss:  0.0023693309631198645\n",
      "Epoch: 3  Step:  54  training loss:  0.0028673491906374693\n",
      "Epoch: 3  Step:  55  training loss:  0.0023666657507419586\n",
      "Epoch: 4  Step:  0  training loss:  0.0022481984924525023\n",
      "Epoch: 4  Step:  1  training loss:  0.0024954776745289564\n",
      "Epoch: 4  Step:  2  training loss:  0.00334590463899076\n",
      "Epoch: 4  Step:  3  training loss:  0.0030734247993677855\n",
      "Epoch: 4  Step:  4  training loss:  0.0028788712806999683\n",
      "Epoch: 4  Step:  5  training loss:  0.0021455269306898117\n",
      "Epoch: 4  Step:  6  training loss:  0.002706456696614623\n",
      "Epoch: 4  Step:  7  training loss:  0.002522172871977091\n",
      "Epoch: 4  Step:  8  training loss:  0.0026727733202278614\n",
      "Epoch: 4  Step:  9  training loss:  0.00238519674167037\n",
      "Epoch: 4  Step:  10  training loss:  0.0027249478735029697\n",
      "Epoch: 4  Step:  11  training loss:  0.0023569418117403984\n",
      "Epoch: 4  Step:  12  training loss:  0.00232233596034348\n",
      "Epoch: 4  Step:  13  training loss:  0.002207490848377347\n",
      "Epoch: 4  Step:  14  training loss:  0.0025538248009979725\n",
      "Epoch: 4  Step:  15  training loss:  0.00220916117541492\n",
      "Epoch: 4  Step:  16  training loss:  0.002373318187892437\n",
      "Epoch: 4  Step:  17  training loss:  0.006444302387535572\n",
      "Epoch: 4  Step:  18  training loss:  0.0024458125699311495\n",
      "Epoch: 4  Step:  19  training loss:  0.002620205981656909\n",
      "Epoch: 4  Step:  20  training loss:  0.002299230545759201\n",
      "Epoch: 4  Step:  21  training loss:  0.0023980410769581795\n",
      "Epoch: 4  Step:  22  training loss:  0.0027678797487169504\n",
      "Epoch: 4  Step:  23  training loss:  0.0033938754349946976\n",
      "Epoch: 4  Step:  24  training loss:  0.003136184997856617\n",
      "Epoch: 4  Step:  25  training loss:  0.0028750686906278133\n",
      "Epoch: 4  Step:  26  training loss:  0.0025005778297781944\n",
      "Epoch: 4  Step:  27  training loss:  0.0018360648537054658\n",
      "Epoch: 4  Step:  28  training loss:  0.0026313208509236574\n",
      "Epoch: 4  Step:  29  training loss:  0.0027362245600670576\n",
      "Epoch: 4  Step:  30  training loss:  0.002449034247547388\n",
      "Epoch: 4  Step:  31  training loss:  0.006149970926344395\n",
      "Epoch: 4  Step:  32  training loss:  0.002543899230659008\n",
      "Epoch: 4  Step:  33  training loss:  0.00280995131470263\n",
      "Epoch: 4  Step:  34  training loss:  0.002612530952319503\n",
      "Epoch: 4  Step:  35  training loss:  0.001898579765111208\n",
      "Epoch: 4  Step:  36  training loss:  0.0018515200354158878\n",
      "Epoch: 4  Step:  37  training loss:  0.0028294629883021116\n",
      "Epoch: 4  Step:  38  training loss:  0.008938743732869625\n",
      "Epoch: 4  Step:  39  training loss:  0.002750203711912036\n",
      "Epoch: 4  Step:  40  training loss:  0.0037759242113679647\n",
      "Epoch: 4  Step:  41  training loss:  0.0022212231997400522\n",
      "Epoch: 4  Step:  42  training loss:  0.0022417211439460516\n",
      "Epoch: 4  Step:  43  training loss:  0.0025293794460594654\n",
      "Epoch: 4  Step:  44  training loss:  0.00404487457126379\n",
      "Epoch: 4  Step:  45  training loss:  0.0024472028017044067\n",
      "Epoch: 4  Step:  46  training loss:  0.0027320575900375843\n",
      "Epoch: 4  Step:  47  training loss:  0.0031039067544043064\n",
      "Epoch: 4  Step:  48  training loss:  0.003207848872989416\n",
      "Epoch: 4  Step:  49  training loss:  0.002242559799924493\n",
      "Epoch: 4  Step:  50  training loss:  0.003654646221548319\n",
      "Epoch: 4  Step:  51  training loss:  0.0020516549702733755\n",
      "Epoch: 4  Step:  52  training loss:  0.002672455972060561\n",
      "Epoch: 4  Step:  53  training loss:  0.002515307627618313\n",
      "Epoch: 4  Step:  54  training loss:  0.00295491935685277\n",
      "Epoch: 4  Step:  55  training loss:  0.0024834705982357264\n",
      "Epoch: 5  Step:  0  training loss:  0.002274235710501671\n",
      "Epoch: 5  Step:  1  training loss:  0.0025389993097633123\n",
      "Epoch: 5  Step:  2  training loss:  0.0024749175645411015\n",
      "Epoch: 5  Step:  3  training loss:  0.0020819883793592453\n",
      "Epoch: 5  Step:  4  training loss:  0.002451313426718116\n",
      "Epoch: 5  Step:  5  training loss:  0.0037062030751258135\n",
      "Epoch: 5  Step:  6  training loss:  0.0018486964982002974\n",
      "Epoch: 5  Step:  7  training loss:  0.003263267921283841\n",
      "Epoch: 5  Step:  8  training loss:  0.0028526410460472107\n",
      "Epoch: 5  Step:  9  training loss:  0.0018909659702330828\n",
      "Epoch: 5  Step:  10  training loss:  0.002558376407250762\n",
      "Epoch: 5  Step:  11  training loss:  0.0024995869025588036\n",
      "Epoch: 5  Step:  12  training loss:  0.002791187958791852\n",
      "Epoch: 5  Step:  13  training loss:  0.001984977163374424\n",
      "Epoch: 5  Step:  14  training loss:  0.0021879037376493216\n",
      "Epoch: 5  Step:  15  training loss:  0.002546116476878524\n",
      "Epoch: 5  Step:  16  training loss:  0.002300410531461239\n",
      "Epoch: 5  Step:  17  training loss:  0.0019543711096048355\n",
      "Epoch: 5  Step:  18  training loss:  0.0028360248543322086\n",
      "Epoch: 5  Step:  19  training loss:  0.003539949655532837\n",
      "Epoch: 5  Step:  20  training loss:  0.0017470350721850991\n",
      "Epoch: 5  Step:  21  training loss:  0.002355363918468356\n",
      "Epoch: 5  Step:  22  training loss:  0.002732503227889538\n",
      "Epoch: 5  Step:  23  training loss:  0.002684440929442644\n",
      "Epoch: 5  Step:  24  training loss:  0.0030621099285781384\n",
      "Epoch: 5  Step:  25  training loss:  0.0033846336882561445\n",
      "Epoch: 5  Step:  26  training loss:  0.002573419827967882\n",
      "Epoch: 5  Step:  27  training loss:  0.0026677530258893967\n",
      "Epoch: 5  Step:  28  training loss:  0.0033483535517007113\n",
      "Epoch: 5  Step:  29  training loss:  0.0033160822931677103\n",
      "Epoch: 5  Step:  30  training loss:  0.003123846836388111\n",
      "Epoch: 5  Step:  31  training loss:  0.002373875118792057\n",
      "Epoch: 5  Step:  32  training loss:  0.0026548043824732304\n",
      "Epoch: 5  Step:  33  training loss:  0.002995242364704609\n",
      "Epoch: 5  Step:  34  training loss:  0.002702375641092658\n",
      "Epoch: 5  Step:  35  training loss:  0.0062463050708174706\n",
      "Epoch: 5  Step:  36  training loss:  0.0023553972132503986\n",
      "Epoch: 5  Step:  37  training loss:  0.0036126708146184683\n",
      "Epoch: 5  Step:  38  training loss:  0.0032428877893835306\n",
      "Epoch: 5  Step:  39  training loss:  0.0019245641306042671\n",
      "Epoch: 5  Step:  40  training loss:  0.00836886651813984\n",
      "Epoch: 5  Step:  41  training loss:  0.006259032059460878\n",
      "Epoch: 5  Step:  42  training loss:  0.0025873747654259205\n",
      "Epoch: 5  Step:  43  training loss:  0.0026916773058474064\n",
      "Epoch: 5  Step:  44  training loss:  0.0020553863141685724\n",
      "Epoch: 5  Step:  45  training loss:  0.003804052248597145\n",
      "Epoch: 5  Step:  46  training loss:  0.0018662032671272755\n",
      "Epoch: 5  Step:  47  training loss:  0.002678423188626766\n",
      "Epoch: 5  Step:  48  training loss:  0.0027244396042078733\n",
      "Epoch: 5  Step:  49  training loss:  0.0026778101455420256\n",
      "Epoch: 5  Step:  50  training loss:  0.0024002217687666416\n",
      "Epoch: 5  Step:  51  training loss:  0.002308937953785062\n",
      "Epoch: 5  Step:  52  training loss:  0.002311404561623931\n",
      "Epoch: 5  Step:  53  training loss:  0.002714088885113597\n",
      "Epoch: 5  Step:  54  training loss:  0.002907382557168603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5  Step:  55  training loss:  0.002301314380019903\n",
      "Epoch: 6  Step:  0  training loss:  0.0024493595119565725\n",
      "Epoch: 6  Step:  1  training loss:  0.00219606957398355\n",
      "Epoch: 6  Step:  2  training loss:  0.002538016065955162\n",
      "Epoch: 6  Step:  3  training loss:  0.0022828225046396255\n",
      "Epoch: 6  Step:  4  training loss:  0.002274517435580492\n",
      "Epoch: 6  Step:  5  training loss:  0.0026751807890832424\n",
      "Epoch: 6  Step:  6  training loss:  0.002855512546375394\n",
      "Epoch: 6  Step:  7  training loss:  0.009088286198675632\n",
      "Epoch: 6  Step:  8  training loss:  0.0021637100726366043\n",
      "Epoch: 6  Step:  9  training loss:  0.002317847218364477\n",
      "Epoch: 6  Step:  10  training loss:  0.002448524348437786\n",
      "Epoch: 6  Step:  11  training loss:  0.0023989409673959017\n",
      "Epoch: 6  Step:  12  training loss:  0.0029308677185326815\n",
      "Epoch: 6  Step:  13  training loss:  0.0022653352934867144\n",
      "Epoch: 6  Step:  14  training loss:  0.00413482403382659\n",
      "Epoch: 6  Step:  15  training loss:  0.0020275407005101442\n",
      "Epoch: 6  Step:  16  training loss:  0.0024105117190629244\n",
      "Epoch: 6  Step:  17  training loss:  0.0024521113373339176\n",
      "Epoch: 6  Step:  18  training loss:  0.0026451735757291317\n",
      "Epoch: 6  Step:  19  training loss:  0.0034689754247665405\n",
      "Epoch: 6  Step:  20  training loss:  0.002729329513385892\n",
      "Epoch: 6  Step:  21  training loss:  0.003765180939808488\n",
      "Epoch: 6  Step:  22  training loss:  0.0031277379021048546\n",
      "Epoch: 6  Step:  23  training loss:  0.002032449934631586\n",
      "Epoch: 6  Step:  24  training loss:  0.0027381961699575186\n",
      "Epoch: 6  Step:  25  training loss:  0.0028434426058083773\n",
      "Epoch: 6  Step:  26  training loss:  0.002508992562070489\n",
      "Epoch: 6  Step:  27  training loss:  0.0026666447520256042\n",
      "Epoch: 6  Step:  28  training loss:  0.0022756545804440975\n",
      "Epoch: 6  Step:  29  training loss:  0.0028821928426623344\n",
      "Epoch: 6  Step:  30  training loss:  0.002628619782626629\n",
      "Epoch: 6  Step:  31  training loss:  0.0029391490388661623\n",
      "Epoch: 6  Step:  32  training loss:  0.002809677505865693\n",
      "Epoch: 6  Step:  33  training loss:  0.0021906322799623013\n",
      "Epoch: 6  Step:  34  training loss:  0.0024421419948339462\n",
      "Epoch: 6  Step:  35  training loss:  0.0020767494570463896\n",
      "Epoch: 6  Step:  36  training loss:  0.002640463411808014\n",
      "Epoch: 6  Step:  37  training loss:  0.003045206656679511\n",
      "Epoch: 6  Step:  38  training loss:  0.004063371568918228\n",
      "Epoch: 6  Step:  39  training loss:  0.0029277531430125237\n",
      "Epoch: 6  Step:  40  training loss:  0.002591577125713229\n",
      "Epoch: 6  Step:  41  training loss:  0.005726637784391642\n",
      "Epoch: 6  Step:  42  training loss:  0.0016514488961547613\n",
      "Epoch: 6  Step:  43  training loss:  0.003584830090403557\n",
      "Epoch: 6  Step:  44  training loss:  0.0024612098932266235\n",
      "Epoch: 6  Step:  45  training loss:  0.0024173802230507135\n",
      "Epoch: 6  Step:  46  training loss:  0.002494419226422906\n",
      "Epoch: 6  Step:  47  training loss:  0.002863009925931692\n",
      "Epoch: 6  Step:  48  training loss:  0.0025565933901816607\n",
      "Epoch: 6  Step:  49  training loss:  0.0016705435700714588\n",
      "Epoch: 6  Step:  50  training loss:  0.0029990598559379578\n",
      "Epoch: 6  Step:  51  training loss:  0.001623579883016646\n",
      "Epoch: 6  Step:  52  training loss:  0.005800166167318821\n",
      "Epoch: 6  Step:  53  training loss:  0.0025252734776586294\n",
      "Epoch: 6  Step:  54  training loss:  0.003186315530911088\n",
      "Epoch: 6  Step:  55  training loss:  0.0022552269510924816\n",
      "Epoch: 7  Step:  0  training loss:  0.002225831849500537\n",
      "Epoch: 7  Step:  1  training loss:  0.00258036726154387\n",
      "Epoch: 7  Step:  2  training loss:  0.002565123373642564\n",
      "Epoch: 7  Step:  3  training loss:  0.002264606300741434\n",
      "Epoch: 7  Step:  4  training loss:  0.0020098453387618065\n",
      "Epoch: 7  Step:  5  training loss:  0.0026330577675253153\n",
      "Epoch: 7  Step:  6  training loss:  0.006159675773233175\n",
      "Epoch: 7  Step:  7  training loss:  0.0023847422562539577\n",
      "Epoch: 7  Step:  8  training loss:  0.00246162386611104\n",
      "Epoch: 7  Step:  9  training loss:  0.0031705161090940237\n",
      "Epoch: 7  Step:  10  training loss:  0.00222222157754004\n",
      "Epoch: 7  Step:  11  training loss:  0.0023667747154831886\n",
      "Epoch: 7  Step:  12  training loss:  0.002662618411704898\n",
      "Epoch: 7  Step:  13  training loss:  0.0038597946986556053\n",
      "Epoch: 7  Step:  14  training loss:  0.0025666672736406326\n",
      "Epoch: 7  Step:  15  training loss:  0.002311740769073367\n",
      "Epoch: 7  Step:  16  training loss:  0.0023919325321912766\n",
      "Epoch: 7  Step:  17  training loss:  0.0022070996928960085\n",
      "Epoch: 7  Step:  18  training loss:  0.002063722349703312\n",
      "Epoch: 7  Step:  19  training loss:  0.0022071348503232002\n",
      "Epoch: 7  Step:  20  training loss:  0.0029039315413683653\n",
      "Epoch: 7  Step:  21  training loss:  0.0023067686706781387\n",
      "Epoch: 7  Step:  22  training loss:  0.003181599313393235\n",
      "Epoch: 7  Step:  23  training loss:  0.009001143276691437\n",
      "Epoch: 7  Step:  24  training loss:  0.0038009416311979294\n",
      "Epoch: 7  Step:  25  training loss:  0.005363344680517912\n",
      "Epoch: 7  Step:  26  training loss:  0.002423518802970648\n",
      "Epoch: 7  Step:  27  training loss:  0.0025001601316034794\n",
      "Epoch: 7  Step:  28  training loss:  0.0019204457057639956\n",
      "Epoch: 7  Step:  29  training loss:  0.003282445017248392\n",
      "Epoch: 7  Step:  30  training loss:  0.0033790075685828924\n",
      "Epoch: 7  Step:  31  training loss:  0.002209864789620042\n",
      "Epoch: 7  Step:  32  training loss:  0.0025816180277615786\n",
      "Epoch: 7  Step:  33  training loss:  0.003083434421569109\n",
      "Epoch: 7  Step:  34  training loss:  0.00333618838340044\n",
      "Epoch: 7  Step:  35  training loss:  0.002942100865766406\n",
      "Epoch: 7  Step:  36  training loss:  0.0025090291164815426\n",
      "Epoch: 7  Step:  37  training loss:  0.002378715667873621\n",
      "Epoch: 7  Step:  38  training loss:  0.002643886487931013\n",
      "Epoch: 7  Step:  39  training loss:  0.003008794505149126\n",
      "Epoch: 7  Step:  40  training loss:  0.002417625393718481\n",
      "Epoch: 7  Step:  41  training loss:  0.0023110797628760338\n",
      "Epoch: 7  Step:  42  training loss:  0.002903477055951953\n",
      "Epoch: 7  Step:  43  training loss:  0.0022075409069657326\n",
      "Epoch: 7  Step:  44  training loss:  0.0032116533257067204\n",
      "Epoch: 7  Step:  45  training loss:  0.003014652756974101\n",
      "Epoch: 7  Step:  46  training loss:  0.002148883882910013\n",
      "Epoch: 7  Step:  47  training loss:  0.002326160902157426\n",
      "Epoch: 7  Step:  48  training loss:  0.002275229664519429\n",
      "Epoch: 7  Step:  49  training loss:  0.002604386303573847\n",
      "Epoch: 7  Step:  50  training loss:  0.0024328543804585934\n",
      "Epoch: 7  Step:  51  training loss:  0.0036365739069879055\n",
      "Epoch: 7  Step:  52  training loss:  0.0022025019861757755\n",
      "Epoch: 7  Step:  53  training loss:  0.002134348265826702\n",
      "Epoch: 7  Step:  54  training loss:  0.003012060420587659\n",
      "Epoch: 7  Step:  55  training loss:  0.002545825904235244\n",
      "Epoch: 8  Step:  0  training loss:  0.0022767018526792526\n",
      "Epoch: 8  Step:  1  training loss:  0.002034963108599186\n",
      "Epoch: 8  Step:  2  training loss:  0.0028418018482625484\n",
      "Epoch: 8  Step:  3  training loss:  0.004123223014175892\n",
      "Epoch: 8  Step:  4  training loss:  0.00316684995777905\n",
      "Epoch: 8  Step:  5  training loss:  0.002428211970254779\n",
      "Epoch: 8  Step:  6  training loss:  0.0059577710926532745\n",
      "Epoch: 8  Step:  7  training loss:  0.009562124498188496\n",
      "Epoch: 8  Step:  8  training loss:  0.003571642329916358\n",
      "Epoch: 8  Step:  9  training loss:  0.002313564997166395\n",
      "Epoch: 8  Step:  10  training loss:  0.0032640937715768814\n",
      "Epoch: 8  Step:  11  training loss:  0.0026528658345341682\n",
      "Epoch: 8  Step:  12  training loss:  0.002302624052390456\n",
      "Epoch: 8  Step:  13  training loss:  0.002853406360372901\n",
      "Epoch: 8  Step:  14  training loss:  0.002762491349130869\n",
      "Epoch: 8  Step:  15  training loss:  0.0018283443059772253\n",
      "Epoch: 8  Step:  16  training loss:  0.002438259543851018\n",
      "Epoch: 8  Step:  17  training loss:  0.0021817877423018217\n",
      "Epoch: 8  Step:  18  training loss:  0.0024446884635835886\n",
      "Epoch: 8  Step:  19  training loss:  0.004127709195017815\n",
      "Epoch: 8  Step:  20  training loss:  0.0024397557135671377\n",
      "Epoch: 8  Step:  21  training loss:  0.0022121912334114313\n",
      "Epoch: 8  Step:  22  training loss:  0.0024427571333944798\n",
      "Epoch: 8  Step:  23  training loss:  0.0021891517098993063\n",
      "Epoch: 8  Step:  24  training loss:  0.002281582448631525\n",
      "Epoch: 8  Step:  25  training loss:  0.0025854315608739853\n",
      "Epoch: 8  Step:  26  training loss:  0.003115949686616659\n",
      "Epoch: 8  Step:  27  training loss:  0.002635533455759287\n",
      "Epoch: 8  Step:  28  training loss:  0.002907016547396779\n",
      "Epoch: 8  Step:  29  training loss:  0.0030424720607697964\n",
      "Epoch: 8  Step:  30  training loss:  0.005267229862511158\n",
      "Epoch: 8  Step:  31  training loss:  0.0027441144920885563\n",
      "Epoch: 8  Step:  32  training loss:  0.0023858174681663513\n",
      "Epoch: 8  Step:  33  training loss:  0.003271477296948433\n",
      "Epoch: 8  Step:  34  training loss:  0.0025160054210573435\n",
      "Epoch: 8  Step:  35  training loss:  0.002729945583269\n",
      "Epoch: 8  Step:  36  training loss:  0.0028792270459234715\n",
      "Epoch: 8  Step:  37  training loss:  0.002660320606082678\n",
      "Epoch: 8  Step:  38  training loss:  0.0016653304919600487\n",
      "Epoch: 8  Step:  39  training loss:  0.0021265761461108923\n",
      "Epoch: 8  Step:  40  training loss:  0.0019985446706414223\n",
      "Epoch: 8  Step:  41  training loss:  0.002743094228208065\n",
      "Epoch: 8  Step:  42  training loss:  0.0026737209409475327\n",
      "Epoch: 8  Step:  43  training loss:  0.0017723088385537267\n",
      "Epoch: 8  Step:  44  training loss:  0.002487773774191737\n",
      "Epoch: 8  Step:  45  training loss:  0.002511317143216729\n",
      "Epoch: 8  Step:  46  training loss:  0.0026165249291807413\n",
      "Epoch: 8  Step:  47  training loss:  0.0022619524970650673\n",
      "Epoch: 8  Step:  48  training loss:  0.0018838164396584034\n",
      "Epoch: 8  Step:  49  training loss:  0.0032966267317533493\n",
      "Epoch: 8  Step:  50  training loss:  0.003458683378994465\n",
      "Epoch: 8  Step:  51  training loss:  0.0018685320392251015\n",
      "Epoch: 8  Step:  52  training loss:  0.002621594350785017\n",
      "Epoch: 8  Step:  53  training loss:  0.0024507029447704554\n",
      "Epoch: 8  Step:  54  training loss:  0.002787676639854908\n",
      "Epoch: 8  Step:  55  training loss:  0.0018790332833305001\n",
      "Epoch: 9  Step:  0  training loss:  0.003023844910785556\n",
      "Epoch: 9  Step:  1  training loss:  0.0023448930587619543\n",
      "Epoch: 9  Step:  2  training loss:  0.003468845970928669\n",
      "Epoch: 9  Step:  3  training loss:  0.0025921871419996023\n",
      "Epoch: 9  Step:  4  training loss:  0.0031954997684806585\n",
      "Epoch: 9  Step:  5  training loss:  0.0026375027373433113\n",
      "Epoch: 9  Step:  6  training loss:  0.0017143167788162827\n",
      "Epoch: 9  Step:  7  training loss:  0.005083860829472542\n",
      "Epoch: 9  Step:  8  training loss:  0.0024622478522360325\n",
      "Epoch: 9  Step:  9  training loss:  0.0020426164846867323\n",
      "Epoch: 9  Step:  10  training loss:  0.0024199567269533873\n",
      "Epoch: 9  Step:  11  training loss:  0.0028193218167871237\n",
      "Epoch: 9  Step:  12  training loss:  0.0023268151562660933\n",
      "Epoch: 9  Step:  13  training loss:  0.002550094621255994\n",
      "Epoch: 9  Step:  14  training loss:  0.002181506482884288\n",
      "Epoch: 9  Step:  15  training loss:  0.002760964445769787\n",
      "Epoch: 9  Step:  16  training loss:  0.002785515272989869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9  Step:  17  training loss:  0.0025119977071881294\n",
      "Epoch: 9  Step:  18  training loss:  0.0027313537430018187\n",
      "Epoch: 9  Step:  19  training loss:  0.0026102191768586636\n",
      "Epoch: 9  Step:  20  training loss:  0.0023908503353595734\n",
      "Epoch: 9  Step:  21  training loss:  0.0027086352929472923\n",
      "Epoch: 9  Step:  22  training loss:  0.001980252331122756\n",
      "Epoch: 9  Step:  23  training loss:  0.002485585166141391\n",
      "Epoch: 9  Step:  24  training loss:  0.0028186982963234186\n",
      "Epoch: 9  Step:  25  training loss:  0.0027758656069636345\n",
      "Epoch: 9  Step:  26  training loss:  0.002883714158087969\n",
      "Epoch: 9  Step:  27  training loss:  0.0022564842365682125\n",
      "Epoch: 9  Step:  28  training loss:  0.0032133308704942465\n",
      "Epoch: 9  Step:  29  training loss:  0.0018774239579215646\n",
      "Epoch: 9  Step:  30  training loss:  0.00285350251942873\n",
      "Epoch: 9  Step:  31  training loss:  0.00280217407271266\n",
      "Epoch: 9  Step:  32  training loss:  0.0027617064770311117\n",
      "Epoch: 9  Step:  33  training loss:  0.005745834670960903\n",
      "Epoch: 9  Step:  34  training loss:  0.002238306449726224\n",
      "Epoch: 9  Step:  35  training loss:  0.009237277321517467\n",
      "Epoch: 9  Step:  36  training loss:  0.0031514528673142195\n",
      "Epoch: 9  Step:  37  training loss:  0.003838108852505684\n",
      "Epoch: 9  Step:  38  training loss:  0.0022958123590797186\n",
      "Epoch: 9  Step:  39  training loss:  0.0033426282461732626\n",
      "Epoch: 9  Step:  40  training loss:  0.002389246365055442\n",
      "Epoch: 9  Step:  41  training loss:  0.002520054578781128\n",
      "Epoch: 9  Step:  42  training loss:  0.002558794105425477\n",
      "Epoch: 9  Step:  43  training loss:  0.0025202003307640553\n",
      "Epoch: 9  Step:  44  training loss:  0.0023708846420049667\n",
      "Epoch: 9  Step:  45  training loss:  0.002278361702337861\n",
      "Epoch: 9  Step:  46  training loss:  0.0030879357364028692\n",
      "Epoch: 9  Step:  47  training loss:  0.0033111057709902525\n",
      "Epoch: 9  Step:  48  training loss:  0.002181129064410925\n",
      "Epoch: 9  Step:  49  training loss:  0.002251375699415803\n",
      "Epoch: 9  Step:  50  training loss:  0.001926235854625702\n",
      "Epoch: 9  Step:  51  training loss:  0.0020933079067617655\n",
      "Epoch: 9  Step:  52  training loss:  0.0024968506768345833\n",
      "Epoch: 9  Step:  53  training loss:  0.0030189419630914927\n",
      "Epoch: 9  Step:  54  training loss:  0.002701877849176526\n",
      "Epoch: 9  Step:  55  training loss:  0.0031335093080997467\n",
      "Epoch: 10  Step:  0  training loss:  0.006097105797380209\n",
      "Epoch: 10  Step:  1  training loss:  0.0031456109136343002\n",
      "Epoch: 10  Step:  2  training loss:  0.002139067044481635\n",
      "Epoch: 10  Step:  3  training loss:  0.007972979918122292\n",
      "Epoch: 10  Step:  4  training loss:  0.0021895538084208965\n",
      "Epoch: 10  Step:  5  training loss:  0.002630384871736169\n",
      "Epoch: 10  Step:  6  training loss:  0.0023159184493124485\n",
      "Epoch: 10  Step:  7  training loss:  0.003055854234844446\n",
      "Epoch: 10  Step:  8  training loss:  0.0031686099246144295\n",
      "Epoch: 10  Step:  9  training loss:  0.001798825105652213\n",
      "Epoch: 10  Step:  10  training loss:  0.0027074324898421764\n",
      "Epoch: 10  Step:  11  training loss:  0.00263218698091805\n",
      "Epoch: 10  Step:  12  training loss:  0.0027187892701476812\n",
      "Epoch: 10  Step:  13  training loss:  0.0017535507213324308\n",
      "Epoch: 10  Step:  14  training loss:  0.002890418516471982\n",
      "Epoch: 10  Step:  15  training loss:  0.005567124113440514\n",
      "Epoch: 10  Step:  16  training loss:  0.0021893884986639023\n",
      "Epoch: 10  Step:  17  training loss:  0.0024641535710543394\n",
      "Epoch: 10  Step:  18  training loss:  0.0027000245172530413\n",
      "Epoch: 10  Step:  19  training loss:  0.0026679569855332375\n",
      "Epoch: 10  Step:  20  training loss:  0.0027114569675177336\n",
      "Epoch: 10  Step:  21  training loss:  0.0024781578686088324\n",
      "Epoch: 10  Step:  22  training loss:  0.0026116473600268364\n",
      "Epoch: 10  Step:  23  training loss:  0.0034067267552018166\n",
      "Epoch: 10  Step:  24  training loss:  0.0020151089411228895\n",
      "Epoch: 10  Step:  25  training loss:  0.0027415293734520674\n",
      "Epoch: 10  Step:  26  training loss:  0.0024862608406692743\n",
      "Epoch: 10  Step:  27  training loss:  0.00306668970733881\n",
      "Epoch: 10  Step:  28  training loss:  0.0024789602030068636\n",
      "Epoch: 10  Step:  29  training loss:  0.003168565919622779\n",
      "Epoch: 10  Step:  30  training loss:  0.0034486257936805487\n",
      "Epoch: 10  Step:  31  training loss:  0.0032201993744820356\n",
      "Epoch: 10  Step:  32  training loss:  0.0020896578207612038\n",
      "Epoch: 10  Step:  33  training loss:  0.002507997676730156\n",
      "Epoch: 10  Step:  34  training loss:  0.0024425501469522715\n",
      "Epoch: 10  Step:  35  training loss:  0.0026910253800451756\n",
      "Epoch: 10  Step:  36  training loss:  0.002102080499753356\n",
      "Epoch: 10  Step:  37  training loss:  0.0027738662902265787\n",
      "Epoch: 10  Step:  38  training loss:  0.0022587545681744814\n",
      "Epoch: 10  Step:  39  training loss:  0.002642799401655793\n",
      "Epoch: 10  Step:  40  training loss:  0.0025511267594993114\n",
      "Epoch: 10  Step:  41  training loss:  0.0031591192819178104\n",
      "Epoch: 10  Step:  42  training loss:  0.002898153616115451\n",
      "Epoch: 10  Step:  43  training loss:  0.0018331229221075773\n",
      "Epoch: 10  Step:  44  training loss:  0.0027624457143247128\n",
      "Epoch: 10  Step:  45  training loss:  0.003749377327039838\n",
      "Epoch: 10  Step:  46  training loss:  0.002661491744220257\n",
      "Epoch: 10  Step:  47  training loss:  0.0019487515091896057\n",
      "Epoch: 10  Step:  48  training loss:  0.002096305601298809\n",
      "Epoch: 10  Step:  49  training loss:  0.0028889821842312813\n",
      "Epoch: 10  Step:  50  training loss:  0.0022446932271122932\n",
      "Epoch: 10  Step:  51  training loss:  0.0035694134421646595\n",
      "Epoch: 10  Step:  52  training loss:  0.002649502130225301\n",
      "Epoch: 10  Step:  53  training loss:  0.002417644951492548\n",
      "Epoch: 10  Step:  54  training loss:  0.0020287600345909595\n",
      "Epoch: 10  Step:  55  training loss:  0.0022679902613162994\n",
      "Epoch: 11  Step:  0  training loss:  0.0028913074638694525\n",
      "Epoch: 11  Step:  1  training loss:  0.0030178376473486423\n",
      "Epoch: 11  Step:  2  training loss:  0.001712404191493988\n",
      "Epoch: 11  Step:  3  training loss:  0.0030923786107450724\n",
      "Epoch: 11  Step:  4  training loss:  0.0017161740688607097\n",
      "Epoch: 11  Step:  5  training loss:  0.0037171440199017525\n",
      "Epoch: 11  Step:  6  training loss:  0.0020416120532900095\n",
      "Epoch: 11  Step:  7  training loss:  0.0024815506767481565\n",
      "Epoch: 11  Step:  8  training loss:  0.002224014839157462\n",
      "Epoch: 11  Step:  9  training loss:  0.002129183616489172\n",
      "Epoch: 11  Step:  10  training loss:  0.0031271905172616243\n",
      "Epoch: 11  Step:  11  training loss:  0.0038557492662221193\n",
      "Epoch: 11  Step:  12  training loss:  0.0029543875716626644\n",
      "Epoch: 11  Step:  13  training loss:  0.002712065353989601\n",
      "Epoch: 11  Step:  14  training loss:  0.0022252968046814203\n",
      "Epoch: 11  Step:  15  training loss:  0.002175993286073208\n",
      "Epoch: 11  Step:  16  training loss:  0.00919458456337452\n",
      "Epoch: 11  Step:  17  training loss:  0.003053817432373762\n",
      "Epoch: 11  Step:  18  training loss:  0.0032336446456611156\n",
      "Epoch: 11  Step:  19  training loss:  0.0023158055264502764\n",
      "Epoch: 11  Step:  20  training loss:  0.0023875052575021982\n",
      "Epoch: 11  Step:  21  training loss:  0.0024475788231939077\n",
      "Epoch: 11  Step:  22  training loss:  0.0024386364966630936\n",
      "Epoch: 11  Step:  23  training loss:  0.0020907889120280743\n",
      "Epoch: 11  Step:  24  training loss:  0.0025711285416036844\n",
      "Epoch: 11  Step:  25  training loss:  0.002812360879033804\n",
      "Epoch: 11  Step:  26  training loss:  0.002266699681058526\n",
      "Epoch: 11  Step:  27  training loss:  0.0022927536629140377\n",
      "Epoch: 11  Step:  28  training loss:  0.0026536404620856047\n",
      "Epoch: 11  Step:  29  training loss:  0.002667152788490057\n",
      "Epoch: 11  Step:  30  training loss:  0.002980556571856141\n",
      "Epoch: 11  Step:  31  training loss:  0.002709768945351243\n",
      "Epoch: 11  Step:  32  training loss:  0.002970233326777816\n",
      "Epoch: 11  Step:  33  training loss:  0.0028946336824446917\n",
      "Epoch: 11  Step:  34  training loss:  0.004471960477530956\n",
      "Epoch: 11  Step:  35  training loss:  0.005853515118360519\n",
      "Epoch: 11  Step:  36  training loss:  0.0021844462025910616\n",
      "Epoch: 11  Step:  37  training loss:  0.0021498941350728273\n",
      "Epoch: 11  Step:  38  training loss:  0.0020207371562719345\n",
      "Epoch: 11  Step:  39  training loss:  0.003132730722427368\n",
      "Epoch: 11  Step:  40  training loss:  0.0018358167726546526\n",
      "Epoch: 11  Step:  41  training loss:  0.005407714284956455\n",
      "Epoch: 11  Step:  42  training loss:  0.0028876301366835833\n",
      "Epoch: 11  Step:  43  training loss:  0.0025428032968193293\n",
      "Epoch: 11  Step:  44  training loss:  0.0024861476849764585\n",
      "Epoch: 11  Step:  45  training loss:  0.002214997075498104\n",
      "Epoch: 11  Step:  46  training loss:  0.0026337949093431234\n",
      "Epoch: 11  Step:  47  training loss:  0.0023785545490682125\n",
      "Epoch: 11  Step:  48  training loss:  0.0022919250186532736\n",
      "Epoch: 11  Step:  49  training loss:  0.002171335509046912\n",
      "Epoch: 11  Step:  50  training loss:  0.0031764826271682978\n",
      "Epoch: 11  Step:  51  training loss:  0.0028770510107278824\n",
      "Epoch: 11  Step:  52  training loss:  0.0020806551910936832\n",
      "Epoch: 11  Step:  53  training loss:  0.001905039302073419\n",
      "Epoch: 11  Step:  54  training loss:  0.00232698954641819\n",
      "Epoch: 11  Step:  55  training loss:  0.0022315653041005135\n",
      "Epoch: 12  Step:  0  training loss:  0.0037311387713998556\n",
      "Epoch: 12  Step:  1  training loss:  0.007728411350399256\n",
      "Epoch: 12  Step:  2  training loss:  0.0025897128507494926\n",
      "Epoch: 12  Step:  3  training loss:  0.0025874334387481213\n",
      "Epoch: 12  Step:  4  training loss:  0.0028654071502387524\n",
      "Epoch: 12  Step:  5  training loss:  0.0018600650364533067\n",
      "Epoch: 12  Step:  6  training loss:  0.0021740386728197336\n",
      "Epoch: 12  Step:  7  training loss:  0.002328340895473957\n",
      "Epoch: 12  Step:  8  training loss:  0.002446256810799241\n",
      "Epoch: 12  Step:  9  training loss:  0.0022636905778199434\n",
      "Epoch: 12  Step:  10  training loss:  0.002102819038555026\n",
      "Epoch: 12  Step:  11  training loss:  0.0024221702478826046\n",
      "Epoch: 12  Step:  12  training loss:  0.0027518332935869694\n",
      "Epoch: 12  Step:  13  training loss:  0.0025404614862054586\n",
      "Epoch: 12  Step:  14  training loss:  0.0027511233929544687\n",
      "Epoch: 12  Step:  15  training loss:  0.0023297721054404974\n",
      "Epoch: 12  Step:  16  training loss:  0.0025683417916297913\n",
      "Epoch: 12  Step:  17  training loss:  0.002341901184991002\n",
      "Epoch: 12  Step:  18  training loss:  0.0037056247238069773\n",
      "Epoch: 12  Step:  19  training loss:  0.0029731669928878546\n",
      "Epoch: 12  Step:  20  training loss:  0.002345621818676591\n",
      "Epoch: 12  Step:  21  training loss:  0.0024190524127334356\n",
      "Epoch: 12  Step:  22  training loss:  0.0035379293840378523\n",
      "Epoch: 12  Step:  23  training loss:  0.004374640062451363\n",
      "Epoch: 12  Step:  24  training loss:  0.0027494262903928757\n",
      "Epoch: 12  Step:  25  training loss:  0.0028518408071249723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12  Step:  26  training loss:  0.0023127070162445307\n",
      "Epoch: 12  Step:  27  training loss:  0.0025680253747850657\n",
      "Epoch: 12  Step:  28  training loss:  0.002109838416799903\n",
      "Epoch: 12  Step:  29  training loss:  0.0029968679882586002\n",
      "Epoch: 12  Step:  30  training loss:  0.0023792015854269266\n",
      "Epoch: 12  Step:  31  training loss:  0.001341316499747336\n",
      "Epoch: 12  Step:  32  training loss:  0.0022129679564386606\n",
      "Epoch: 12  Step:  33  training loss:  0.0019459958421066403\n",
      "Epoch: 12  Step:  34  training loss:  0.002441767370328307\n",
      "Epoch: 12  Step:  35  training loss:  0.0020173636730760336\n",
      "Epoch: 12  Step:  36  training loss:  0.0030101758893579245\n",
      "Epoch: 12  Step:  37  training loss:  0.008623430505394936\n",
      "Epoch: 12  Step:  38  training loss:  0.0058101569302380085\n",
      "Epoch: 12  Step:  39  training loss:  0.0029015729669481516\n",
      "Epoch: 12  Step:  40  training loss:  0.0024463958106935024\n",
      "Epoch: 12  Step:  41  training loss:  0.0028639959637075663\n",
      "Epoch: 12  Step:  42  training loss:  0.0018394981743767858\n",
      "Epoch: 12  Step:  43  training loss:  0.0025132778100669384\n",
      "Epoch: 12  Step:  44  training loss:  0.0017246064962819219\n",
      "Epoch: 12  Step:  45  training loss:  0.002930998569354415\n",
      "Epoch: 12  Step:  46  training loss:  0.002960262820124626\n",
      "Epoch: 12  Step:  47  training loss:  0.002375771291553974\n",
      "Epoch: 12  Step:  48  training loss:  0.0021287165582180023\n",
      "Epoch: 12  Step:  49  training loss:  0.0021137974690645933\n",
      "Epoch: 12  Step:  50  training loss:  0.0019686464220285416\n",
      "Epoch: 12  Step:  51  training loss:  0.002245652489364147\n",
      "Epoch: 12  Step:  52  training loss:  0.0029612488579005003\n",
      "Epoch: 12  Step:  53  training loss:  0.001883191056549549\n",
      "Epoch: 12  Step:  54  training loss:  0.0033678405452519655\n",
      "Epoch: 12  Step:  55  training loss:  0.003170008072629571\n",
      "Epoch: 13  Step:  0  training loss:  0.0026434322353452444\n",
      "Epoch: 13  Step:  1  training loss:  0.002387851011008024\n",
      "Epoch: 13  Step:  2  training loss:  0.0031107431277632713\n",
      "Epoch: 13  Step:  3  training loss:  0.002027394250035286\n",
      "Epoch: 13  Step:  4  training loss:  0.002604125067591667\n",
      "Epoch: 13  Step:  5  training loss:  0.002746078884229064\n",
      "Epoch: 13  Step:  6  training loss:  0.002603846602141857\n",
      "Epoch: 13  Step:  7  training loss:  0.0023619607090950012\n",
      "Epoch: 13  Step:  8  training loss:  0.002036572666838765\n",
      "Epoch: 13  Step:  9  training loss:  0.0024967787321656942\n",
      "Epoch: 13  Step:  10  training loss:  0.005577777046710253\n",
      "Epoch: 13  Step:  11  training loss:  0.0023673998657613993\n",
      "Epoch: 13  Step:  12  training loss:  0.0036399373784661293\n",
      "Epoch: 13  Step:  13  training loss:  0.0031832177191972733\n",
      "Epoch: 13  Step:  14  training loss:  0.005880086217075586\n",
      "Epoch: 13  Step:  15  training loss:  0.0026367693208158016\n",
      "Epoch: 13  Step:  16  training loss:  0.002491289284080267\n",
      "Epoch: 13  Step:  17  training loss:  0.002358892234042287\n",
      "Epoch: 13  Step:  18  training loss:  0.0083094397559762\n",
      "Epoch: 13  Step:  19  training loss:  0.002027845475822687\n",
      "Epoch: 13  Step:  20  training loss:  0.0021709229331463575\n",
      "Epoch: 13  Step:  21  training loss:  0.001998936990275979\n",
      "Epoch: 13  Step:  22  training loss:  0.0030713325832039118\n",
      "Epoch: 13  Step:  23  training loss:  0.002586656017228961\n",
      "Epoch: 13  Step:  24  training loss:  0.001763633918017149\n",
      "Epoch: 13  Step:  25  training loss:  0.002639679703861475\n",
      "Epoch: 13  Step:  26  training loss:  0.0029483684338629246\n",
      "Epoch: 13  Step:  27  training loss:  0.0023740101605653763\n",
      "Epoch: 13  Step:  28  training loss:  0.003043197328224778\n",
      "Epoch: 13  Step:  29  training loss:  0.0019852013792842627\n",
      "Epoch: 13  Step:  30  training loss:  0.0017680760938674212\n",
      "Epoch: 13  Step:  31  training loss:  0.002462361240759492\n",
      "Epoch: 13  Step:  32  training loss:  0.002296263352036476\n",
      "Epoch: 13  Step:  33  training loss:  0.0026987718883901834\n",
      "Epoch: 13  Step:  34  training loss:  0.0033026295714080334\n",
      "Epoch: 13  Step:  35  training loss:  0.003174335928633809\n",
      "Epoch: 13  Step:  36  training loss:  0.00208803778514266\n",
      "Epoch: 13  Step:  37  training loss:  0.0026379262562841177\n",
      "Epoch: 13  Step:  38  training loss:  0.0022377714049071074\n",
      "Epoch: 13  Step:  39  training loss:  0.002710778033360839\n",
      "Epoch: 13  Step:  40  training loss:  0.002452182350680232\n",
      "Epoch: 13  Step:  41  training loss:  0.002150071319192648\n",
      "Epoch: 13  Step:  42  training loss:  0.003178009297698736\n",
      "Epoch: 13  Step:  43  training loss:  0.0030585669446736574\n",
      "Epoch: 13  Step:  44  training loss:  0.002073509618639946\n",
      "Epoch: 13  Step:  45  training loss:  0.002536520129069686\n",
      "Epoch: 13  Step:  46  training loss:  0.0020803543739020824\n",
      "Epoch: 13  Step:  47  training loss:  0.0024281300138682127\n",
      "Epoch: 13  Step:  48  training loss:  0.004830984864383936\n",
      "Epoch: 13  Step:  49  training loss:  0.003347582882270217\n",
      "Epoch: 13  Step:  50  training loss:  0.0025016290601342916\n",
      "Epoch: 13  Step:  51  training loss:  0.0024996132124215364\n",
      "Epoch: 13  Step:  52  training loss:  0.0032973771449178457\n",
      "Epoch: 13  Step:  53  training loss:  0.00237724999897182\n",
      "Epoch: 13  Step:  54  training loss:  0.0020809248089790344\n",
      "Epoch: 13  Step:  55  training loss:  0.002104075625538826\n",
      "Epoch: 14  Step:  0  training loss:  0.0027690043207257986\n",
      "Epoch: 14  Step:  1  training loss:  0.0025079273618757725\n",
      "Epoch: 14  Step:  2  training loss:  0.0019437355222180486\n",
      "Epoch: 14  Step:  3  training loss:  0.002061176113784313\n",
      "Epoch: 14  Step:  4  training loss:  0.0021014600060880184\n",
      "Epoch: 14  Step:  5  training loss:  0.0022691250778734684\n",
      "Epoch: 14  Step:  6  training loss:  0.0022872493136674166\n",
      "Epoch: 14  Step:  7  training loss:  0.005362199619412422\n",
      "Epoch: 14  Step:  8  training loss:  0.002644880907610059\n",
      "Epoch: 14  Step:  9  training loss:  0.0027286664117127657\n",
      "Epoch: 14  Step:  10  training loss:  0.0024311929009854794\n",
      "Epoch: 14  Step:  11  training loss:  0.0019790867809206247\n",
      "Epoch: 14  Step:  12  training loss:  0.0030814548954367638\n",
      "Epoch: 14  Step:  13  training loss:  0.001780989347025752\n",
      "Epoch: 14  Step:  14  training loss:  0.0023543762508779764\n",
      "Epoch: 14  Step:  15  training loss:  0.001954213483259082\n",
      "Epoch: 14  Step:  16  training loss:  0.002394553506746888\n",
      "Epoch: 14  Step:  17  training loss:  0.002453765831887722\n",
      "Epoch: 14  Step:  18  training loss:  0.0022278509568423033\n",
      "Epoch: 14  Step:  19  training loss:  0.0028763425070792437\n",
      "Epoch: 14  Step:  20  training loss:  0.0029586635064333677\n",
      "Epoch: 14  Step:  21  training loss:  0.002935740863904357\n",
      "Epoch: 14  Step:  22  training loss:  0.0023560593836009502\n",
      "Epoch: 14  Step:  23  training loss:  0.001723643857985735\n",
      "Epoch: 14  Step:  24  training loss:  0.003512448165565729\n",
      "Epoch: 14  Step:  25  training loss:  0.00251310714520514\n",
      "Epoch: 14  Step:  26  training loss:  0.0022069329861551523\n",
      "Epoch: 14  Step:  27  training loss:  0.0029114761855453253\n",
      "Epoch: 14  Step:  28  training loss:  0.0027876354288309813\n",
      "Epoch: 14  Step:  29  training loss:  0.0034350911155343056\n",
      "Epoch: 14  Step:  30  training loss:  0.002642404055222869\n",
      "Epoch: 14  Step:  31  training loss:  0.0024123191833496094\n",
      "Epoch: 14  Step:  32  training loss:  0.002819798653945327\n",
      "Epoch: 14  Step:  33  training loss:  0.00359904021024704\n",
      "Epoch: 14  Step:  34  training loss:  0.0029244443867355585\n",
      "Epoch: 14  Step:  35  training loss:  0.0033617273438721895\n",
      "Epoch: 14  Step:  36  training loss:  0.0018896039109677076\n",
      "Epoch: 14  Step:  37  training loss:  0.0017889239825308323\n",
      "Epoch: 14  Step:  38  training loss:  0.003975620958954096\n",
      "Epoch: 14  Step:  39  training loss:  0.008240259252488613\n",
      "Epoch: 14  Step:  40  training loss:  0.0023278400767594576\n",
      "Epoch: 14  Step:  41  training loss:  0.0022792425006628036\n",
      "Epoch: 14  Step:  42  training loss:  0.002817375585436821\n",
      "Epoch: 14  Step:  43  training loss:  0.0017578090773895383\n",
      "Epoch: 14  Step:  44  training loss:  0.003555208444595337\n",
      "Epoch: 14  Step:  45  training loss:  0.002471400424838066\n",
      "Epoch: 14  Step:  46  training loss:  0.002361690392717719\n",
      "Epoch: 14  Step:  47  training loss:  0.0016846837243065238\n",
      "Epoch: 14  Step:  48  training loss:  0.002245082287117839\n",
      "Epoch: 14  Step:  49  training loss:  0.003945376258343458\n",
      "Epoch: 14  Step:  50  training loss:  0.006739273201674223\n",
      "Epoch: 14  Step:  51  training loss:  0.0020956345833837986\n",
      "Epoch: 14  Step:  52  training loss:  0.0030768769793212414\n",
      "Epoch: 14  Step:  53  training loss:  0.003049402264878154\n",
      "Epoch: 14  Step:  54  training loss:  0.0019837403669953346\n",
      "Epoch: 14  Step:  55  training loss:  0.002783346688374877\n",
      "Epoch: 15  Step:  0  training loss:  0.0029484808910638094\n",
      "Epoch: 15  Step:  1  training loss:  0.0024932946544140577\n",
      "Epoch: 15  Step:  2  training loss:  0.0023086180444806814\n",
      "Epoch: 15  Step:  3  training loss:  0.002201935974881053\n",
      "Epoch: 15  Step:  4  training loss:  0.003172049531713128\n",
      "Epoch: 15  Step:  5  training loss:  0.001806881744414568\n",
      "Epoch: 15  Step:  6  training loss:  0.0030945646576583385\n",
      "Epoch: 15  Step:  7  training loss:  0.002167502185329795\n",
      "Epoch: 15  Step:  8  training loss:  0.0028260534163564444\n",
      "Epoch: 15  Step:  9  training loss:  0.0026142001152038574\n",
      "Epoch: 15  Step:  10  training loss:  0.0030152974650263786\n",
      "Epoch: 15  Step:  11  training loss:  0.003082082374021411\n",
      "Epoch: 15  Step:  12  training loss:  0.00557633675634861\n",
      "Epoch: 15  Step:  13  training loss:  0.0017894563497975469\n",
      "Epoch: 15  Step:  14  training loss:  0.001943766139447689\n",
      "Epoch: 15  Step:  15  training loss:  0.002986401552334428\n",
      "Epoch: 15  Step:  16  training loss:  0.0027057360857725143\n",
      "Epoch: 15  Step:  17  training loss:  0.002791009843349457\n",
      "Epoch: 15  Step:  18  training loss:  0.003917269874364138\n",
      "Epoch: 15  Step:  19  training loss:  0.003236480290070176\n",
      "Epoch: 15  Step:  20  training loss:  0.0016288284678012133\n",
      "Epoch: 15  Step:  21  training loss:  0.002110069850459695\n",
      "Epoch: 15  Step:  22  training loss:  0.005821152124553919\n",
      "Epoch: 15  Step:  23  training loss:  0.004234671127051115\n",
      "Epoch: 15  Step:  24  training loss:  0.0027873360086232424\n",
      "Epoch: 15  Step:  25  training loss:  0.0020453915931284428\n",
      "Epoch: 15  Step:  26  training loss:  0.0016723942244425416\n",
      "Epoch: 15  Step:  27  training loss:  0.003799932077527046\n",
      "Epoch: 15  Step:  28  training loss:  0.0020009749568998814\n",
      "Epoch: 15  Step:  29  training loss:  0.004113062750548124\n",
      "Epoch: 15  Step:  30  training loss:  0.002621393883600831\n",
      "Epoch: 15  Step:  31  training loss:  0.0020040813833475113\n",
      "Epoch: 15  Step:  32  training loss:  0.003103382885456085\n",
      "Epoch: 15  Step:  33  training loss:  0.0030943455640226603\n",
      "Epoch: 15  Step:  34  training loss:  0.003687557764351368\n",
      "Epoch: 15  Step:  35  training loss:  0.0021995846182107925\n",
      "Epoch: 15  Step:  36  training loss:  0.002022562548518181\n",
      "Epoch: 15  Step:  37  training loss:  0.0018883810844272375\n",
      "Epoch: 15  Step:  38  training loss:  0.0016974813770502806\n",
      "Epoch: 15  Step:  39  training loss:  0.0017744258511811495\n",
      "Epoch: 15  Step:  40  training loss:  0.0025422268081456423\n",
      "Epoch: 15  Step:  41  training loss:  0.00328750628978014\n",
      "Epoch: 15  Step:  42  training loss:  0.002249491633847356\n",
      "Epoch: 15  Step:  43  training loss:  0.0020909069571644068\n",
      "Epoch: 15  Step:  44  training loss:  0.002384761581197381\n",
      "Epoch: 15  Step:  45  training loss:  0.0026426520198583603\n",
      "Epoch: 15  Step:  46  training loss:  0.0022067225072532892\n",
      "Epoch: 15  Step:  47  training loss:  0.002261474961414933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15  Step:  48  training loss:  0.0023537343367934227\n",
      "Epoch: 15  Step:  49  training loss:  0.0032178061082959175\n",
      "Epoch: 15  Step:  50  training loss:  0.0019394881092011929\n",
      "Epoch: 15  Step:  51  training loss:  0.001891412539407611\n",
      "Epoch: 15  Step:  52  training loss:  0.008328999392688274\n",
      "Epoch: 15  Step:  53  training loss:  0.0029230332002043724\n",
      "Epoch: 15  Step:  54  training loss:  0.0021525619085878134\n",
      "Epoch: 15  Step:  55  training loss:  0.002258995547890663\n",
      "Epoch: 16  Step:  0  training loss:  0.0034727391321212053\n",
      "Epoch: 16  Step:  1  training loss:  0.0027744865510612726\n",
      "Epoch: 16  Step:  2  training loss:  0.0028061559423804283\n",
      "Epoch: 16  Step:  3  training loss:  0.002030694857239723\n",
      "Epoch: 16  Step:  4  training loss:  0.0031398290302604437\n",
      "Epoch: 16  Step:  5  training loss:  0.0028393480461090803\n",
      "Epoch: 16  Step:  6  training loss:  0.002007592935115099\n",
      "Epoch: 16  Step:  7  training loss:  0.0017952510388568044\n",
      "Epoch: 16  Step:  8  training loss:  0.0034951367415487766\n",
      "Epoch: 16  Step:  9  training loss:  0.001946660690009594\n",
      "Epoch: 16  Step:  10  training loss:  0.00210003275424242\n",
      "Epoch: 16  Step:  11  training loss:  0.0026596751995384693\n",
      "Epoch: 16  Step:  12  training loss:  0.002571221673861146\n",
      "Epoch: 16  Step:  13  training loss:  0.005793070420622826\n",
      "Epoch: 16  Step:  14  training loss:  0.004056987352669239\n",
      "Epoch: 16  Step:  15  training loss:  0.0023910198360681534\n",
      "Epoch: 16  Step:  16  training loss:  0.00288182171061635\n",
      "Epoch: 16  Step:  17  training loss:  0.002265684073790908\n",
      "Epoch: 16  Step:  18  training loss:  0.00242688599973917\n",
      "Epoch: 16  Step:  19  training loss:  0.002177888061851263\n",
      "Epoch: 16  Step:  20  training loss:  0.0023759447503834963\n",
      "Epoch: 16  Step:  21  training loss:  0.0023609050549566746\n",
      "Epoch: 16  Step:  22  training loss:  0.003213674994185567\n",
      "Epoch: 16  Step:  23  training loss:  0.0025765595491975546\n",
      "Epoch: 16  Step:  24  training loss:  0.0021257256157696247\n",
      "Epoch: 16  Step:  25  training loss:  0.002044396009296179\n",
      "Epoch: 16  Step:  26  training loss:  0.002692386507987976\n",
      "Epoch: 16  Step:  27  training loss:  0.002062888815999031\n",
      "Epoch: 16  Step:  28  training loss:  0.007458461448550224\n",
      "Epoch: 16  Step:  29  training loss:  0.0027803608682006598\n",
      "Epoch: 16  Step:  30  training loss:  0.002945006126537919\n",
      "Epoch: 16  Step:  31  training loss:  0.008077613078057766\n",
      "Epoch: 16  Step:  32  training loss:  0.0032002597581595182\n",
      "Epoch: 16  Step:  33  training loss:  0.0018225383246317506\n",
      "Epoch: 16  Step:  34  training loss:  0.002158855553716421\n",
      "Epoch: 16  Step:  35  training loss:  0.0025058123283088207\n",
      "Epoch: 16  Step:  36  training loss:  0.002275625243782997\n",
      "Epoch: 16  Step:  37  training loss:  0.0025218473747372627\n",
      "Epoch: 16  Step:  38  training loss:  0.0020555942319333553\n",
      "Epoch: 16  Step:  39  training loss:  0.002958748722448945\n",
      "Epoch: 16  Step:  40  training loss:  0.0022261086851358414\n",
      "Epoch: 16  Step:  41  training loss:  0.0023911241441965103\n",
      "Epoch: 16  Step:  42  training loss:  0.0024753264151513577\n",
      "Epoch: 16  Step:  43  training loss:  0.001959849614650011\n",
      "Epoch: 16  Step:  44  training loss:  0.0024348131846636534\n",
      "Epoch: 16  Step:  45  training loss:  0.0022629154846072197\n",
      "Epoch: 16  Step:  46  training loss:  0.002454356523230672\n",
      "Epoch: 16  Step:  47  training loss:  0.0021502976305782795\n",
      "Epoch: 16  Step:  48  training loss:  0.0023219226859509945\n",
      "Epoch: 16  Step:  49  training loss:  0.003059751121327281\n",
      "Epoch: 16  Step:  50  training loss:  0.002217425499111414\n",
      "Epoch: 16  Step:  51  training loss:  0.0021144940983504057\n",
      "Epoch: 16  Step:  52  training loss:  0.0023493582848459482\n",
      "Epoch: 16  Step:  53  training loss:  0.0029595866799354553\n",
      "Epoch: 16  Step:  54  training loss:  0.0036074731033295393\n",
      "Epoch: 16  Step:  55  training loss:  0.0028906960505992174\n",
      "Epoch: 17  Step:  0  training loss:  0.0021796671207994223\n",
      "Epoch: 17  Step:  1  training loss:  0.003048899583518505\n",
      "Epoch: 17  Step:  2  training loss:  0.002433453919366002\n",
      "Epoch: 17  Step:  3  training loss:  0.002532058395445347\n",
      "Epoch: 17  Step:  4  training loss:  0.0030085851904004812\n",
      "Epoch: 17  Step:  5  training loss:  0.0026991753838956356\n",
      "Epoch: 17  Step:  6  training loss:  0.008353798650205135\n",
      "Epoch: 17  Step:  7  training loss:  0.002179236151278019\n",
      "Epoch: 17  Step:  8  training loss:  0.002621730323880911\n",
      "Epoch: 17  Step:  9  training loss:  0.0019933078438043594\n",
      "Epoch: 17  Step:  10  training loss:  0.0022826271597296\n",
      "Epoch: 17  Step:  11  training loss:  0.0029239600989967585\n",
      "Epoch: 17  Step:  12  training loss:  0.0025066935922950506\n",
      "Epoch: 17  Step:  13  training loss:  0.0053923409432172775\n",
      "Epoch: 17  Step:  14  training loss:  0.002915108110755682\n",
      "Epoch: 17  Step:  15  training loss:  0.0030730620492249727\n",
      "Epoch: 17  Step:  16  training loss:  0.0017857893835753202\n",
      "Epoch: 17  Step:  17  training loss:  0.0025047403760254383\n",
      "Epoch: 17  Step:  18  training loss:  0.0020262582693248987\n",
      "Epoch: 17  Step:  19  training loss:  0.002982361940667033\n",
      "Epoch: 17  Step:  20  training loss:  0.0024076923727989197\n",
      "Epoch: 17  Step:  21  training loss:  0.0018638776382431388\n",
      "Epoch: 17  Step:  22  training loss:  0.0028948422987014055\n",
      "Epoch: 17  Step:  23  training loss:  0.0033584751654416323\n",
      "Epoch: 17  Step:  24  training loss:  0.003589047584682703\n",
      "Epoch: 17  Step:  25  training loss:  0.00252650398761034\n",
      "Epoch: 17  Step:  26  training loss:  0.002280078362673521\n",
      "Epoch: 17  Step:  27  training loss:  0.0026485177222639322\n",
      "Epoch: 17  Step:  28  training loss:  0.001896475674584508\n",
      "Epoch: 17  Step:  29  training loss:  0.0028275169897824526\n",
      "Epoch: 17  Step:  30  training loss:  0.002327348804101348\n",
      "Epoch: 17  Step:  31  training loss:  0.0021096011623740196\n",
      "Epoch: 17  Step:  32  training loss:  0.0027942187152802944\n",
      "Epoch: 17  Step:  33  training loss:  0.0021530077792704105\n",
      "Epoch: 17  Step:  34  training loss:  0.0031458798330277205\n",
      "Epoch: 17  Step:  35  training loss:  0.0020992413628846407\n",
      "Epoch: 17  Step:  36  training loss:  0.0023075996432453394\n",
      "Epoch: 17  Step:  37  training loss:  0.002170863561332226\n",
      "Epoch: 17  Step:  38  training loss:  0.0021415704395622015\n",
      "Epoch: 17  Step:  39  training loss:  0.0026399153284728527\n",
      "Epoch: 17  Step:  40  training loss:  0.002442440716549754\n",
      "Epoch: 17  Step:  41  training loss:  0.002847797004505992\n",
      "Epoch: 17  Step:  42  training loss:  0.0029112163465470076\n",
      "Epoch: 17  Step:  43  training loss:  0.0022786324843764305\n",
      "Epoch: 17  Step:  44  training loss:  0.002580938395112753\n",
      "Epoch: 17  Step:  45  training loss:  0.005888513755053282\n",
      "Epoch: 17  Step:  46  training loss:  0.002585889305919409\n",
      "Epoch: 17  Step:  47  training loss:  0.0020978664979338646\n",
      "Epoch: 17  Step:  48  training loss:  0.0026848814450204372\n",
      "Epoch: 17  Step:  49  training loss:  0.004041013307869434\n",
      "Epoch: 17  Step:  50  training loss:  0.0031634799670428038\n",
      "Epoch: 17  Step:  51  training loss:  0.001951400889083743\n",
      "Epoch: 17  Step:  52  training loss:  0.0026734883431345224\n",
      "Epoch: 17  Step:  53  training loss:  0.0026120678521692753\n",
      "Epoch: 17  Step:  54  training loss:  0.0022101751528680325\n",
      "Epoch: 17  Step:  55  training loss:  0.002526384312659502\n",
      "Epoch: 18  Step:  0  training loss:  0.002621824387460947\n",
      "Epoch: 18  Step:  1  training loss:  0.002263119909912348\n",
      "Epoch: 18  Step:  2  training loss:  0.002166836289688945\n",
      "Epoch: 18  Step:  3  training loss:  0.0021432333160191774\n",
      "Epoch: 18  Step:  4  training loss:  0.003970174118876457\n",
      "Epoch: 18  Step:  5  training loss:  0.0024292408488690853\n",
      "Epoch: 18  Step:  6  training loss:  0.0032402530778199434\n",
      "Epoch: 18  Step:  7  training loss:  0.0024534729309380054\n",
      "Epoch: 18  Step:  8  training loss:  0.0024920860305428505\n",
      "Epoch: 18  Step:  9  training loss:  0.0028543006628751755\n",
      "Epoch: 18  Step:  10  training loss:  0.0028764076996594667\n",
      "Epoch: 18  Step:  11  training loss:  0.0024617540184408426\n",
      "Epoch: 18  Step:  12  training loss:  0.0029865719843655825\n",
      "Epoch: 18  Step:  13  training loss:  0.003071058774366975\n",
      "Epoch: 18  Step:  14  training loss:  0.002521432936191559\n",
      "Epoch: 18  Step:  15  training loss:  0.0024463494773954153\n",
      "Epoch: 18  Step:  16  training loss:  0.002108596032485366\n",
      "Epoch: 18  Step:  17  training loss:  0.0017740853363648057\n",
      "Epoch: 18  Step:  18  training loss:  0.0036637848243117332\n",
      "Epoch: 18  Step:  19  training loss:  0.002415119670331478\n",
      "Epoch: 18  Step:  20  training loss:  0.0030050731729716063\n",
      "Epoch: 18  Step:  21  training loss:  0.0026955432258546352\n",
      "Epoch: 18  Step:  22  training loss:  0.0024200663901865482\n",
      "Epoch: 18  Step:  23  training loss:  0.0026120408438146114\n",
      "Epoch: 18  Step:  24  training loss:  0.002110453089699149\n",
      "Epoch: 18  Step:  25  training loss:  0.002987942658364773\n",
      "Epoch: 18  Step:  26  training loss:  0.0027321905363351107\n",
      "Epoch: 18  Step:  27  training loss:  0.0023508681915700436\n",
      "Epoch: 18  Step:  28  training loss:  0.0017970174085348845\n",
      "Epoch: 18  Step:  29  training loss:  0.00618828134611249\n",
      "Epoch: 18  Step:  30  training loss:  0.0026075318455696106\n",
      "Epoch: 18  Step:  31  training loss:  0.0027327369898557663\n",
      "Epoch: 18  Step:  32  training loss:  0.0023180488497018814\n",
      "Epoch: 18  Step:  33  training loss:  0.0056712087243795395\n",
      "Epoch: 18  Step:  34  training loss:  0.002511517610400915\n",
      "Epoch: 18  Step:  35  training loss:  0.0018408047035336494\n",
      "Epoch: 18  Step:  36  training loss:  0.002627190900966525\n",
      "Epoch: 18  Step:  37  training loss:  0.002477882895618677\n",
      "Epoch: 18  Step:  38  training loss:  0.0031578936614096165\n",
      "Epoch: 18  Step:  39  training loss:  0.0020446262788027525\n",
      "Epoch: 18  Step:  40  training loss:  0.002722156932577491\n",
      "Epoch: 18  Step:  41  training loss:  0.0029786729719489813\n",
      "Epoch: 18  Step:  42  training loss:  0.0024168260861188173\n",
      "Epoch: 18  Step:  43  training loss:  0.0022195028141140938\n",
      "Epoch: 18  Step:  44  training loss:  0.001800150377675891\n",
      "Epoch: 18  Step:  45  training loss:  0.008964763954281807\n",
      "Epoch: 18  Step:  46  training loss:  0.0019980079960078\n",
      "Epoch: 18  Step:  47  training loss:  0.0023154495283961296\n",
      "Epoch: 18  Step:  48  training loss:  0.0021922048181295395\n",
      "Epoch: 18  Step:  49  training loss:  0.002233959035947919\n",
      "Epoch: 18  Step:  50  training loss:  0.0018484335159882903\n",
      "Epoch: 18  Step:  51  training loss:  0.0020686860661953688\n",
      "Epoch: 18  Step:  52  training loss:  0.0025282984133809805\n",
      "Epoch: 18  Step:  53  training loss:  0.0028878820594400167\n",
      "Epoch: 18  Step:  54  training loss:  0.0028228885494172573\n",
      "Epoch: 18  Step:  55  training loss:  0.003412853693589568\n",
      "Epoch: 19  Step:  0  training loss:  0.0026513319462537766\n",
      "Epoch: 19  Step:  1  training loss:  0.0025800601579248905\n",
      "Epoch: 19  Step:  2  training loss:  0.0029582201968878508\n",
      "Epoch: 19  Step:  3  training loss:  0.0031095058657228947\n",
      "Epoch: 19  Step:  4  training loss:  0.007928479462862015\n",
      "Epoch: 19  Step:  5  training loss:  0.002343702595680952\n",
      "Epoch: 19  Step:  6  training loss:  0.003152324818074703\n",
      "Epoch: 19  Step:  7  training loss:  0.002110380446538329\n",
      "Epoch: 19  Step:  8  training loss:  0.0021075382828712463\n",
      "Epoch: 19  Step:  9  training loss:  0.0027779238298535347\n",
      "Epoch: 19  Step:  10  training loss:  0.003190232440829277\n",
      "Epoch: 19  Step:  11  training loss:  0.0024319754447788\n",
      "Epoch: 19  Step:  12  training loss:  0.0058232564479112625\n",
      "Epoch: 19  Step:  13  training loss:  0.002584664151072502\n",
      "Epoch: 19  Step:  14  training loss:  0.0028092162683606148\n",
      "Epoch: 19  Step:  15  training loss:  0.001827243366278708\n",
      "Epoch: 19  Step:  16  training loss:  0.0029377052560448647\n",
      "Epoch: 19  Step:  17  training loss:  0.0024607693776488304\n",
      "Epoch: 19  Step:  18  training loss:  0.0020028611179441214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19  Step:  19  training loss:  0.002485483419150114\n",
      "Epoch: 19  Step:  20  training loss:  0.0022862206678837538\n",
      "Epoch: 19  Step:  21  training loss:  0.0024279390927404165\n",
      "Epoch: 19  Step:  22  training loss:  0.002008653711527586\n",
      "Epoch: 19  Step:  23  training loss:  0.002114620991051197\n",
      "Epoch: 19  Step:  24  training loss:  0.0019408927764743567\n",
      "Epoch: 19  Step:  25  training loss:  0.0031113869044929743\n",
      "Epoch: 19  Step:  26  training loss:  0.002616496291011572\n",
      "Epoch: 19  Step:  27  training loss:  0.00239726435393095\n",
      "Epoch: 19  Step:  28  training loss:  0.0021491744555532932\n",
      "Epoch: 19  Step:  29  training loss:  0.0020947821903973818\n",
      "Epoch: 19  Step:  30  training loss:  0.00235526985488832\n",
      "Epoch: 19  Step:  31  training loss:  0.0024839676916599274\n",
      "Epoch: 19  Step:  32  training loss:  0.0025912874843925238\n",
      "Epoch: 19  Step:  33  training loss:  0.0030254933517426252\n",
      "Epoch: 19  Step:  34  training loss:  0.002777143381536007\n",
      "Epoch: 19  Step:  35  training loss:  0.002318295883014798\n",
      "Epoch: 19  Step:  36  training loss:  0.003173207398504019\n",
      "Epoch: 19  Step:  37  training loss:  0.0027179259341210127\n",
      "Epoch: 19  Step:  38  training loss:  0.002805211814120412\n",
      "Epoch: 19  Step:  39  training loss:  0.0028398388531059027\n",
      "Epoch: 19  Step:  40  training loss:  0.002334465505555272\n",
      "Epoch: 19  Step:  41  training loss:  0.003123224712908268\n",
      "Epoch: 19  Step:  42  training loss:  0.0025335305836051702\n",
      "Epoch: 19  Step:  43  training loss:  0.0023370583076030016\n",
      "Epoch: 19  Step:  44  training loss:  0.0025508201215416193\n",
      "Epoch: 19  Step:  45  training loss:  0.0020958445966243744\n",
      "Epoch: 19  Step:  46  training loss:  0.002244284376502037\n",
      "Epoch: 19  Step:  47  training loss:  0.002225541975349188\n",
      "Epoch: 19  Step:  48  training loss:  0.002669587032869458\n",
      "Epoch: 19  Step:  49  training loss:  0.002988662337884307\n",
      "Epoch: 19  Step:  50  training loss:  0.0059194802306592464\n",
      "Epoch: 19  Step:  51  training loss:  0.0028026560321450233\n",
      "Epoch: 19  Step:  52  training loss:  0.002350436057895422\n",
      "Epoch: 19  Step:  53  training loss:  0.0023372257128357887\n",
      "Epoch: 19  Step:  54  training loss:  0.002577377948909998\n",
      "Epoch: 19  Step:  55  training loss:  0.0030565967317670584\n",
      "Epoch: 20  Step:  0  training loss:  0.0032016595359891653\n",
      "Epoch: 20  Step:  1  training loss:  0.0027792928740382195\n",
      "Epoch: 20  Step:  2  training loss:  0.002267824951559305\n",
      "Epoch: 20  Step:  3  training loss:  0.002161996439099312\n",
      "Epoch: 20  Step:  4  training loss:  0.002440208801999688\n",
      "Epoch: 20  Step:  5  training loss:  0.0026772157289087772\n",
      "Epoch: 20  Step:  6  training loss:  0.0022871075198054314\n",
      "Epoch: 20  Step:  7  training loss:  0.002600660314783454\n",
      "Epoch: 20  Step:  8  training loss:  0.0029755481518805027\n",
      "Epoch: 20  Step:  9  training loss:  0.0027515750844031572\n",
      "Epoch: 20  Step:  10  training loss:  0.002158311428502202\n",
      "Epoch: 20  Step:  11  training loss:  0.0021499490831047297\n",
      "Epoch: 20  Step:  12  training loss:  0.0027284356765449047\n",
      "Epoch: 20  Step:  13  training loss:  0.002301126951351762\n",
      "Epoch: 20  Step:  14  training loss:  0.0020788987167179585\n",
      "Epoch: 20  Step:  15  training loss:  0.002353111281991005\n",
      "Epoch: 20  Step:  16  training loss:  0.002162731485441327\n",
      "Epoch: 20  Step:  17  training loss:  0.0023653923999518156\n",
      "Epoch: 20  Step:  18  training loss:  0.0018293843604624271\n",
      "Epoch: 20  Step:  19  training loss:  0.0031329868361353874\n",
      "Epoch: 20  Step:  20  training loss:  0.0030230653937906027\n",
      "Epoch: 20  Step:  21  training loss:  0.002078321063891053\n",
      "Epoch: 20  Step:  22  training loss:  0.0031976455356925726\n",
      "Epoch: 20  Step:  23  training loss:  0.0027929097414016724\n",
      "Epoch: 20  Step:  24  training loss:  0.002172498032450676\n",
      "Epoch: 20  Step:  25  training loss:  0.002449161373078823\n",
      "Epoch: 20  Step:  26  training loss:  0.003675231011584401\n",
      "Epoch: 20  Step:  27  training loss:  0.002334963995963335\n",
      "Epoch: 20  Step:  28  training loss:  0.0026684189215302467\n",
      "Epoch: 20  Step:  29  training loss:  0.0018391036428511143\n",
      "Epoch: 20  Step:  30  training loss:  0.0023355300072580576\n",
      "Epoch: 20  Step:  31  training loss:  0.002395589603111148\n",
      "Epoch: 20  Step:  32  training loss:  0.0022624258417636156\n",
      "Epoch: 20  Step:  33  training loss:  0.002260294510051608\n",
      "Epoch: 20  Step:  34  training loss:  0.0022902253549546003\n",
      "Epoch: 20  Step:  35  training loss:  0.004852834157645702\n",
      "Epoch: 20  Step:  36  training loss:  0.00375218759290874\n",
      "Epoch: 20  Step:  37  training loss:  0.0017688258085399866\n",
      "Epoch: 20  Step:  38  training loss:  0.002359234495088458\n",
      "Epoch: 20  Step:  39  training loss:  0.0026135235093533993\n",
      "Epoch: 20  Step:  40  training loss:  0.002756500616669655\n",
      "Epoch: 20  Step:  41  training loss:  0.0021480261348187923\n",
      "Epoch: 20  Step:  42  training loss:  0.0027055907994508743\n",
      "Epoch: 20  Step:  43  training loss:  0.003324979916214943\n",
      "Epoch: 20  Step:  44  training loss:  0.0021040751598775387\n",
      "Epoch: 20  Step:  45  training loss:  0.0029543102718889713\n",
      "Epoch: 20  Step:  46  training loss:  0.0027417740784585476\n",
      "Epoch: 20  Step:  47  training loss:  0.002809790661558509\n",
      "Epoch: 20  Step:  48  training loss:  0.0022796925622969866\n",
      "Epoch: 20  Step:  49  training loss:  0.006019837222993374\n",
      "Epoch: 20  Step:  50  training loss:  0.0034052624832838774\n",
      "Epoch: 20  Step:  51  training loss:  0.002972567919641733\n",
      "Epoch: 20  Step:  52  training loss:  0.0019594631157815456\n",
      "Epoch: 20  Step:  53  training loss:  0.0025360933504998684\n",
      "Epoch: 20  Step:  54  training loss:  0.0018815520452335477\n",
      "Epoch: 20  Step:  55  training loss:  0.018270138651132584\n",
      "Epoch: 21  Step:  0  training loss:  0.002299914602190256\n",
      "Epoch: 21  Step:  1  training loss:  0.002882710425183177\n",
      "Epoch: 21  Step:  2  training loss:  0.003028539475053549\n",
      "Epoch: 21  Step:  3  training loss:  0.002784539945423603\n",
      "Epoch: 21  Step:  4  training loss:  0.0015473797684535384\n",
      "Epoch: 21  Step:  5  training loss:  0.002912322524935007\n",
      "Epoch: 21  Step:  6  training loss:  0.0019435037393122911\n",
      "Epoch: 21  Step:  7  training loss:  0.0025042288471013308\n",
      "Epoch: 21  Step:  8  training loss:  0.0031497774180024862\n",
      "Epoch: 21  Step:  9  training loss:  0.0018734573386609554\n",
      "Epoch: 21  Step:  10  training loss:  0.002252915408462286\n",
      "Epoch: 21  Step:  11  training loss:  0.0028371994849294424\n",
      "Epoch: 21  Step:  12  training loss:  0.0019222879782319069\n",
      "Epoch: 21  Step:  13  training loss:  0.0026076114736497402\n",
      "Epoch: 21  Step:  14  training loss:  0.0027743943501263857\n",
      "Epoch: 21  Step:  15  training loss:  0.002355402335524559\n",
      "Epoch: 21  Step:  16  training loss:  0.0026233699172735214\n",
      "Epoch: 21  Step:  17  training loss:  0.0029809391126036644\n",
      "Epoch: 21  Step:  18  training loss:  0.0030555082485079765\n",
      "Epoch: 21  Step:  19  training loss:  0.0022949716076254845\n",
      "Epoch: 21  Step:  20  training loss:  0.0028753643855452538\n",
      "Epoch: 21  Step:  21  training loss:  0.0020121592096984386\n",
      "Epoch: 21  Step:  22  training loss:  0.0024907777551561594\n",
      "Epoch: 21  Step:  23  training loss:  0.008043701760470867\n",
      "Epoch: 21  Step:  24  training loss:  0.002314037410542369\n",
      "Epoch: 21  Step:  25  training loss:  0.004153715446591377\n",
      "Epoch: 21  Step:  26  training loss:  0.0020423613023012877\n",
      "Epoch: 21  Step:  27  training loss:  0.003038432216271758\n",
      "Epoch: 21  Step:  28  training loss:  0.0023456395138055086\n",
      "Epoch: 21  Step:  29  training loss:  0.00548382755368948\n",
      "Epoch: 21  Step:  30  training loss:  0.0025046730879694223\n",
      "Epoch: 21  Step:  31  training loss:  0.002091196831315756\n",
      "Epoch: 21  Step:  32  training loss:  0.002395607763901353\n",
      "Epoch: 21  Step:  33  training loss:  0.002854784717783332\n",
      "Epoch: 21  Step:  34  training loss:  0.002155642258003354\n",
      "Epoch: 21  Step:  35  training loss:  0.005068833939731121\n",
      "Epoch: 21  Step:  36  training loss:  0.0035185583401471376\n",
      "Epoch: 21  Step:  37  training loss:  0.0018737418577075005\n",
      "Epoch: 21  Step:  38  training loss:  0.0025134170427918434\n",
      "Epoch: 21  Step:  39  training loss:  0.0023731798864901066\n",
      "Epoch: 21  Step:  40  training loss:  0.002979539567604661\n",
      "Epoch: 21  Step:  41  training loss:  0.0021305305417627096\n",
      "Epoch: 21  Step:  42  training loss:  0.0025169115979224443\n",
      "Epoch: 21  Step:  43  training loss:  0.00291892746463418\n",
      "Epoch: 21  Step:  44  training loss:  0.002670638496056199\n",
      "Epoch: 21  Step:  45  training loss:  0.0022424778435379267\n",
      "Epoch: 21  Step:  46  training loss:  0.002923480933532119\n",
      "Epoch: 21  Step:  47  training loss:  0.0028471939731389284\n",
      "Epoch: 21  Step:  48  training loss:  0.0023964503780007362\n",
      "Epoch: 21  Step:  49  training loss:  0.0020447750575840473\n",
      "Epoch: 21  Step:  50  training loss:  0.002693673362955451\n",
      "Epoch: 21  Step:  51  training loss:  0.001929668360389769\n",
      "Epoch: 21  Step:  52  training loss:  0.00268546212464571\n",
      "Epoch: 21  Step:  53  training loss:  0.0031375291291624308\n",
      "Epoch: 21  Step:  54  training loss:  0.002355918986722827\n",
      "Epoch: 21  Step:  55  training loss:  0.002411981811746955\n",
      "Epoch: 22  Step:  0  training loss:  0.0021493202075362206\n",
      "Epoch: 22  Step:  1  training loss:  0.0022918926551938057\n",
      "Epoch: 22  Step:  2  training loss:  0.0027711670845746994\n",
      "Epoch: 22  Step:  3  training loss:  0.002235821448266506\n",
      "Epoch: 22  Step:  4  training loss:  0.002201722003519535\n",
      "Epoch: 22  Step:  5  training loss:  0.0024459450505673885\n",
      "Epoch: 22  Step:  6  training loss:  0.0027075877878814936\n",
      "Epoch: 22  Step:  7  training loss:  0.003777491394430399\n",
      "Epoch: 22  Step:  8  training loss:  0.0022683637216687202\n",
      "Epoch: 22  Step:  9  training loss:  0.0057255858555436134\n",
      "Epoch: 22  Step:  10  training loss:  0.0028096348978579044\n",
      "Epoch: 22  Step:  11  training loss:  0.003076668130233884\n",
      "Epoch: 22  Step:  12  training loss:  0.0029229053761810064\n",
      "Epoch: 22  Step:  13  training loss:  0.0025313985534012318\n",
      "Epoch: 22  Step:  14  training loss:  0.0020338038448244333\n",
      "Epoch: 22  Step:  15  training loss:  0.0022095139138400555\n",
      "Epoch: 22  Step:  16  training loss:  0.0027299588546156883\n",
      "Epoch: 22  Step:  17  training loss:  0.0021996106952428818\n",
      "Epoch: 22  Step:  18  training loss:  0.0028435802087187767\n",
      "Epoch: 22  Step:  19  training loss:  0.008699217811226845\n",
      "Epoch: 22  Step:  20  training loss:  0.0027473466470837593\n",
      "Epoch: 22  Step:  21  training loss:  0.002285091672092676\n",
      "Epoch: 22  Step:  22  training loss:  0.0022134368773549795\n",
      "Epoch: 22  Step:  23  training loss:  0.0020343149080872536\n",
      "Epoch: 22  Step:  24  training loss:  0.003906343597918749\n",
      "Epoch: 22  Step:  25  training loss:  0.001964652445167303\n",
      "Epoch: 22  Step:  26  training loss:  0.005511818453669548\n",
      "Epoch: 22  Step:  27  training loss:  0.0023395472671836615\n",
      "Epoch: 22  Step:  28  training loss:  0.0023505408316850662\n",
      "Epoch: 22  Step:  29  training loss:  0.002268951153382659\n",
      "Epoch: 22  Step:  30  training loss:  0.0018902347655966878\n",
      "Epoch: 22  Step:  31  training loss:  0.0029157670214772224\n",
      "Epoch: 22  Step:  32  training loss:  0.0026887347921729088\n",
      "Epoch: 22  Step:  33  training loss:  0.0031389526557177305\n",
      "Epoch: 22  Step:  34  training loss:  0.002358352532610297\n",
      "Epoch: 22  Step:  35  training loss:  0.0028056674636900425\n",
      "Epoch: 22  Step:  36  training loss:  0.0029102517291903496\n",
      "Epoch: 22  Step:  37  training loss:  0.0025953680742532015\n",
      "Epoch: 22  Step:  38  training loss:  0.0024626199156045914\n",
      "Epoch: 22  Step:  39  training loss:  0.002585098147392273\n",
      "Epoch: 22  Step:  40  training loss:  0.002084306674078107\n",
      "Epoch: 22  Step:  41  training loss:  0.0021114295814186335\n",
      "Epoch: 22  Step:  42  training loss:  0.0027617013547569513\n",
      "Epoch: 22  Step:  43  training loss:  0.003262419020757079\n",
      "Epoch: 22  Step:  44  training loss:  0.0023430720902979374\n",
      "Epoch: 22  Step:  45  training loss:  0.002823564689606428\n",
      "Epoch: 22  Step:  46  training loss:  0.0020100369583815336\n",
      "Epoch: 22  Step:  47  training loss:  0.0017269294476136565\n",
      "Epoch: 22  Step:  48  training loss:  0.0022458627354353666\n",
      "Epoch: 22  Step:  49  training loss:  0.002463672077283263\n",
      "Epoch: 22  Step:  50  training loss:  0.0022024493664503098\n",
      "Epoch: 22  Step:  51  training loss:  0.002584559377282858\n",
      "Epoch: 22  Step:  52  training loss:  0.0026718061417341232\n",
      "Epoch: 22  Step:  53  training loss:  0.002629417460411787\n",
      "Epoch: 22  Step:  54  training loss:  0.002297239378094673\n",
      "Epoch: 22  Step:  55  training loss:  0.002613549353554845\n",
      "Epoch: 23  Step:  0  training loss:  0.0023046457208693027\n",
      "Epoch: 23  Step:  1  training loss:  0.0024679554626345634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23  Step:  2  training loss:  0.0025671408511698246\n",
      "Epoch: 23  Step:  3  training loss:  0.002543604001402855\n",
      "Epoch: 23  Step:  4  training loss:  0.005031692795455456\n",
      "Epoch: 23  Step:  5  training loss:  0.003235416952520609\n",
      "Epoch: 23  Step:  6  training loss:  0.0025680148974061012\n",
      "Epoch: 23  Step:  7  training loss:  0.0030415253713726997\n",
      "Epoch: 23  Step:  8  training loss:  0.0032087001018226147\n",
      "Epoch: 23  Step:  9  training loss:  0.0035331568215042353\n",
      "Epoch: 23  Step:  10  training loss:  0.001807070104405284\n",
      "Epoch: 23  Step:  11  training loss:  0.002690173452720046\n",
      "Epoch: 23  Step:  12  training loss:  0.0019258154788985848\n",
      "Epoch: 23  Step:  13  training loss:  0.0025249761529266834\n",
      "Epoch: 23  Step:  14  training loss:  0.0022590074222534895\n",
      "Epoch: 23  Step:  15  training loss:  0.0018841278506442904\n",
      "Epoch: 23  Step:  16  training loss:  0.0031545122619718313\n",
      "Epoch: 23  Step:  17  training loss:  0.0027133142575621605\n",
      "Epoch: 23  Step:  18  training loss:  0.002395464340224862\n",
      "Epoch: 23  Step:  19  training loss:  0.002884047105908394\n",
      "Epoch: 23  Step:  20  training loss:  0.0019383463077247143\n",
      "Epoch: 23  Step:  21  training loss:  0.0024279789067804813\n",
      "Epoch: 23  Step:  22  training loss:  0.002715302398428321\n",
      "Epoch: 23  Step:  23  training loss:  0.002563708694651723\n",
      "Epoch: 23  Step:  24  training loss:  0.002310953801497817\n",
      "Epoch: 23  Step:  25  training loss:  0.0023262035101652145\n",
      "Epoch: 23  Step:  26  training loss:  0.002871772041544318\n",
      "Epoch: 23  Step:  27  training loss:  0.002744107274338603\n",
      "Epoch: 23  Step:  28  training loss:  0.002475881716236472\n",
      "Epoch: 23  Step:  29  training loss:  0.00237655988894403\n",
      "Epoch: 23  Step:  30  training loss:  0.0022826949134469032\n",
      "Epoch: 23  Step:  31  training loss:  0.0022632626350969076\n",
      "Epoch: 23  Step:  32  training loss:  0.0023878549691289663\n",
      "Epoch: 23  Step:  33  training loss:  0.0027564233168959618\n",
      "Epoch: 23  Step:  34  training loss:  0.0023617870174348354\n",
      "Epoch: 23  Step:  35  training loss:  0.0026321380864828825\n",
      "Epoch: 23  Step:  36  training loss:  0.0029422640800476074\n",
      "Epoch: 23  Step:  37  training loss:  0.0032590965274721384\n",
      "Epoch: 23  Step:  38  training loss:  0.0024891840294003487\n",
      "Epoch: 23  Step:  39  training loss:  0.0030140546150505543\n",
      "Epoch: 23  Step:  40  training loss:  0.0026521100662648678\n",
      "Epoch: 23  Step:  41  training loss:  0.0018301610834896564\n",
      "Epoch: 23  Step:  42  training loss:  0.005589957814663649\n",
      "Epoch: 23  Step:  43  training loss:  0.0020283148624002934\n",
      "Epoch: 23  Step:  44  training loss:  0.0017754633445292711\n",
      "Epoch: 23  Step:  45  training loss:  0.009652595035731792\n",
      "Epoch: 23  Step:  46  training loss:  0.002623249776661396\n",
      "Epoch: 23  Step:  47  training loss:  0.0030709803104400635\n",
      "Epoch: 23  Step:  48  training loss:  0.0024097443092614412\n",
      "Epoch: 23  Step:  49  training loss:  0.0023454297333955765\n",
      "Epoch: 23  Step:  50  training loss:  0.002382009057328105\n",
      "Epoch: 23  Step:  51  training loss:  0.0021802387200295925\n",
      "Epoch: 23  Step:  52  training loss:  0.0025236490182578564\n",
      "Epoch: 23  Step:  53  training loss:  0.00198822608217597\n",
      "Epoch: 23  Step:  54  training loss:  0.0019791789818555117\n",
      "Epoch: 23  Step:  55  training loss:  0.0019409494707360864\n",
      "Epoch: 24  Step:  0  training loss:  0.0030195903964340687\n",
      "Epoch: 24  Step:  1  training loss:  0.0038024557288736105\n",
      "Epoch: 24  Step:  2  training loss:  0.002693973481655121\n",
      "Epoch: 24  Step:  3  training loss:  0.008216433227062225\n",
      "Epoch: 24  Step:  4  training loss:  0.0023429759312421083\n",
      "Epoch: 24  Step:  5  training loss:  0.0019878321327269077\n",
      "Epoch: 24  Step:  6  training loss:  0.002502295421436429\n",
      "Epoch: 24  Step:  7  training loss:  0.0028599118813872337\n",
      "Epoch: 24  Step:  8  training loss:  0.0028436502907425165\n",
      "Epoch: 24  Step:  9  training loss:  0.001991034485399723\n",
      "Epoch: 24  Step:  10  training loss:  0.002718043513596058\n",
      "Epoch: 24  Step:  11  training loss:  0.002164471661671996\n",
      "Epoch: 24  Step:  12  training loss:  0.0020538305398076773\n",
      "Epoch: 24  Step:  13  training loss:  0.0029208308551460505\n",
      "Epoch: 24  Step:  14  training loss:  0.0018830898916348815\n",
      "Epoch: 24  Step:  15  training loss:  0.0023961495608091354\n",
      "Epoch: 24  Step:  16  training loss:  0.0024098260328173637\n",
      "Epoch: 24  Step:  17  training loss:  0.0019255264196544886\n",
      "Epoch: 24  Step:  18  training loss:  0.004321082029491663\n",
      "Epoch: 24  Step:  19  training loss:  0.002219193149358034\n",
      "Epoch: 24  Step:  20  training loss:  0.002028660848736763\n",
      "Epoch: 24  Step:  21  training loss:  0.0031013458501547575\n",
      "Epoch: 24  Step:  22  training loss:  0.0028396500274538994\n",
      "Epoch: 24  Step:  23  training loss:  0.002747311256825924\n",
      "Epoch: 24  Step:  24  training loss:  0.002329667564481497\n",
      "Epoch: 24  Step:  25  training loss:  0.0025434724520891905\n",
      "Epoch: 24  Step:  26  training loss:  0.002536711050197482\n",
      "Epoch: 24  Step:  27  training loss:  0.0018198362085968256\n",
      "Epoch: 24  Step:  28  training loss:  0.0020197622943669558\n",
      "Epoch: 24  Step:  29  training loss:  0.0020372061990201473\n",
      "Epoch: 24  Step:  30  training loss:  0.001527058077044785\n",
      "Epoch: 24  Step:  31  training loss:  0.005351581610739231\n",
      "Epoch: 24  Step:  32  training loss:  0.0018978186417371035\n",
      "Epoch: 24  Step:  33  training loss:  0.002232461469247937\n",
      "Epoch: 24  Step:  34  training loss:  0.0034228928852826357\n",
      "Epoch: 24  Step:  35  training loss:  0.0031565935350954533\n",
      "Epoch: 24  Step:  36  training loss:  0.00227548205293715\n",
      "Epoch: 24  Step:  37  training loss:  0.00297397468239069\n",
      "Epoch: 24  Step:  38  training loss:  0.0030130220111459494\n",
      "Epoch: 24  Step:  39  training loss:  0.006849322933703661\n",
      "Epoch: 24  Step:  40  training loss:  0.0017496737418696284\n",
      "Epoch: 24  Step:  41  training loss:  0.0019701486453413963\n",
      "Epoch: 24  Step:  42  training loss:  0.0024337361101061106\n",
      "Epoch: 24  Step:  43  training loss:  0.003009356325492263\n",
      "Epoch: 24  Step:  44  training loss:  0.002228210214525461\n",
      "Epoch: 24  Step:  45  training loss:  0.003946161828935146\n",
      "Epoch: 24  Step:  46  training loss:  0.002310987329110503\n",
      "Epoch: 24  Step:  47  training loss:  0.0026776278391480446\n",
      "Epoch: 24  Step:  48  training loss:  0.0022034859284758568\n",
      "Epoch: 24  Step:  49  training loss:  0.002442923141643405\n",
      "Epoch: 24  Step:  50  training loss:  0.001890357118099928\n",
      "Epoch: 24  Step:  51  training loss:  0.0024656024761497974\n",
      "Epoch: 24  Step:  52  training loss:  0.002974509261548519\n",
      "Epoch: 24  Step:  53  training loss:  0.001995950471609831\n",
      "Epoch: 24  Step:  54  training loss:  0.002410284010693431\n",
      "Epoch: 24  Step:  55  training loss:  0.0015680270735174417\n",
      "Epoch: 25  Step:  0  training loss:  0.0026935094501823187\n",
      "Epoch: 25  Step:  1  training loss:  0.003006523009389639\n",
      "Epoch: 25  Step:  2  training loss:  0.0018514783587306738\n",
      "Epoch: 25  Step:  3  training loss:  0.0032080872915685177\n",
      "Epoch: 25  Step:  4  training loss:  0.00232204282656312\n",
      "Epoch: 25  Step:  5  training loss:  0.0020357188768684864\n",
      "Epoch: 25  Step:  6  training loss:  0.0033320458605885506\n",
      "Epoch: 25  Step:  7  training loss:  0.0018182559870183468\n",
      "Epoch: 25  Step:  8  training loss:  0.0028728297911584377\n",
      "Epoch: 25  Step:  9  training loss:  0.0021216217428445816\n",
      "Epoch: 25  Step:  10  training loss:  0.002655365504324436\n",
      "Epoch: 25  Step:  11  training loss:  0.003234461648389697\n",
      "Epoch: 25  Step:  12  training loss:  0.0020769720431417227\n",
      "Epoch: 25  Step:  13  training loss:  0.0023490122985094786\n",
      "Epoch: 25  Step:  14  training loss:  0.002210486913099885\n",
      "Epoch: 25  Step:  15  training loss:  0.002772877225652337\n",
      "Epoch: 25  Step:  16  training loss:  0.0024085009936243296\n",
      "Epoch: 25  Step:  17  training loss:  0.0019840425811707973\n",
      "Epoch: 25  Step:  18  training loss:  0.002012306824326515\n",
      "Epoch: 25  Step:  19  training loss:  0.002363271778449416\n",
      "Epoch: 25  Step:  20  training loss:  0.0018971454119309783\n",
      "Epoch: 25  Step:  21  training loss:  0.0022488264366984367\n",
      "Epoch: 25  Step:  22  training loss:  0.002608956303447485\n",
      "Epoch: 25  Step:  23  training loss:  0.0021170570980757475\n",
      "Epoch: 25  Step:  24  training loss:  0.0025277265813201666\n",
      "Epoch: 25  Step:  25  training loss:  0.004316012840718031\n",
      "Epoch: 25  Step:  26  training loss:  0.002776628825813532\n",
      "Epoch: 25  Step:  27  training loss:  0.005444860085844994\n",
      "Epoch: 25  Step:  28  training loss:  0.0021852285135537386\n",
      "Epoch: 25  Step:  29  training loss:  0.0024570298846811056\n",
      "Epoch: 25  Step:  30  training loss:  0.002514441264793277\n",
      "Epoch: 25  Step:  31  training loss:  0.0024089831858873367\n",
      "Epoch: 25  Step:  32  training loss:  0.0024660779163241386\n",
      "Epoch: 25  Step:  33  training loss:  0.0021109506487846375\n",
      "Epoch: 25  Step:  34  training loss:  0.0019890316762030125\n",
      "Epoch: 25  Step:  35  training loss:  0.008158157579600811\n",
      "Epoch: 25  Step:  36  training loss:  0.002701494609937072\n",
      "Epoch: 25  Step:  37  training loss:  0.0026703455951064825\n",
      "Epoch: 25  Step:  38  training loss:  0.003930828534066677\n",
      "Epoch: 25  Step:  39  training loss:  0.0027827660087496042\n",
      "Epoch: 25  Step:  40  training loss:  0.002378670498728752\n",
      "Epoch: 25  Step:  41  training loss:  0.0025824913755059242\n",
      "Epoch: 25  Step:  42  training loss:  0.002413860522210598\n",
      "Epoch: 25  Step:  43  training loss:  0.002659162040799856\n",
      "Epoch: 25  Step:  44  training loss:  0.0029776347801089287\n",
      "Epoch: 25  Step:  45  training loss:  0.001617373782210052\n",
      "Epoch: 25  Step:  46  training loss:  0.0033453986980021\n",
      "Epoch: 25  Step:  47  training loss:  0.003073330270126462\n",
      "Epoch: 25  Step:  48  training loss:  0.0027372499462217093\n",
      "Epoch: 25  Step:  49  training loss:  0.002601595828309655\n",
      "Epoch: 25  Step:  50  training loss:  0.0023502653930336237\n",
      "Epoch: 25  Step:  51  training loss:  0.0016140623483806849\n",
      "Epoch: 25  Step:  52  training loss:  0.005320251453667879\n",
      "Epoch: 25  Step:  53  training loss:  0.002590178046375513\n",
      "Epoch: 25  Step:  54  training loss:  0.00238602957688272\n",
      "Epoch: 25  Step:  55  training loss:  0.001557371113449335\n",
      "Epoch: 26  Step:  0  training loss:  0.0025649324525147676\n",
      "Epoch: 26  Step:  1  training loss:  0.0025002623442560434\n",
      "Epoch: 26  Step:  2  training loss:  0.002020791405811906\n",
      "Epoch: 26  Step:  3  training loss:  0.0020842519588768482\n",
      "Epoch: 26  Step:  4  training loss:  0.002383588347584009\n",
      "Epoch: 26  Step:  5  training loss:  0.0019230128964409232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26  Step:  6  training loss:  0.0037005115300416946\n",
      "Epoch: 26  Step:  7  training loss:  0.002924563828855753\n",
      "Epoch: 26  Step:  8  training loss:  0.0021479306742548943\n",
      "Epoch: 26  Step:  9  training loss:  0.002099674893543124\n",
      "Epoch: 26  Step:  10  training loss:  0.0026621082797646523\n",
      "Epoch: 26  Step:  11  training loss:  0.0028346737381070852\n",
      "Epoch: 26  Step:  12  training loss:  0.0024588247761130333\n",
      "Epoch: 26  Step:  13  training loss:  0.001804710947908461\n",
      "Epoch: 26  Step:  14  training loss:  0.003369646379724145\n",
      "Epoch: 26  Step:  15  training loss:  0.001965655479580164\n",
      "Epoch: 26  Step:  16  training loss:  0.0019598216749727726\n",
      "Epoch: 26  Step:  17  training loss:  0.00348351476714015\n",
      "Epoch: 26  Step:  18  training loss:  0.0021934593096375465\n",
      "Epoch: 26  Step:  19  training loss:  0.0023026985581964254\n",
      "Epoch: 26  Step:  20  training loss:  0.002304958878085017\n",
      "Epoch: 26  Step:  21  training loss:  0.001998433843255043\n",
      "Epoch: 26  Step:  22  training loss:  0.001858814968727529\n",
      "Epoch: 26  Step:  23  training loss:  0.0026261387392878532\n",
      "Epoch: 26  Step:  24  training loss:  0.0031279665417969227\n",
      "Epoch: 26  Step:  25  training loss:  0.0024659289047122\n",
      "Epoch: 26  Step:  26  training loss:  0.0018568311352282763\n",
      "Epoch: 26  Step:  27  training loss:  0.0023613928351551294\n",
      "Epoch: 26  Step:  28  training loss:  0.002148459665477276\n",
      "Epoch: 26  Step:  29  training loss:  0.0027261516079306602\n",
      "Epoch: 26  Step:  30  training loss:  0.0021869256161153316\n",
      "Epoch: 26  Step:  31  training loss:  0.0028280937112867832\n",
      "Epoch: 26  Step:  32  training loss:  0.0019539494533091784\n",
      "Epoch: 26  Step:  33  training loss:  0.0025913813151419163\n",
      "Epoch: 26  Step:  34  training loss:  0.0021378505043685436\n",
      "Epoch: 26  Step:  35  training loss:  0.0024849867913872004\n",
      "Epoch: 26  Step:  36  training loss:  0.003996249288320541\n",
      "Epoch: 26  Step:  37  training loss:  0.00290070497430861\n",
      "Epoch: 26  Step:  38  training loss:  0.002622867003083229\n",
      "Epoch: 26  Step:  39  training loss:  0.001963501563295722\n",
      "Epoch: 26  Step:  40  training loss:  0.0025957569014281034\n",
      "Epoch: 26  Step:  41  training loss:  0.0028675931971520185\n",
      "Epoch: 26  Step:  42  training loss:  0.002208709018304944\n",
      "Epoch: 26  Step:  43  training loss:  0.0022242488339543343\n",
      "Epoch: 26  Step:  44  training loss:  0.0024798994418233633\n",
      "Epoch: 26  Step:  45  training loss:  0.0024196344893425703\n",
      "Epoch: 26  Step:  46  training loss:  0.0027240177150815725\n",
      "Epoch: 26  Step:  47  training loss:  0.002445175312459469\n",
      "Epoch: 26  Step:  48  training loss:  0.002632886404171586\n",
      "Epoch: 26  Step:  49  training loss:  0.005315808579325676\n",
      "Epoch: 26  Step:  50  training loss:  0.0026333830319344997\n",
      "Epoch: 26  Step:  51  training loss:  0.006485281977802515\n",
      "Epoch: 26  Step:  52  training loss:  0.009058676660060883\n",
      "Epoch: 26  Step:  53  training loss:  0.002382703823968768\n",
      "Epoch: 26  Step:  54  training loss:  0.002177076181396842\n",
      "Epoch: 26  Step:  55  training loss:  0.0037610698491334915\n",
      "Epoch: 27  Step:  0  training loss:  0.003556846873834729\n",
      "Epoch: 27  Step:  1  training loss:  0.002353857969865203\n",
      "Epoch: 27  Step:  2  training loss:  0.002601330168545246\n",
      "Epoch: 27  Step:  3  training loss:  0.0017912770854309201\n",
      "Epoch: 27  Step:  4  training loss:  0.0023028389550745487\n",
      "Epoch: 27  Step:  5  training loss:  0.0029463372193276882\n",
      "Epoch: 27  Step:  6  training loss:  0.002995055867359042\n",
      "Epoch: 27  Step:  7  training loss:  0.002956925891339779\n",
      "Epoch: 27  Step:  8  training loss:  0.002830272074788809\n",
      "Epoch: 27  Step:  9  training loss:  0.0017288589151576161\n",
      "Epoch: 27  Step:  10  training loss:  0.0026451239828020334\n",
      "Epoch: 27  Step:  11  training loss:  0.002583855763077736\n",
      "Epoch: 27  Step:  12  training loss:  0.0061738211661577225\n",
      "Epoch: 27  Step:  13  training loss:  0.0017063741106539965\n",
      "Epoch: 27  Step:  14  training loss:  0.002617214573547244\n",
      "Epoch: 27  Step:  15  training loss:  0.0024258028715848923\n",
      "Epoch: 27  Step:  16  training loss:  0.0028107070829719305\n",
      "Epoch: 27  Step:  17  training loss:  0.0024430786725133657\n",
      "Epoch: 27  Step:  18  training loss:  0.00206603086553514\n",
      "Epoch: 27  Step:  19  training loss:  0.0023965409491211176\n",
      "Epoch: 27  Step:  20  training loss:  0.00272757257334888\n",
      "Epoch: 27  Step:  21  training loss:  0.0019197415094822645\n",
      "Epoch: 27  Step:  22  training loss:  0.0025518762413412333\n",
      "Epoch: 27  Step:  23  training loss:  0.0027208710089325905\n",
      "Epoch: 27  Step:  24  training loss:  0.0017145359888672829\n",
      "Epoch: 27  Step:  25  training loss:  0.0037060794420540333\n",
      "Epoch: 27  Step:  26  training loss:  0.002028569346293807\n",
      "Epoch: 27  Step:  27  training loss:  0.0017927178414538503\n",
      "Epoch: 27  Step:  28  training loss:  0.0014712034026160836\n",
      "Epoch: 27  Step:  29  training loss:  0.0029016872867941856\n",
      "Epoch: 27  Step:  30  training loss:  0.002391873626038432\n",
      "Epoch: 27  Step:  31  training loss:  0.002872949466109276\n",
      "Epoch: 27  Step:  32  training loss:  0.008990074507892132\n",
      "Epoch: 27  Step:  33  training loss:  0.0022904686629772186\n",
      "Epoch: 27  Step:  34  training loss:  0.005935470573604107\n",
      "Epoch: 27  Step:  35  training loss:  0.002404120285063982\n",
      "Epoch: 27  Step:  36  training loss:  0.003080995986238122\n",
      "Epoch: 27  Step:  37  training loss:  0.0023174122907221317\n",
      "Epoch: 27  Step:  38  training loss:  0.0024781334213912487\n",
      "Epoch: 27  Step:  39  training loss:  0.0021360665559768677\n",
      "Epoch: 27  Step:  40  training loss:  0.002561363857239485\n",
      "Epoch: 27  Step:  41  training loss:  0.0019787088967859745\n",
      "Epoch: 27  Step:  42  training loss:  0.002335157012566924\n",
      "Epoch: 27  Step:  43  training loss:  0.0024632359854876995\n",
      "Epoch: 27  Step:  44  training loss:  0.0023702748585492373\n",
      "Epoch: 27  Step:  45  training loss:  0.002114666858687997\n",
      "Epoch: 27  Step:  46  training loss:  0.0034384739119559526\n",
      "Epoch: 27  Step:  47  training loss:  0.00232113734818995\n",
      "Epoch: 27  Step:  48  training loss:  0.0027782192919403315\n",
      "Epoch: 27  Step:  49  training loss:  0.002494668820872903\n",
      "Epoch: 27  Step:  50  training loss:  0.002391940914094448\n",
      "Epoch: 27  Step:  51  training loss:  0.0023724890779703856\n",
      "Epoch: 27  Step:  52  training loss:  0.0022163644898682833\n",
      "Epoch: 27  Step:  53  training loss:  0.003269979963079095\n",
      "Epoch: 27  Step:  54  training loss:  0.001959733199328184\n",
      "Epoch: 27  Step:  55  training loss:  0.0024320255033671856\n",
      "Epoch: 28  Step:  0  training loss:  0.002402353100478649\n",
      "Epoch: 28  Step:  1  training loss:  0.0034805575851351023\n",
      "Epoch: 28  Step:  2  training loss:  0.002059521619230509\n",
      "Epoch: 28  Step:  3  training loss:  0.0018613663269206882\n",
      "Epoch: 28  Step:  4  training loss:  0.0019693048670887947\n",
      "Epoch: 28  Step:  5  training loss:  0.0022953953593969345\n",
      "Epoch: 28  Step:  6  training loss:  0.002890657866373658\n",
      "Epoch: 28  Step:  7  training loss:  0.0020194007083773613\n",
      "Epoch: 28  Step:  8  training loss:  0.0022941031493246555\n",
      "Epoch: 28  Step:  9  training loss:  0.0036089131608605385\n",
      "Epoch: 28  Step:  10  training loss:  0.0024155371356755495\n",
      "Epoch: 28  Step:  11  training loss:  0.0022304109297692776\n",
      "Epoch: 28  Step:  12  training loss:  0.00800695363432169\n",
      "Epoch: 28  Step:  13  training loss:  0.0024905349127948284\n",
      "Epoch: 28  Step:  14  training loss:  0.0023269369266927242\n",
      "Epoch: 28  Step:  15  training loss:  0.0021762435790151358\n",
      "Epoch: 28  Step:  16  training loss:  0.002349625341594219\n",
      "Epoch: 28  Step:  17  training loss:  0.002468266524374485\n",
      "Epoch: 28  Step:  18  training loss:  0.002311841817572713\n",
      "Epoch: 28  Step:  19  training loss:  0.0023430248256772757\n",
      "Epoch: 28  Step:  20  training loss:  0.0024203851353377104\n",
      "Epoch: 28  Step:  21  training loss:  0.002227427437901497\n",
      "Epoch: 28  Step:  22  training loss:  0.003309364663437009\n",
      "Epoch: 28  Step:  23  training loss:  0.0023760816548019648\n",
      "Epoch: 28  Step:  24  training loss:  0.001610133913345635\n",
      "Epoch: 28  Step:  25  training loss:  0.0022484413348138332\n",
      "Epoch: 28  Step:  26  training loss:  0.0028976593166589737\n",
      "Epoch: 28  Step:  27  training loss:  0.0018743580440059304\n",
      "Epoch: 28  Step:  28  training loss:  0.001782347564585507\n",
      "Epoch: 28  Step:  29  training loss:  0.0024753885809332132\n",
      "Epoch: 28  Step:  30  training loss:  0.0027427226305007935\n",
      "Epoch: 28  Step:  31  training loss:  0.002588036935776472\n",
      "Epoch: 28  Step:  32  training loss:  0.002832747297361493\n",
      "Epoch: 28  Step:  33  training loss:  0.001953323371708393\n",
      "Epoch: 28  Step:  34  training loss:  0.00519889360293746\n",
      "Epoch: 28  Step:  35  training loss:  0.002187353791669011\n",
      "Epoch: 28  Step:  36  training loss:  0.0031016641296446323\n",
      "Epoch: 28  Step:  37  training loss:  0.001909284619614482\n",
      "Epoch: 28  Step:  38  training loss:  0.0023786816745996475\n",
      "Epoch: 28  Step:  39  training loss:  0.002939163241535425\n",
      "Epoch: 28  Step:  40  training loss:  0.0020882755052298307\n",
      "Epoch: 28  Step:  41  training loss:  0.002952742390334606\n",
      "Epoch: 28  Step:  42  training loss:  0.0027361426036804914\n",
      "Epoch: 28  Step:  43  training loss:  0.0025251649785786867\n",
      "Epoch: 28  Step:  44  training loss:  0.001994420774281025\n",
      "Epoch: 28  Step:  45  training loss:  0.0025725967716425657\n",
      "Epoch: 28  Step:  46  training loss:  0.0026400291826575994\n",
      "Epoch: 28  Step:  47  training loss:  0.008987732231616974\n",
      "Epoch: 28  Step:  48  training loss:  0.002582577057182789\n",
      "Epoch: 28  Step:  49  training loss:  0.0025639552623033524\n",
      "Epoch: 28  Step:  50  training loss:  0.002660313853994012\n",
      "Epoch: 28  Step:  51  training loss:  0.0020945509895682335\n",
      "Epoch: 28  Step:  52  training loss:  0.0021077971905469894\n",
      "Epoch: 28  Step:  53  training loss:  0.00364303938113153\n",
      "Epoch: 28  Step:  54  training loss:  0.002157469280064106\n",
      "Epoch: 28  Step:  55  training loss:  0.0020499499514698982\n",
      "Epoch: 29  Step:  0  training loss:  0.0020876824855804443\n",
      "Epoch: 29  Step:  1  training loss:  0.002203677548095584\n",
      "Epoch: 29  Step:  2  training loss:  0.0017613747622817755\n",
      "Epoch: 29  Step:  3  training loss:  0.0023409794084727764\n",
      "Epoch: 29  Step:  4  training loss:  0.001994191436097026\n",
      "Epoch: 29  Step:  5  training loss:  0.005860194098204374\n",
      "Epoch: 29  Step:  6  training loss:  0.002427805447950959\n",
      "Epoch: 29  Step:  7  training loss:  0.0024605421349406242\n",
      "Epoch: 29  Step:  8  training loss:  0.001872850232757628\n",
      "Epoch: 29  Step:  9  training loss:  0.0031130313873291016\n",
      "Epoch: 29  Step:  10  training loss:  0.002138421405106783\n",
      "Epoch: 29  Step:  11  training loss:  0.001909442595206201\n",
      "Epoch: 29  Step:  12  training loss:  0.002174210734665394\n",
      "Epoch: 29  Step:  13  training loss:  0.0023714497219771147\n",
      "Epoch: 29  Step:  14  training loss:  0.0030441752169281244\n",
      "Epoch: 29  Step:  15  training loss:  0.002375054406002164\n",
      "Epoch: 29  Step:  16  training loss:  0.0023768674582242966\n",
      "Epoch: 29  Step:  17  training loss:  0.0029200830031186342\n",
      "Epoch: 29  Step:  18  training loss:  0.0019755621906369925\n",
      "Epoch: 29  Step:  19  training loss:  0.002267231233417988\n",
      "Epoch: 29  Step:  20  training loss:  0.0025506464298814535\n",
      "Epoch: 29  Step:  21  training loss:  0.005193833727389574\n",
      "Epoch: 29  Step:  22  training loss:  0.008544180542230606\n",
      "Epoch: 29  Step:  23  training loss:  0.0017104372382164001\n",
      "Epoch: 29  Step:  24  training loss:  0.002126143779605627\n",
      "Epoch: 29  Step:  25  training loss:  0.002348714042454958\n",
      "Epoch: 29  Step:  26  training loss:  0.002485480159521103\n",
      "Epoch: 29  Step:  27  training loss:  0.0029852723237127066\n",
      "Epoch: 29  Step:  28  training loss:  0.002413749461993575\n",
      "Epoch: 29  Step:  29  training loss:  0.0047419448383152485\n",
      "Epoch: 29  Step:  30  training loss:  0.0021144282072782516\n",
      "Epoch: 29  Step:  31  training loss:  0.002394502516835928\n",
      "Epoch: 29  Step:  32  training loss:  0.002865075133740902\n",
      "Epoch: 29  Step:  33  training loss:  0.0025905149523168802\n",
      "Epoch: 29  Step:  34  training loss:  0.0020887781865894794\n",
      "Epoch: 29  Step:  35  training loss:  0.0019893497228622437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29  Step:  36  training loss:  0.004325965419411659\n",
      "Epoch: 29  Step:  37  training loss:  0.002302564214915037\n",
      "Epoch: 29  Step:  38  training loss:  0.0026019797660410404\n",
      "Epoch: 29  Step:  39  training loss:  0.0025373019743710756\n",
      "Epoch: 29  Step:  40  training loss:  0.002220015972852707\n",
      "Epoch: 29  Step:  41  training loss:  0.002614931669086218\n",
      "Epoch: 29  Step:  42  training loss:  0.003099780296906829\n",
      "Epoch: 29  Step:  43  training loss:  0.0027328806463629007\n",
      "Epoch: 29  Step:  44  training loss:  0.0027000552508980036\n",
      "Epoch: 29  Step:  45  training loss:  0.0018126931972801685\n",
      "Epoch: 29  Step:  46  training loss:  0.002921626903116703\n",
      "Epoch: 29  Step:  47  training loss:  0.0023152458015829325\n",
      "Epoch: 29  Step:  48  training loss:  0.00211234949529171\n",
      "Epoch: 29  Step:  49  training loss:  0.0028322325088083744\n",
      "Epoch: 29  Step:  50  training loss:  0.002479689894244075\n",
      "Epoch: 29  Step:  51  training loss:  0.0025033249985426664\n",
      "Epoch: 29  Step:  52  training loss:  0.0026827282272279263\n",
      "Epoch: 29  Step:  53  training loss:  0.0023825643584132195\n",
      "Epoch: 29  Step:  54  training loss:  0.002275995211675763\n",
      "Epoch: 29  Step:  55  training loss:  0.004047700669616461\n",
      "Epoch: 30  Step:  0  training loss:  0.0030012675561010838\n",
      "Epoch: 30  Step:  1  training loss:  0.002014842350035906\n",
      "Epoch: 30  Step:  2  training loss:  0.0022915161680430174\n",
      "Epoch: 30  Step:  3  training loss:  0.001845243270508945\n",
      "Epoch: 30  Step:  4  training loss:  0.006430000066757202\n",
      "Epoch: 30  Step:  5  training loss:  0.0020580196287482977\n",
      "Epoch: 30  Step:  6  training loss:  0.0030760737136006355\n",
      "Epoch: 30  Step:  7  training loss:  0.0024130279198288918\n",
      "Epoch: 30  Step:  8  training loss:  0.0020995219238102436\n",
      "Epoch: 30  Step:  9  training loss:  0.0029468282591551542\n",
      "Epoch: 30  Step:  10  training loss:  0.0021619917824864388\n",
      "Epoch: 30  Step:  11  training loss:  0.0025233200285583735\n",
      "Epoch: 30  Step:  12  training loss:  0.002160613425076008\n",
      "Epoch: 30  Step:  13  training loss:  0.0026498183142393827\n",
      "Epoch: 30  Step:  14  training loss:  0.003161753062158823\n",
      "Epoch: 30  Step:  15  training loss:  0.002458741655573249\n",
      "Epoch: 30  Step:  16  training loss:  0.002538177650421858\n",
      "Epoch: 30  Step:  17  training loss:  0.00324547803029418\n",
      "Epoch: 30  Step:  18  training loss:  0.0028727108146995306\n",
      "Epoch: 30  Step:  19  training loss:  0.0024043729063123465\n",
      "Epoch: 30  Step:  20  training loss:  0.002031623385846615\n",
      "Epoch: 30  Step:  21  training loss:  0.002851353958249092\n",
      "Epoch: 30  Step:  22  training loss:  0.002320224652066827\n",
      "Epoch: 30  Step:  23  training loss:  0.0026104613207280636\n",
      "Epoch: 30  Step:  24  training loss:  0.0036878467071801424\n",
      "Epoch: 30  Step:  25  training loss:  0.0020023563411086798\n",
      "Epoch: 30  Step:  26  training loss:  0.008603802882134914\n",
      "Epoch: 30  Step:  27  training loss:  0.002816180232912302\n",
      "Epoch: 30  Step:  28  training loss:  0.002070703776553273\n",
      "Epoch: 30  Step:  29  training loss:  0.0024717794731259346\n",
      "Epoch: 30  Step:  30  training loss:  0.0028143110685050488\n",
      "Epoch: 30  Step:  31  training loss:  0.0025628244038671255\n",
      "Epoch: 30  Step:  32  training loss:  0.0024697163607925177\n",
      "Epoch: 30  Step:  33  training loss:  0.00154839304741472\n",
      "Epoch: 30  Step:  34  training loss:  0.00576536962762475\n",
      "Epoch: 30  Step:  35  training loss:  0.0017593908123672009\n",
      "Epoch: 30  Step:  36  training loss:  0.0020954713691025972\n",
      "Epoch: 30  Step:  37  training loss:  0.0030987393110990524\n",
      "Epoch: 30  Step:  38  training loss:  0.002573312260210514\n",
      "Epoch: 30  Step:  39  training loss:  0.0024782270193099976\n",
      "Epoch: 30  Step:  40  training loss:  0.002199488691985607\n",
      "Epoch: 30  Step:  41  training loss:  0.0015851744683459401\n",
      "Epoch: 30  Step:  42  training loss:  0.001956271706148982\n",
      "Epoch: 30  Step:  43  training loss:  0.0028856247663497925\n",
      "Epoch: 30  Step:  44  training loss:  0.00289844605140388\n",
      "Epoch: 30  Step:  45  training loss:  0.002937125274911523\n",
      "Epoch: 30  Step:  46  training loss:  0.002054974902421236\n",
      "Epoch: 30  Step:  47  training loss:  0.0018378638196736574\n",
      "Epoch: 30  Step:  48  training loss:  0.002713509602472186\n",
      "Epoch: 30  Step:  49  training loss:  0.0028333007358014584\n",
      "Epoch: 30  Step:  50  training loss:  0.001896973350085318\n",
      "Epoch: 30  Step:  51  training loss:  0.0027505571488291025\n",
      "Epoch: 30  Step:  52  training loss:  0.0022662377450615168\n",
      "Epoch: 30  Step:  53  training loss:  0.0019071713322773576\n",
      "Epoch: 30  Step:  54  training loss:  0.0030116161797195673\n",
      "Epoch: 30  Step:  55  training loss:  0.0022438950836658478\n",
      "Epoch: 31  Step:  0  training loss:  0.0025588006246834993\n",
      "Epoch: 31  Step:  1  training loss:  0.0022737944964319468\n",
      "Epoch: 31  Step:  2  training loss:  0.003048997139558196\n",
      "Epoch: 31  Step:  3  training loss:  0.002303485060110688\n",
      "Epoch: 31  Step:  4  training loss:  0.003008100902661681\n",
      "Epoch: 31  Step:  5  training loss:  0.008009477518498898\n",
      "Epoch: 31  Step:  6  training loss:  0.0025244816206395626\n",
      "Epoch: 31  Step:  7  training loss:  0.0026796460151672363\n",
      "Epoch: 31  Step:  8  training loss:  0.002168240025639534\n",
      "Epoch: 31  Step:  9  training loss:  0.00226354761980474\n",
      "Epoch: 31  Step:  10  training loss:  0.003493278054520488\n",
      "Epoch: 31  Step:  11  training loss:  0.002836195519194007\n",
      "Epoch: 31  Step:  12  training loss:  0.0020726046059280634\n",
      "Epoch: 31  Step:  13  training loss:  0.0026816288009285927\n",
      "Epoch: 31  Step:  14  training loss:  0.002047803485766053\n",
      "Epoch: 31  Step:  15  training loss:  0.0026337220333516598\n",
      "Epoch: 31  Step:  16  training loss:  0.00222436198964715\n",
      "Epoch: 31  Step:  17  training loss:  0.0056364513002336025\n",
      "Epoch: 31  Step:  18  training loss:  0.0025866369251161814\n",
      "Epoch: 31  Step:  19  training loss:  0.004710940644145012\n",
      "Epoch: 31  Step:  20  training loss:  0.0022817712742835283\n",
      "Epoch: 31  Step:  21  training loss:  0.0023139251861721277\n",
      "Epoch: 31  Step:  22  training loss:  0.002111883368343115\n",
      "Epoch: 31  Step:  23  training loss:  0.003852264257147908\n",
      "Epoch: 31  Step:  24  training loss:  0.002392736030742526\n",
      "Epoch: 31  Step:  25  training loss:  0.002287465613335371\n",
      "Epoch: 31  Step:  26  training loss:  0.0023833676241338253\n",
      "Epoch: 31  Step:  27  training loss:  0.002408639993518591\n",
      "Epoch: 31  Step:  28  training loss:  0.0023819683119654655\n",
      "Epoch: 31  Step:  29  training loss:  0.002149498090147972\n",
      "Epoch: 31  Step:  30  training loss:  0.0030452150385826826\n",
      "Epoch: 31  Step:  31  training loss:  0.0027619702741503716\n",
      "Epoch: 31  Step:  32  training loss:  0.00232701119966805\n",
      "Epoch: 31  Step:  33  training loss:  0.001655972097069025\n",
      "Epoch: 31  Step:  34  training loss:  0.0019238726235926151\n",
      "Epoch: 31  Step:  35  training loss:  0.0025550953578203917\n",
      "Epoch: 31  Step:  36  training loss:  0.0019561999943107367\n",
      "Epoch: 31  Step:  37  training loss:  0.002360051963478327\n",
      "Epoch: 31  Step:  38  training loss:  0.0017394056776538491\n",
      "Epoch: 31  Step:  39  training loss:  0.0028566517867147923\n",
      "Epoch: 31  Step:  40  training loss:  0.002722840989008546\n",
      "Epoch: 31  Step:  41  training loss:  0.002167105209082365\n",
      "Epoch: 31  Step:  42  training loss:  0.0027559392619878054\n",
      "Epoch: 31  Step:  43  training loss:  0.0030164476484060287\n",
      "Epoch: 31  Step:  44  training loss:  0.0021892639342695475\n",
      "Epoch: 31  Step:  45  training loss:  0.0026923068799078465\n",
      "Epoch: 31  Step:  46  training loss:  0.0023096841759979725\n",
      "Epoch: 31  Step:  47  training loss:  0.0021780519746243954\n",
      "Epoch: 31  Step:  48  training loss:  0.0027665155939757824\n",
      "Epoch: 31  Step:  49  training loss:  0.0025166019331663847\n",
      "Epoch: 31  Step:  50  training loss:  0.0025968963745981455\n",
      "Epoch: 31  Step:  51  training loss:  0.0021010655909776688\n",
      "Epoch: 31  Step:  52  training loss:  0.00232366262935102\n",
      "Epoch: 31  Step:  53  training loss:  0.002218738431110978\n",
      "Epoch: 31  Step:  54  training loss:  0.0032654586248099804\n",
      "Epoch: 31  Step:  55  training loss:  0.005086969118565321\n",
      "Epoch: 32  Step:  0  training loss:  0.001574307563714683\n",
      "Epoch: 32  Step:  1  training loss:  0.0019951669964939356\n",
      "Epoch: 32  Step:  2  training loss:  0.002682850696146488\n",
      "Epoch: 32  Step:  3  training loss:  0.002015114529058337\n",
      "Epoch: 32  Step:  4  training loss:  0.002755229827016592\n",
      "Epoch: 32  Step:  5  training loss:  0.0021029070485383272\n",
      "Epoch: 32  Step:  6  training loss:  0.002178235212340951\n",
      "Epoch: 32  Step:  7  training loss:  0.003138825297355652\n",
      "Epoch: 32  Step:  8  training loss:  0.0031773450318723917\n",
      "Epoch: 32  Step:  9  training loss:  0.0017672500107437372\n",
      "Epoch: 32  Step:  10  training loss:  0.0036507719196379185\n",
      "Epoch: 32  Step:  11  training loss:  0.003079063491895795\n",
      "Epoch: 32  Step:  12  training loss:  0.0023837913759052753\n",
      "Epoch: 32  Step:  13  training loss:  0.002493295818567276\n",
      "Epoch: 32  Step:  14  training loss:  0.00272480514831841\n",
      "Epoch: 32  Step:  15  training loss:  0.005216721445322037\n",
      "Epoch: 32  Step:  16  training loss:  0.0021326225250959396\n",
      "Epoch: 32  Step:  17  training loss:  0.004197169095277786\n",
      "Epoch: 32  Step:  18  training loss:  0.0019417022122070193\n",
      "Epoch: 32  Step:  19  training loss:  0.0022610158193856478\n",
      "Epoch: 32  Step:  20  training loss:  0.0019966550171375275\n",
      "Epoch: 32  Step:  21  training loss:  0.0020483084954321384\n",
      "Epoch: 32  Step:  22  training loss:  0.0031017682049423456\n",
      "Epoch: 32  Step:  23  training loss:  0.0020075752399861813\n",
      "Epoch: 32  Step:  24  training loss:  0.0025070542469620705\n",
      "Epoch: 32  Step:  25  training loss:  0.005146427545696497\n",
      "Epoch: 32  Step:  26  training loss:  0.00262175383977592\n",
      "Epoch: 32  Step:  27  training loss:  0.00265020364895463\n",
      "Epoch: 32  Step:  28  training loss:  0.00195752689614892\n",
      "Epoch: 32  Step:  29  training loss:  0.0023760171607136726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32  Step:  30  training loss:  0.0025309831835329533\n",
      "Epoch: 32  Step:  31  training loss:  0.002688483102247119\n",
      "Epoch: 32  Step:  32  training loss:  0.0017398069612681866\n",
      "Epoch: 32  Step:  33  training loss:  0.0027551616076380014\n",
      "Epoch: 32  Step:  34  training loss:  0.0022353054955601692\n",
      "Epoch: 32  Step:  35  training loss:  0.0027941567823290825\n",
      "Epoch: 32  Step:  36  training loss:  0.0020378895569592714\n",
      "Epoch: 32  Step:  37  training loss:  0.003378558438271284\n",
      "Epoch: 32  Step:  38  training loss:  0.0021508808713406324\n",
      "Epoch: 32  Step:  39  training loss:  0.00852044578641653\n",
      "Epoch: 32  Step:  40  training loss:  0.001690467121079564\n",
      "Epoch: 32  Step:  41  training loss:  0.0030012831557542086\n",
      "Epoch: 32  Step:  42  training loss:  0.0035614364314824343\n",
      "Epoch: 32  Step:  43  training loss:  0.003978474996984005\n",
      "Epoch: 32  Step:  44  training loss:  0.002136467257514596\n",
      "Epoch: 32  Step:  45  training loss:  0.0025297862011939287\n",
      "Epoch: 32  Step:  46  training loss:  0.002795478794723749\n",
      "Epoch: 32  Step:  47  training loss:  0.002088766312226653\n",
      "Epoch: 32  Step:  48  training loss:  0.002501948969438672\n",
      "Epoch: 32  Step:  49  training loss:  0.001970487181097269\n",
      "Epoch: 32  Step:  50  training loss:  0.002198874019086361\n",
      "Epoch: 32  Step:  51  training loss:  0.0023927814327180386\n",
      "Epoch: 32  Step:  52  training loss:  0.002151952125132084\n",
      "Epoch: 32  Step:  53  training loss:  0.0017704161582514644\n",
      "Epoch: 32  Step:  54  training loss:  0.0024488638155162334\n",
      "Epoch: 32  Step:  55  training loss:  0.003022031392902136\n",
      "Epoch: 33  Step:  0  training loss:  0.004265060648322105\n",
      "Epoch: 33  Step:  1  training loss:  0.003121996531262994\n",
      "Epoch: 33  Step:  2  training loss:  0.001937328022904694\n",
      "Epoch: 33  Step:  3  training loss:  0.00249207834713161\n",
      "Epoch: 33  Step:  4  training loss:  0.0020079221576452255\n",
      "Epoch: 33  Step:  5  training loss:  0.0029096670914441347\n",
      "Epoch: 33  Step:  6  training loss:  0.006071784533560276\n",
      "Epoch: 33  Step:  7  training loss:  0.003943483345210552\n",
      "Epoch: 33  Step:  8  training loss:  0.003112474689260125\n",
      "Epoch: 33  Step:  9  training loss:  0.008686191402375698\n",
      "Epoch: 33  Step:  10  training loss:  0.002277353312820196\n",
      "Epoch: 33  Step:  11  training loss:  0.002317295176908374\n",
      "Epoch: 33  Step:  12  training loss:  0.0029500469099730253\n",
      "Epoch: 33  Step:  13  training loss:  0.0017734069842845201\n",
      "Epoch: 33  Step:  14  training loss:  0.001953511033207178\n",
      "Epoch: 33  Step:  15  training loss:  0.002599415136501193\n",
      "Epoch: 33  Step:  16  training loss:  0.002827761461958289\n",
      "Epoch: 33  Step:  17  training loss:  0.002435118891298771\n",
      "Epoch: 33  Step:  18  training loss:  0.002481125993654132\n",
      "Epoch: 33  Step:  19  training loss:  0.00263723311945796\n",
      "Epoch: 33  Step:  20  training loss:  0.0025505060330033302\n",
      "Epoch: 33  Step:  21  training loss:  0.0022307380568236113\n",
      "Epoch: 33  Step:  22  training loss:  0.0024097675923258066\n",
      "Epoch: 33  Step:  23  training loss:  0.002883884124457836\n",
      "Epoch: 33  Step:  24  training loss:  0.0020979014225304127\n",
      "Epoch: 33  Step:  25  training loss:  0.0024103375617414713\n",
      "Epoch: 33  Step:  26  training loss:  0.00279084499925375\n",
      "Epoch: 33  Step:  27  training loss:  0.00247841770760715\n",
      "Epoch: 33  Step:  28  training loss:  0.0026057951617985964\n",
      "Epoch: 33  Step:  29  training loss:  0.002781503600999713\n",
      "Epoch: 33  Step:  30  training loss:  0.0020665754564106464\n",
      "Epoch: 33  Step:  31  training loss:  0.0015447881305590272\n",
      "Epoch: 33  Step:  32  training loss:  0.001613234868273139\n",
      "Epoch: 33  Step:  33  training loss:  0.002504711737856269\n",
      "Epoch: 33  Step:  34  training loss:  0.0020199627615511417\n",
      "Epoch: 33  Step:  35  training loss:  0.0028366418555378914\n",
      "Epoch: 33  Step:  36  training loss:  0.002019378123804927\n",
      "Epoch: 33  Step:  37  training loss:  0.001752672833390534\n",
      "Epoch: 33  Step:  38  training loss:  0.0038815054576843977\n",
      "Epoch: 33  Step:  39  training loss:  0.0031624510884284973\n",
      "Epoch: 33  Step:  40  training loss:  0.0022140126675367355\n",
      "Epoch: 33  Step:  41  training loss:  0.0025647825095802546\n",
      "Epoch: 33  Step:  42  training loss:  0.001410380587913096\n",
      "Epoch: 33  Step:  43  training loss:  0.0020480751991271973\n",
      "Epoch: 33  Step:  44  training loss:  0.002302381210029125\n",
      "Epoch: 33  Step:  45  training loss:  0.0021847165189683437\n",
      "Epoch: 33  Step:  46  training loss:  0.0020214791875332594\n",
      "Epoch: 33  Step:  47  training loss:  0.0024793357588350773\n",
      "Epoch: 33  Step:  48  training loss:  0.00381792476400733\n",
      "Epoch: 33  Step:  49  training loss:  0.0024606548249721527\n",
      "Epoch: 33  Step:  50  training loss:  0.00171667302493006\n",
      "Epoch: 33  Step:  51  training loss:  0.0020332953426986933\n",
      "Epoch: 33  Step:  52  training loss:  0.005074439570307732\n",
      "Epoch: 33  Step:  53  training loss:  0.0023210125509649515\n",
      "Epoch: 33  Step:  54  training loss:  0.0020489187445491552\n",
      "Epoch: 33  Step:  55  training loss:  0.001813447568565607\n",
      "Epoch: 34  Step:  0  training loss:  0.002037633443251252\n",
      "Epoch: 34  Step:  1  training loss:  0.002225436270236969\n",
      "Epoch: 34  Step:  2  training loss:  0.0032301428727805614\n",
      "Epoch: 34  Step:  3  training loss:  0.0025269347243010998\n",
      "Epoch: 34  Step:  4  training loss:  0.005343571770936251\n",
      "Epoch: 34  Step:  5  training loss:  0.00339714577421546\n",
      "Epoch: 34  Step:  6  training loss:  0.0024557707365602255\n",
      "Epoch: 34  Step:  7  training loss:  0.0021458687260746956\n",
      "Epoch: 34  Step:  8  training loss:  0.002483157906681299\n",
      "Epoch: 34  Step:  9  training loss:  0.002598743187263608\n",
      "Epoch: 34  Step:  10  training loss:  0.0020787110552191734\n",
      "Epoch: 34  Step:  11  training loss:  0.001954261912032962\n",
      "Epoch: 34  Step:  12  training loss:  0.008247104473412037\n",
      "Epoch: 34  Step:  13  training loss:  0.0028222608380019665\n",
      "Epoch: 34  Step:  14  training loss:  0.0021090295631438494\n",
      "Epoch: 34  Step:  15  training loss:  0.0029953874181956053\n",
      "Epoch: 34  Step:  16  training loss:  0.002190937288105488\n",
      "Epoch: 34  Step:  17  training loss:  0.0024275111500173807\n",
      "Epoch: 34  Step:  18  training loss:  0.0021255463361740112\n",
      "Epoch: 34  Step:  19  training loss:  0.00383495120331645\n",
      "Epoch: 34  Step:  20  training loss:  0.002536145970225334\n",
      "Epoch: 34  Step:  21  training loss:  0.002621725434437394\n",
      "Epoch: 34  Step:  22  training loss:  0.002720528282225132\n",
      "Epoch: 34  Step:  23  training loss:  0.001652335631661117\n",
      "Epoch: 34  Step:  24  training loss:  0.00249957456253469\n",
      "Epoch: 34  Step:  25  training loss:  0.002351590199396014\n",
      "Epoch: 34  Step:  26  training loss:  0.002306062262505293\n",
      "Epoch: 34  Step:  27  training loss:  0.0020353668369352818\n",
      "Epoch: 34  Step:  28  training loss:  0.0024350283201783895\n",
      "Epoch: 34  Step:  29  training loss:  0.002196215558797121\n",
      "Epoch: 34  Step:  30  training loss:  0.0020554354414343834\n",
      "Epoch: 34  Step:  31  training loss:  0.002304256893694401\n",
      "Epoch: 34  Step:  32  training loss:  0.005601693410426378\n",
      "Epoch: 34  Step:  33  training loss:  0.0026962009724229574\n",
      "Epoch: 34  Step:  34  training loss:  0.003330947831273079\n",
      "Epoch: 34  Step:  35  training loss:  0.0020020664669573307\n",
      "Epoch: 34  Step:  36  training loss:  0.0021274604368954897\n",
      "Epoch: 34  Step:  37  training loss:  0.0023829799611121416\n",
      "Epoch: 34  Step:  38  training loss:  0.002309444360435009\n",
      "Epoch: 34  Step:  39  training loss:  0.0017604919848963618\n",
      "Epoch: 34  Step:  40  training loss:  0.002703859005123377\n",
      "Epoch: 34  Step:  41  training loss:  0.001962368842214346\n",
      "Epoch: 34  Step:  42  training loss:  0.0036398759111762047\n",
      "Epoch: 34  Step:  43  training loss:  0.003821620950475335\n",
      "Epoch: 34  Step:  44  training loss:  0.002280081855133176\n",
      "Epoch: 34  Step:  45  training loss:  0.0023792327847331762\n",
      "Epoch: 34  Step:  46  training loss:  0.0025408801157027483\n",
      "Epoch: 34  Step:  47  training loss:  0.002019190462306142\n",
      "Epoch: 34  Step:  48  training loss:  0.0024046101607382298\n",
      "Epoch: 34  Step:  49  training loss:  0.002043329179286957\n",
      "Epoch: 34  Step:  50  training loss:  0.0016564386896789074\n",
      "Epoch: 34  Step:  51  training loss:  0.002605213550850749\n",
      "Epoch: 34  Step:  52  training loss:  0.004269017372280359\n",
      "Epoch: 34  Step:  53  training loss:  0.002228274242952466\n",
      "Epoch: 34  Step:  54  training loss:  0.0019531964790076017\n",
      "Epoch: 34  Step:  55  training loss:  0.002481085481122136\n",
      "Epoch: 35  Step:  0  training loss:  0.0022938998881727457\n",
      "Epoch: 35  Step:  1  training loss:  0.0022748103365302086\n",
      "Epoch: 35  Step:  2  training loss:  0.002964167622849345\n",
      "Epoch: 35  Step:  3  training loss:  0.003000875934958458\n",
      "Epoch: 35  Step:  4  training loss:  0.0025908255483955145\n",
      "Epoch: 35  Step:  5  training loss:  0.0028576182667165995\n",
      "Epoch: 35  Step:  6  training loss:  0.002486387500539422\n",
      "Epoch: 35  Step:  7  training loss:  0.002834159415215254\n",
      "Epoch: 35  Step:  8  training loss:  0.0020325127989053726\n",
      "Epoch: 35  Step:  9  training loss:  0.002196977846324444\n",
      "Epoch: 35  Step:  10  training loss:  0.002550621284171939\n",
      "Epoch: 35  Step:  11  training loss:  0.0028795015532523394\n",
      "Epoch: 35  Step:  12  training loss:  0.001827937550842762\n",
      "Epoch: 35  Step:  13  training loss:  0.002432666951790452\n",
      "Epoch: 35  Step:  14  training loss:  0.002229963894933462\n",
      "Epoch: 35  Step:  15  training loss:  0.0023497003130614758\n",
      "Epoch: 35  Step:  16  training loss:  0.002980477875098586\n",
      "Epoch: 35  Step:  17  training loss:  0.0024549290537834167\n",
      "Epoch: 35  Step:  18  training loss:  0.002076156670227647\n",
      "Epoch: 35  Step:  19  training loss:  0.002560460940003395\n",
      "Epoch: 35  Step:  20  training loss:  0.002296904567629099\n",
      "Epoch: 35  Step:  21  training loss:  0.002365709049627185\n",
      "Epoch: 35  Step:  22  training loss:  0.002328702714294195\n",
      "Epoch: 35  Step:  23  training loss:  0.0026140694972127676\n",
      "Epoch: 35  Step:  24  training loss:  0.003377391491085291\n",
      "Epoch: 35  Step:  25  training loss:  0.0019635132048279047\n",
      "Epoch: 35  Step:  26  training loss:  0.0018548129592090845\n",
      "Epoch: 35  Step:  27  training loss:  0.0023908326402306557\n",
      "Epoch: 35  Step:  28  training loss:  0.0026196253020316362\n",
      "Epoch: 35  Step:  29  training loss:  0.0029122084379196167\n",
      "Epoch: 35  Step:  30  training loss:  0.002520859008654952\n",
      "Epoch: 35  Step:  31  training loss:  0.0019084038212895393\n",
      "Epoch: 35  Step:  32  training loss:  0.0025547093246132135\n",
      "Epoch: 35  Step:  33  training loss:  0.0023322664201259613\n",
      "Epoch: 35  Step:  34  training loss:  0.002236624015495181\n",
      "Epoch: 35  Step:  35  training loss:  0.001644478994421661\n",
      "Epoch: 35  Step:  36  training loss:  0.0017807011026889086\n",
      "Epoch: 35  Step:  37  training loss:  0.002527083968743682\n",
      "Epoch: 35  Step:  38  training loss:  0.009535610675811768\n",
      "Epoch: 35  Step:  39  training loss:  0.0020257034339010715\n",
      "Epoch: 35  Step:  40  training loss:  0.0022731968201696873\n",
      "Epoch: 35  Step:  41  training loss:  0.002053998177871108\n",
      "Epoch: 35  Step:  42  training loss:  0.002194688655436039\n",
      "Epoch: 35  Step:  43  training loss:  0.002418621676042676\n",
      "Epoch: 35  Step:  44  training loss:  0.0026888223364949226\n",
      "Epoch: 35  Step:  45  training loss:  0.0020993819925934076\n",
      "Epoch: 35  Step:  46  training loss:  0.009101451374590397\n",
      "Epoch: 35  Step:  47  training loss:  0.0027174814604222775\n",
      "Epoch: 35  Step:  48  training loss:  0.001749544171616435\n",
      "Epoch: 35  Step:  49  training loss:  0.002997106406837702\n",
      "Epoch: 35  Step:  50  training loss:  0.002938751596957445\n",
      "Epoch: 35  Step:  51  training loss:  0.002070771064609289\n",
      "Epoch: 35  Step:  52  training loss:  0.0031354310922324657\n",
      "Epoch: 35  Step:  53  training loss:  0.00320995831862092\n",
      "Epoch: 35  Step:  54  training loss:  0.002379594836384058\n",
      "Epoch: 35  Step:  55  training loss:  0.001818547840230167\n",
      "Epoch: 36  Step:  0  training loss:  0.0021318013314157724\n",
      "Epoch: 36  Step:  1  training loss:  0.0024544941261410713\n",
      "Epoch: 36  Step:  2  training loss:  0.0024451410863548517\n",
      "Epoch: 36  Step:  3  training loss:  0.0019010922405868769\n",
      "Epoch: 36  Step:  4  training loss:  0.0049437228590250015\n",
      "Epoch: 36  Step:  5  training loss:  0.0024249758571386337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36  Step:  6  training loss:  0.0024282140657305717\n",
      "Epoch: 36  Step:  7  training loss:  0.002192959189414978\n",
      "Epoch: 36  Step:  8  training loss:  0.002646883949637413\n",
      "Epoch: 36  Step:  9  training loss:  0.0028311305213719606\n",
      "Epoch: 36  Step:  10  training loss:  0.0029772883281111717\n",
      "Epoch: 36  Step:  11  training loss:  0.0017994069494307041\n",
      "Epoch: 36  Step:  12  training loss:  0.002369266003370285\n",
      "Epoch: 36  Step:  13  training loss:  0.0038439854979515076\n",
      "Epoch: 36  Step:  14  training loss:  0.0020582827273756266\n",
      "Epoch: 36  Step:  15  training loss:  0.001969297882169485\n",
      "Epoch: 36  Step:  16  training loss:  0.0028555116150528193\n",
      "Epoch: 36  Step:  17  training loss:  0.0025107190012931824\n",
      "Epoch: 36  Step:  18  training loss:  0.0026035986375063658\n",
      "Epoch: 36  Step:  19  training loss:  0.0024523409083485603\n",
      "Epoch: 36  Step:  20  training loss:  0.0036377599462866783\n",
      "Epoch: 36  Step:  21  training loss:  0.006178287789225578\n",
      "Epoch: 36  Step:  22  training loss:  0.0024496002588421106\n",
      "Epoch: 36  Step:  23  training loss:  0.002043843036517501\n",
      "Epoch: 36  Step:  24  training loss:  0.0023956620134413242\n",
      "Epoch: 36  Step:  25  training loss:  0.002200337126851082\n",
      "Epoch: 36  Step:  26  training loss:  0.002333024749532342\n",
      "Epoch: 36  Step:  27  training loss:  0.0025349745992571115\n",
      "Epoch: 36  Step:  28  training loss:  0.002406827872619033\n",
      "Epoch: 36  Step:  29  training loss:  0.002819780493155122\n",
      "Epoch: 36  Step:  30  training loss:  0.0026525212451815605\n",
      "Epoch: 36  Step:  31  training loss:  0.003201812505722046\n",
      "Epoch: 36  Step:  32  training loss:  0.00273765972815454\n",
      "Epoch: 36  Step:  33  training loss:  0.0033539566211402416\n",
      "Epoch: 36  Step:  34  training loss:  0.0026909091975539923\n",
      "Epoch: 36  Step:  35  training loss:  0.0024240403436124325\n",
      "Epoch: 36  Step:  36  training loss:  0.002354877069592476\n",
      "Epoch: 36  Step:  37  training loss:  0.0026479228399693966\n",
      "Epoch: 36  Step:  38  training loss:  0.0016705398447811604\n",
      "Epoch: 36  Step:  39  training loss:  0.0021737245842814445\n",
      "Epoch: 36  Step:  40  training loss:  0.0026029590517282486\n",
      "Epoch: 36  Step:  41  training loss:  0.0018241236684843898\n",
      "Epoch: 36  Step:  42  training loss:  0.008391620591282845\n",
      "Epoch: 36  Step:  43  training loss:  0.0016890581464394927\n",
      "Epoch: 36  Step:  44  training loss:  0.0022211577743291855\n",
      "Epoch: 36  Step:  45  training loss:  0.002788218669593334\n",
      "Epoch: 36  Step:  46  training loss:  0.0021996900904923677\n",
      "Epoch: 36  Step:  47  training loss:  0.0017971524503082037\n",
      "Epoch: 36  Step:  48  training loss:  0.0025457865558564663\n",
      "Epoch: 36  Step:  49  training loss:  0.0023677514400333166\n",
      "Epoch: 36  Step:  50  training loss:  0.002123248064890504\n",
      "Epoch: 36  Step:  51  training loss:  0.002095215953886509\n",
      "Epoch: 36  Step:  52  training loss:  0.003141478868201375\n",
      "Epoch: 36  Step:  53  training loss:  0.0025993369054049253\n",
      "Epoch: 36  Step:  54  training loss:  0.0019691637717187405\n",
      "Epoch: 36  Step:  55  training loss:  0.002521814312785864\n",
      "Epoch: 37  Step:  0  training loss:  0.0025723876897245646\n",
      "Epoch: 37  Step:  1  training loss:  0.002707029227167368\n",
      "Epoch: 37  Step:  2  training loss:  0.002778940601274371\n",
      "Epoch: 37  Step:  3  training loss:  0.0029555188957601786\n",
      "Epoch: 37  Step:  4  training loss:  0.0019342188024893403\n",
      "Epoch: 37  Step:  5  training loss:  0.0026123824063688517\n",
      "Epoch: 37  Step:  6  training loss:  0.0024210307747125626\n",
      "Epoch: 37  Step:  7  training loss:  0.003881954587996006\n",
      "Epoch: 37  Step:  8  training loss:  0.0026304256170988083\n",
      "Epoch: 37  Step:  9  training loss:  0.003103657392784953\n",
      "Epoch: 37  Step:  10  training loss:  0.005858447402715683\n",
      "Epoch: 37  Step:  11  training loss:  0.0022978822235018015\n",
      "Epoch: 37  Step:  12  training loss:  0.0022266514133661985\n",
      "Epoch: 37  Step:  13  training loss:  0.00361049035564065\n",
      "Epoch: 37  Step:  14  training loss:  0.00245532370172441\n",
      "Epoch: 37  Step:  15  training loss:  0.002323162043467164\n",
      "Epoch: 37  Step:  16  training loss:  0.0019157436909154058\n",
      "Epoch: 37  Step:  17  training loss:  0.0021334176417440176\n",
      "Epoch: 37  Step:  18  training loss:  0.0019255816005170345\n",
      "Epoch: 37  Step:  19  training loss:  0.0023586866445839405\n",
      "Epoch: 37  Step:  20  training loss:  0.002732012188062072\n",
      "Epoch: 37  Step:  21  training loss:  0.002255223458632827\n",
      "Epoch: 37  Step:  22  training loss:  0.0024785050190985203\n",
      "Epoch: 37  Step:  23  training loss:  0.0016102140070870519\n",
      "Epoch: 37  Step:  24  training loss:  0.002044311724603176\n",
      "Epoch: 37  Step:  25  training loss:  0.0023110753390938044\n",
      "Epoch: 37  Step:  26  training loss:  0.002346290275454521\n",
      "Epoch: 37  Step:  27  training loss:  0.002209015190601349\n",
      "Epoch: 37  Step:  28  training loss:  0.00243966793641448\n",
      "Epoch: 37  Step:  29  training loss:  0.002323948312550783\n",
      "Epoch: 37  Step:  30  training loss:  0.0023038771469146013\n",
      "Epoch: 37  Step:  31  training loss:  0.005364410113543272\n",
      "Epoch: 37  Step:  32  training loss:  0.0025382735766470432\n",
      "Epoch: 37  Step:  33  training loss:  0.002678463701158762\n",
      "Epoch: 37  Step:  34  training loss:  0.003040132811293006\n",
      "Epoch: 37  Step:  35  training loss:  0.008270371705293655\n",
      "Epoch: 37  Step:  36  training loss:  0.0017943083075806499\n",
      "Epoch: 37  Step:  37  training loss:  0.002516093198210001\n",
      "Epoch: 37  Step:  38  training loss:  0.002541818656027317\n",
      "Epoch: 37  Step:  39  training loss:  0.0027605085633695126\n",
      "Epoch: 37  Step:  40  training loss:  0.0024791439063847065\n",
      "Epoch: 37  Step:  41  training loss:  0.0026362561620771885\n",
      "Epoch: 37  Step:  42  training loss:  0.0017837672494351864\n",
      "Epoch: 37  Step:  43  training loss:  0.0016426258953288198\n",
      "Epoch: 37  Step:  44  training loss:  0.002292308956384659\n",
      "Epoch: 37  Step:  45  training loss:  0.003247214946895838\n",
      "Epoch: 37  Step:  46  training loss:  0.0024607370141893625\n",
      "Epoch: 37  Step:  47  training loss:  0.0020428088027983904\n",
      "Epoch: 37  Step:  48  training loss:  0.0019737554248422384\n",
      "Epoch: 37  Step:  49  training loss:  0.002097515854984522\n",
      "Epoch: 37  Step:  50  training loss:  0.0025967564433813095\n",
      "Epoch: 37  Step:  51  training loss:  0.00241255690343678\n",
      "Epoch: 37  Step:  52  training loss:  0.0025463507045060396\n",
      "Epoch: 37  Step:  53  training loss:  0.0026391365099698305\n",
      "Epoch: 37  Step:  54  training loss:  0.0030231752898544073\n",
      "Epoch: 37  Step:  55  training loss:  0.002093629213050008\n",
      "Epoch: 38  Step:  0  training loss:  0.003420759690925479\n",
      "Epoch: 38  Step:  1  training loss:  0.0021135620772838593\n",
      "Epoch: 38  Step:  2  training loss:  0.00253797834739089\n",
      "Epoch: 38  Step:  3  training loss:  0.0019421817269176245\n",
      "Epoch: 38  Step:  4  training loss:  0.0020577656105160713\n",
      "Epoch: 38  Step:  5  training loss:  0.0018020917195826769\n",
      "Epoch: 38  Step:  6  training loss:  0.0027264675591140985\n",
      "Epoch: 38  Step:  7  training loss:  0.002393307164311409\n",
      "Epoch: 38  Step:  8  training loss:  0.0022933189757168293\n",
      "Epoch: 38  Step:  9  training loss:  0.002543170703575015\n",
      "Epoch: 38  Step:  10  training loss:  0.002440712647512555\n",
      "Epoch: 38  Step:  11  training loss:  0.00212531303986907\n",
      "Epoch: 38  Step:  12  training loss:  0.002451038220897317\n",
      "Epoch: 38  Step:  13  training loss:  0.0026912123430520296\n",
      "Epoch: 38  Step:  14  training loss:  0.002566843293607235\n",
      "Epoch: 38  Step:  15  training loss:  0.0027814796194434166\n",
      "Epoch: 38  Step:  16  training loss:  0.00213308772072196\n",
      "Epoch: 38  Step:  17  training loss:  0.0020836247131228447\n",
      "Epoch: 38  Step:  18  training loss:  0.01336047425866127\n",
      "Epoch: 38  Step:  19  training loss:  0.002163559664040804\n",
      "Epoch: 38  Step:  20  training loss:  0.002082157414406538\n",
      "Epoch: 38  Step:  21  training loss:  0.0018630576087161899\n",
      "Epoch: 38  Step:  22  training loss:  0.002513859886676073\n",
      "Epoch: 38  Step:  23  training loss:  0.0023322231136262417\n",
      "Epoch: 38  Step:  24  training loss:  0.0030454250518232584\n",
      "Epoch: 38  Step:  25  training loss:  0.002095687435939908\n",
      "Epoch: 38  Step:  26  training loss:  0.002092816634103656\n",
      "Epoch: 38  Step:  27  training loss:  0.002340467181056738\n",
      "Epoch: 38  Step:  28  training loss:  0.002647300949320197\n",
      "Epoch: 38  Step:  29  training loss:  0.0023248675279319286\n",
      "Epoch: 38  Step:  30  training loss:  0.005974948406219482\n",
      "Epoch: 38  Step:  31  training loss:  0.0018838575342670083\n",
      "Epoch: 38  Step:  32  training loss:  0.0032391983550041914\n",
      "Epoch: 38  Step:  33  training loss:  0.002030844334512949\n",
      "Epoch: 38  Step:  34  training loss:  0.0017999623669311404\n",
      "Epoch: 38  Step:  35  training loss:  0.003012480214238167\n",
      "Epoch: 38  Step:  36  training loss:  0.002181055722758174\n",
      "Epoch: 38  Step:  37  training loss:  0.0023723950143903494\n",
      "Epoch: 38  Step:  38  training loss:  0.0031116914469748735\n",
      "Epoch: 38  Step:  39  training loss:  0.0028498214669525623\n",
      "Epoch: 38  Step:  40  training loss:  0.0022865086793899536\n",
      "Epoch: 38  Step:  41  training loss:  0.0025450079701840878\n",
      "Epoch: 38  Step:  42  training loss:  0.002493929350748658\n",
      "Epoch: 38  Step:  43  training loss:  0.0026009678840637207\n",
      "Epoch: 38  Step:  44  training loss:  0.0030384124256670475\n",
      "Epoch: 38  Step:  45  training loss:  0.0021981350146234035\n",
      "Epoch: 38  Step:  46  training loss:  0.0030189717654138803\n",
      "Epoch: 38  Step:  47  training loss:  0.0030668955296278\n",
      "Epoch: 38  Step:  48  training loss:  0.0021391864866018295\n",
      "Epoch: 38  Step:  49  training loss:  0.0029517116490751505\n",
      "Epoch: 38  Step:  50  training loss:  0.0020780409686267376\n",
      "Epoch: 38  Step:  51  training loss:  0.002083011670038104\n",
      "Epoch: 38  Step:  52  training loss:  0.0015329292509704828\n",
      "Epoch: 38  Step:  53  training loss:  0.0024029253982007504\n",
      "Epoch: 38  Step:  54  training loss:  0.002116014016792178\n",
      "Epoch: 38  Step:  55  training loss:  0.0018104418413713574\n",
      "Epoch: 39  Step:  0  training loss:  0.001970599638298154\n",
      "Epoch: 39  Step:  1  training loss:  0.0024306203704327345\n",
      "Epoch: 39  Step:  2  training loss:  0.0026977991219609976\n",
      "Epoch: 39  Step:  3  training loss:  0.0017718315357342362\n",
      "Epoch: 39  Step:  4  training loss:  0.0021035342942923307\n",
      "Epoch: 39  Step:  5  training loss:  0.0022501726634800434\n",
      "Epoch: 39  Step:  6  training loss:  0.004635556135326624\n",
      "Epoch: 39  Step:  7  training loss:  0.001408603391610086\n",
      "Epoch: 39  Step:  8  training loss:  0.002572472207248211\n",
      "Epoch: 39  Step:  9  training loss:  0.0023292875848710537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39  Step:  10  training loss:  0.0025227402802556753\n",
      "Epoch: 39  Step:  11  training loss:  0.0025760724674910307\n",
      "Epoch: 39  Step:  12  training loss:  0.0021322262473404408\n",
      "Epoch: 39  Step:  13  training loss:  0.0020670839585363865\n",
      "Epoch: 39  Step:  14  training loss:  0.00604125065729022\n",
      "Epoch: 39  Step:  15  training loss:  0.0027200374752283096\n",
      "Epoch: 39  Step:  16  training loss:  0.002177854534238577\n",
      "Epoch: 39  Step:  17  training loss:  0.002178773982450366\n",
      "Epoch: 39  Step:  18  training loss:  0.002414335496723652\n",
      "Epoch: 39  Step:  19  training loss:  0.002555476501584053\n",
      "Epoch: 39  Step:  20  training loss:  0.002847104100510478\n",
      "Epoch: 39  Step:  21  training loss:  0.001729257288388908\n",
      "Epoch: 39  Step:  22  training loss:  0.0019990585278719664\n",
      "Epoch: 39  Step:  23  training loss:  0.0022551019210368395\n",
      "Epoch: 39  Step:  24  training loss:  0.003037111833691597\n",
      "Epoch: 39  Step:  25  training loss:  0.0031667116563767195\n",
      "Epoch: 39  Step:  26  training loss:  0.0017983498983085155\n",
      "Epoch: 39  Step:  27  training loss:  0.0019154734909534454\n",
      "Epoch: 39  Step:  28  training loss:  0.002252269769087434\n",
      "Epoch: 39  Step:  29  training loss:  0.002526594791561365\n",
      "Epoch: 39  Step:  30  training loss:  0.002240927889943123\n",
      "Epoch: 39  Step:  31  training loss:  0.001958872191607952\n",
      "Epoch: 39  Step:  32  training loss:  0.00208108639344573\n",
      "Epoch: 39  Step:  33  training loss:  0.0020287304650992155\n",
      "Epoch: 39  Step:  34  training loss:  0.0018781108083203435\n",
      "Epoch: 39  Step:  35  training loss:  0.002828475320711732\n",
      "Epoch: 39  Step:  36  training loss:  0.008836915716528893\n",
      "Epoch: 39  Step:  37  training loss:  0.0029348668176680803\n",
      "Epoch: 39  Step:  38  training loss:  0.0026880172081291676\n",
      "Epoch: 39  Step:  39  training loss:  0.002212297869846225\n",
      "Epoch: 39  Step:  40  training loss:  0.0032138132955878973\n",
      "Epoch: 39  Step:  41  training loss:  0.0024275623727589846\n",
      "Epoch: 39  Step:  42  training loss:  0.00214191060513258\n",
      "Epoch: 39  Step:  43  training loss:  0.0030673870351165533\n",
      "Epoch: 39  Step:  44  training loss:  0.0024458596017211676\n",
      "Epoch: 39  Step:  45  training loss:  0.002636043820530176\n",
      "Epoch: 39  Step:  46  training loss:  0.0026546779554337263\n",
      "Epoch: 39  Step:  47  training loss:  0.0021074104588478804\n",
      "Epoch: 39  Step:  48  training loss:  0.003367820056155324\n",
      "Epoch: 39  Step:  49  training loss:  0.0020811271388083696\n",
      "Epoch: 39  Step:  50  training loss:  0.002266403054818511\n",
      "Epoch: 39  Step:  51  training loss:  0.005549857392907143\n",
      "Epoch: 39  Step:  52  training loss:  0.0028204978443682194\n",
      "Epoch: 39  Step:  53  training loss:  0.003187742317095399\n",
      "Epoch: 39  Step:  54  training loss:  0.001943047158420086\n",
      "Epoch: 39  Step:  55  training loss:  0.0019999141804873943\n",
      "Epoch: 40  Step:  0  training loss:  0.0018370019970461726\n",
      "Epoch: 40  Step:  1  training loss:  0.0016737966798245907\n",
      "Epoch: 40  Step:  2  training loss:  0.0025833614636212587\n",
      "Epoch: 40  Step:  3  training loss:  0.0024611575063318014\n",
      "Epoch: 40  Step:  4  training loss:  0.002297397004440427\n",
      "Epoch: 40  Step:  5  training loss:  0.0033206779044121504\n",
      "Epoch: 40  Step:  6  training loss:  0.001744006178341806\n",
      "Epoch: 40  Step:  7  training loss:  0.0029861945658922195\n",
      "Epoch: 40  Step:  8  training loss:  0.001973395701497793\n",
      "Epoch: 40  Step:  9  training loss:  0.002596844919025898\n",
      "Epoch: 40  Step:  10  training loss:  0.008450793102383614\n",
      "Epoch: 40  Step:  11  training loss:  0.002646852983161807\n",
      "Epoch: 40  Step:  12  training loss:  0.004269219469279051\n",
      "Epoch: 40  Step:  13  training loss:  0.0024218305479735136\n",
      "Epoch: 40  Step:  14  training loss:  0.002553787315264344\n",
      "Epoch: 40  Step:  15  training loss:  0.0033181095495820045\n",
      "Epoch: 40  Step:  16  training loss:  0.0016367409843951464\n",
      "Epoch: 40  Step:  17  training loss:  0.002538938308134675\n",
      "Epoch: 40  Step:  18  training loss:  0.0027204432990401983\n",
      "Epoch: 40  Step:  19  training loss:  0.0032124596182256937\n",
      "Epoch: 40  Step:  20  training loss:  0.002397236181423068\n",
      "Epoch: 40  Step:  21  training loss:  0.0017006939742714167\n",
      "Epoch: 40  Step:  22  training loss:  0.002126808511093259\n",
      "Epoch: 40  Step:  23  training loss:  0.0022437339648604393\n",
      "Epoch: 40  Step:  24  training loss:  0.006152842193841934\n",
      "Epoch: 40  Step:  25  training loss:  0.0016472149873152375\n",
      "Epoch: 40  Step:  26  training loss:  0.005388536024838686\n",
      "Epoch: 40  Step:  27  training loss:  0.0025757772382348776\n",
      "Epoch: 40  Step:  28  training loss:  0.002220176625996828\n",
      "Epoch: 40  Step:  29  training loss:  0.002162773860618472\n",
      "Epoch: 40  Step:  30  training loss:  0.002574104117229581\n",
      "Epoch: 40  Step:  31  training loss:  0.0035442248918116093\n",
      "Epoch: 40  Step:  32  training loss:  0.0023863280657678843\n",
      "Epoch: 40  Step:  33  training loss:  0.002171992091462016\n",
      "Epoch: 40  Step:  34  training loss:  0.002799074398353696\n",
      "Epoch: 40  Step:  35  training loss:  0.0024170419201254845\n",
      "Epoch: 40  Step:  36  training loss:  0.0016956799663603306\n",
      "Epoch: 40  Step:  37  training loss:  0.0023330983240157366\n",
      "Epoch: 40  Step:  38  training loss:  0.0019005637150257826\n",
      "Epoch: 40  Step:  39  training loss:  0.0020822170190513134\n",
      "Epoch: 40  Step:  40  training loss:  0.002290040021762252\n",
      "Epoch: 40  Step:  41  training loss:  0.0029117607045918703\n",
      "Epoch: 40  Step:  42  training loss:  0.0032344746869057417\n",
      "Epoch: 40  Step:  43  training loss:  0.0019671849440783262\n",
      "Epoch: 40  Step:  44  training loss:  0.002071206923574209\n",
      "Epoch: 40  Step:  45  training loss:  0.00302218203432858\n",
      "Epoch: 40  Step:  46  training loss:  0.0018913598032668233\n",
      "Epoch: 40  Step:  47  training loss:  0.0023002242669463158\n",
      "Epoch: 40  Step:  48  training loss:  0.0025020847097039223\n",
      "Epoch: 40  Step:  49  training loss:  0.002255295868963003\n",
      "Epoch: 40  Step:  50  training loss:  0.002129349624738097\n",
      "Epoch: 40  Step:  51  training loss:  0.0026976580265909433\n",
      "Epoch: 40  Step:  52  training loss:  0.00233114929869771\n",
      "Epoch: 40  Step:  53  training loss:  0.001939048059284687\n",
      "Epoch: 40  Step:  54  training loss:  0.0027242854703217745\n",
      "Epoch: 40  Step:  55  training loss:  0.0031987836118787527\n",
      "Epoch: 41  Step:  0  training loss:  0.002953276736661792\n",
      "Epoch: 41  Step:  1  training loss:  0.0019841098692268133\n",
      "Epoch: 41  Step:  2  training loss:  0.00218956358730793\n",
      "Epoch: 41  Step:  3  training loss:  0.0026637562550604343\n",
      "Epoch: 41  Step:  4  training loss:  0.0027211413253098726\n",
      "Epoch: 41  Step:  5  training loss:  0.002920371014624834\n",
      "Epoch: 41  Step:  6  training loss:  0.0029823698569089174\n",
      "Epoch: 41  Step:  7  training loss:  0.0027374911587685347\n",
      "Epoch: 41  Step:  8  training loss:  0.0022280460689216852\n",
      "Epoch: 41  Step:  9  training loss:  0.002325776731595397\n",
      "Epoch: 41  Step:  10  training loss:  0.0031935053411871195\n",
      "Epoch: 41  Step:  11  training loss:  0.005316914524883032\n",
      "Epoch: 41  Step:  12  training loss:  0.0019152957247570157\n",
      "Epoch: 41  Step:  13  training loss:  0.002178689232096076\n",
      "Epoch: 41  Step:  14  training loss:  0.0022759721614420414\n",
      "Epoch: 41  Step:  15  training loss:  0.0037557592149823904\n",
      "Epoch: 41  Step:  16  training loss:  0.0022639285307377577\n",
      "Epoch: 41  Step:  17  training loss:  0.0022826248314231634\n",
      "Epoch: 41  Step:  18  training loss:  0.0020108656026422977\n",
      "Epoch: 41  Step:  19  training loss:  0.0030578149016946554\n",
      "Epoch: 41  Step:  20  training loss:  0.001979280961677432\n",
      "Epoch: 41  Step:  21  training loss:  0.0020104616414755583\n",
      "Epoch: 41  Step:  22  training loss:  0.0019767286721616983\n",
      "Epoch: 41  Step:  23  training loss:  0.0022237927187234163\n",
      "Epoch: 41  Step:  24  training loss:  0.0028960227500647306\n",
      "Epoch: 41  Step:  25  training loss:  0.0043938253074884415\n",
      "Epoch: 41  Step:  26  training loss:  0.002436114475131035\n",
      "Epoch: 41  Step:  27  training loss:  0.0020163708832114935\n",
      "Epoch: 41  Step:  28  training loss:  0.002367059700191021\n",
      "Epoch: 41  Step:  29  training loss:  0.00226021371781826\n",
      "Epoch: 41  Step:  30  training loss:  0.0018732354510575533\n",
      "Epoch: 41  Step:  31  training loss:  0.002300403080880642\n",
      "Epoch: 41  Step:  32  training loss:  0.001903527183458209\n",
      "Epoch: 41  Step:  33  training loss:  0.0023509154561907053\n",
      "Epoch: 41  Step:  34  training loss:  0.004337615333497524\n",
      "Epoch: 41  Step:  35  training loss:  0.002012689830735326\n",
      "Epoch: 41  Step:  36  training loss:  0.0029451909940689802\n",
      "Epoch: 41  Step:  37  training loss:  0.0023380680941045284\n",
      "Epoch: 41  Step:  38  training loss:  0.0022127735428512096\n",
      "Epoch: 41  Step:  39  training loss:  0.002534837694838643\n",
      "Epoch: 41  Step:  40  training loss:  0.0023449044674634933\n",
      "Epoch: 41  Step:  41  training loss:  0.0026546490844339132\n",
      "Epoch: 41  Step:  42  training loss:  0.002365665975958109\n",
      "Epoch: 41  Step:  43  training loss:  0.0018252214649692178\n",
      "Epoch: 41  Step:  44  training loss:  0.005590884480625391\n",
      "Epoch: 41  Step:  45  training loss:  0.0016440765466541052\n",
      "Epoch: 41  Step:  46  training loss:  0.0025296437088400126\n",
      "Epoch: 41  Step:  47  training loss:  0.001665216637775302\n",
      "Epoch: 41  Step:  48  training loss:  0.002461907919496298\n",
      "Epoch: 41  Step:  49  training loss:  0.0023662971798330545\n",
      "Epoch: 41  Step:  50  training loss:  0.002327383728697896\n",
      "Epoch: 41  Step:  51  training loss:  0.0022200364619493484\n",
      "Epoch: 41  Step:  52  training loss:  0.008472895249724388\n",
      "Epoch: 41  Step:  53  training loss:  0.00244702841155231\n",
      "Epoch: 41  Step:  54  training loss:  0.0016163609689101577\n",
      "Epoch: 41  Step:  55  training loss:  0.0030168406665325165\n",
      "Epoch: 42  Step:  0  training loss:  0.006105480249971151\n",
      "Epoch: 42  Step:  1  training loss:  0.002862208988517523\n",
      "Epoch: 42  Step:  2  training loss:  0.003057963214814663\n",
      "Epoch: 42  Step:  3  training loss:  0.001968672964721918\n",
      "Epoch: 42  Step:  4  training loss:  0.002626242348924279\n",
      "Epoch: 42  Step:  5  training loss:  0.0031206589192152023\n",
      "Epoch: 42  Step:  6  training loss:  0.0025447141379117966\n",
      "Epoch: 42  Step:  7  training loss:  0.00230522477068007\n",
      "Epoch: 42  Step:  8  training loss:  0.0023526798468083143\n",
      "Epoch: 42  Step:  9  training loss:  0.0017767032841220498\n",
      "Epoch: 42  Step:  10  training loss:  0.0021322788670659065\n",
      "Epoch: 42  Step:  11  training loss:  0.001710015581920743\n",
      "Epoch: 42  Step:  12  training loss:  0.0032288816291838884\n",
      "Epoch: 42  Step:  13  training loss:  0.0022944489028304815\n",
      "Epoch: 42  Step:  14  training loss:  0.0024931214284151793\n",
      "Epoch: 42  Step:  15  training loss:  0.002230088459327817\n",
      "Epoch: 42  Step:  16  training loss:  0.003107505152001977\n",
      "Epoch: 42  Step:  17  training loss:  0.0025134917814284563\n",
      "Epoch: 42  Step:  18  training loss:  0.0019000344909727573\n",
      "Epoch: 42  Step:  19  training loss:  0.0023549855686724186\n",
      "Epoch: 42  Step:  20  training loss:  0.003606189275160432\n",
      "Epoch: 42  Step:  21  training loss:  0.001559085096232593\n",
      "Epoch: 42  Step:  22  training loss:  0.002174126449972391\n",
      "Epoch: 42  Step:  23  training loss:  0.002798639005050063\n",
      "Epoch: 42  Step:  24  training loss:  0.0025780396535992622\n",
      "Epoch: 42  Step:  25  training loss:  0.0020693500991910696\n",
      "Epoch: 42  Step:  26  training loss:  0.0025576958432793617\n",
      "Epoch: 42  Step:  27  training loss:  0.004969537258148193\n",
      "Epoch: 42  Step:  28  training loss:  0.0019169935258105397\n",
      "Epoch: 42  Step:  29  training loss:  0.002437412738800049\n",
      "Epoch: 42  Step:  30  training loss:  0.0026742115151137114\n",
      "Epoch: 42  Step:  31  training loss:  0.0022103693336248398\n",
      "Epoch: 42  Step:  32  training loss:  0.0022395611740648746\n",
      "Epoch: 42  Step:  33  training loss:  0.002709656720981002\n",
      "Epoch: 42  Step:  34  training loss:  0.0019089889246970415\n",
      "Epoch: 42  Step:  35  training loss:  0.0033906742464751005\n",
      "Epoch: 42  Step:  36  training loss:  0.002620374783873558\n",
      "Epoch: 42  Step:  37  training loss:  0.0022315767128020525\n",
      "Epoch: 42  Step:  38  training loss:  0.002347102388739586\n",
      "Epoch: 42  Step:  39  training loss:  0.002422909252345562\n",
      "Epoch: 42  Step:  40  training loss:  0.0017749572871252894\n",
      "Epoch: 42  Step:  41  training loss:  0.002519836649298668\n",
      "Epoch: 42  Step:  42  training loss:  0.0034962566569447517\n",
      "Epoch: 42  Step:  43  training loss:  0.0018256179755553603\n",
      "Epoch: 42  Step:  44  training loss:  0.0028765241149812937\n",
      "Epoch: 42  Step:  45  training loss:  0.002201476600021124\n",
      "Epoch: 42  Step:  46  training loss:  0.0026248313952237368\n",
      "Epoch: 42  Step:  47  training loss:  0.0018184739165008068\n",
      "Epoch: 42  Step:  48  training loss:  0.0028140246868133545\n",
      "Epoch: 42  Step:  49  training loss:  0.0014292929554358125\n",
      "Epoch: 42  Step:  50  training loss:  0.0036737355403602123\n",
      "Epoch: 42  Step:  51  training loss:  0.002026079222559929\n",
      "Epoch: 42  Step:  52  training loss:  0.00895645096898079\n",
      "Epoch: 42  Step:  53  training loss:  0.0021438472904264927\n",
      "Epoch: 42  Step:  54  training loss:  0.0019717367831617594\n",
      "Epoch: 42  Step:  55  training loss:  0.001606570091098547\n"
     ]
    }
   ],
   "source": [
    "#find optimal training epoch\n",
    "for epoch in range(43):\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        \n",
    "        x = Variable(batch_x.float())\n",
    "        y = Variable(batch_y.float())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = net(x)\n",
    "        train_loss = loss_func(prediction, y)\n",
    "        print('Epoch:', epoch, ' Step: ', step,' training loss: ',train_loss.data[0])        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = net(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13430915520419529"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test.data, prediction.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0023677375"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test.data, prediction.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=11,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RF\n",
    "\n",
    "##Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(n_X, n_y, test_size=0.25, random_state=42)\n",
    "\n",
    "##Add a Bias Variable\n",
    "X_train=np.hstack((X_train,np.ones((X_train.shape[0],1))))\n",
    "X_test=np.hstack((X_test,np.ones((X_test.shape[0],1))))\n",
    "\n",
    "X = np.nan_to_num(X_train)\n",
    "y = y_train.values\n",
    "regr = RandomForestRegressor(max_depth=11, random_state=42,n_estimators=100)\n",
    "regr.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 of best reduced form regressor model:  0.0685188891493\n",
      "MSE of best reduced form regressor model:  0.00194435739202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##Get R^2 and MSE error\n",
    "print('R^2 of best reduced form regressor model: ',r2_score(y_test.values, regr.predict(np.nan_to_num(X_test))))\n",
    "##0.106690850879\n",
    "\n",
    "print('MSE of best reduced form regressor model: ',mean_squared_error(y_test.values, regr.predict(np.nan_to_num(X_test))))\n",
    "##0.00185661687535"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81260553902315136"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(regr.predict(np.nan_to_num(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01457440800938045"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(regr.predict(np.nan_to_num(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD0CAYAAACVbe2MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b04fda93c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(regr.predict(np.nan_to_num(X_test)), bins=50)  # arguments are passed to np.histogram\n",
    "plt.xlim(xmin=0.7, xmax = 0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_importances:\n",
      "x_instate_ba_dm 0.069308686\n",
      "x_b10s_dm 0.0479696634\n",
      "x_mainline_dm 0.0467233361\n",
      "x_sdem_dm 0.0433034768\n",
      "x_pssenate_dm 0.0429862463\n",
      "x_ageon40orless_dm 0.039850547\n",
      "x_agecommi_dm 0.0285297318\n",
      "x_unity_dm 0.0283212245\n",
      "x_pgovt_dm 0.0237391068\n",
      "x_b20s_dm 0.0235830425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print Top 10 Feature Importance\n",
    "d=(sorted(zip(map(lambda x: round(x, 10), regr.feature_importances_), list(rf[cols].columns)+['intercept']), \n",
    "             reverse=True))\n",
    "print('feature_importances:')\n",
    "importances=[]\n",
    "features=[]\n",
    "for (i,j) in d[:10]:\n",
    "    importances.append(i)\n",
    "    features.append(j)\n",
    "    print(j,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = rf.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEPCAYAAAA6WX8sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cbdfb10320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEBCAYAAACZhwWsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cbde60a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Plot feature importances\n",
    "plt.figure(1)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(d[:10])), importances[::-1], align='center')\n",
    "plt.yticks(range(len(d[:10])), features[::-1])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "\n",
    "## Plot Real vs Predicted scatter plot\n",
    "plt.scatter(y_test.values,regr.predict(np.nan_to_num(X_test)))\n",
    "plt.xlabel('Real_target')\n",
    "plt.ylabel('Predicted_target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########this is when turning this into classification problem by categorize sentencing length changes\n",
    "\n",
    "##Catergorize feature\n",
    "def cat(v):\n",
    "    if v>(-0.5) and v<0.5:\n",
    "        return 0\n",
    "    elif v>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "X=rf[['1',\n",
    "       '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n",
    "       '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24','25']]\n",
    "\n",
    "Y=rf.length_3m_dif\n",
    "\n",
    "##Catergorize feature\n",
    "Y=Y.apply(cat)\n",
    "##Rescale to Normalize\n",
    "n_X=(X-X.min())/(X.max()-X.min())\n",
    "\n",
    "##Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(n_X, Y, test_size=0.25, random_state=42)   \n",
    "\n",
    "\n",
    "X = np.nan_to_num(X_train)\n",
    "y = y_train.values\n",
    "##Build Classifier\n",
    "classifier =ensemble.GradientBoostingClassifier(max_depth=5)\n",
    "classifier.fit(X,y)\n",
    "\n",
    "##Get predicted category\n",
    "y_pred = classifier.predict(np.nan_to_num(X_test))\n",
    "reversefactor = dict(zip([-1,0,1],['Decrease','NotChanged','Increase']))\n",
    "y_testing = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   Decrease  Increase  NotChanged\n",
      "Actual                                    \n",
      "Decrease         270       270          55\n",
      "Increase         186       511          86\n",
      "NotChanged       114       217         138\n",
      "F1 Score is:  0.497563616676\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Making the Confusion Matrix\n",
    "print(pd.crosstab(y_testing, y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
    "print('F1 Score is: ',f1_score(y_testing, y_pred, labels=None, average='micro', sample_weight=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inference_data = pd.read_csv('bio_txt2.csv')\n",
    "\n",
    "rf=inference_data[['1',\n",
    "       '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n",
    "       '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24','25','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "########this is when turning this into classification problem by categorize sentencing length changes\n",
    "\n",
    "##Catergorize feature\n",
    "def cat(v):\n",
    "    if v>(-0.5) and v<0.5:\n",
    "        return 0\n",
    "    elif v>=0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "X=rf[['1',\n",
    "       '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n",
    "       '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24','25']]\n",
    "\n",
    "Y=rf.length_3m_dif\n",
    "\n",
    "##Catergorize feature\n",
    "Y=Y.apply(cat)\n",
    "##Rescale to Normalize\n",
    "n_X=(X-X.min())/(X.max()-X.min())\n",
    "\n",
    "##Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(n_X, Y, test_size=0.25, random_state=42)   \n",
    "\n",
    "\n",
    "X = np.nan_to_num(X_train)\n",
    "y = y_train.values\n",
    "##Build Classifier\n",
    "classifier =ensemble.GradientBoostingClassifier(max_depth=5)\n",
    "classifier.fit(X,y)\n",
    "\n",
    "##Get predicted category\n",
    "y_pred = classifier.predict(np.nan_to_num(X_test))\n",
    "reversefactor = dict(zip([-1,0,1],['Decrease','NotChanged','Increase']))\n",
    "y_testing = np.vectorize(reversefactor.get)(y_test)\n",
    "y_pred = np.vectorize(reversefactor.get)(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted   Decrease  Increase  NotChanged\n",
      "Actual                                    \n",
      "Decrease         195       361          39\n",
      "Increase         217       522          44\n",
      "NotChanged       126       322          21\n",
      "F1 Score is:  0.399566865187\n"
     ]
    }
   ],
   "source": [
    "#Making the Confusion Matrix\n",
    "print(pd.crosstab(y_testing, y_pred, rownames=['Actual'], colnames=['Predicted']))\n",
    "print('F1 Score is: ',f1_score(y_testing, y_pred, labels=None, average='micro', sample_weight=None))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
