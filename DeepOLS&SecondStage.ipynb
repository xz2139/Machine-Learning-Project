{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# General Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cc_merged_0429.csv\",sep='\\t',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>caseid</th>\n",
       "      <th>opinion_type</th>\n",
       "      <th>judge_name</th>\n",
       "      <th>decision</th>\n",
       "      <th>date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>...</th>\n",
       "      <th>month_3m_b</th>\n",
       "      <th>month_3_b</th>\n",
       "      <th>length_3m_dif</th>\n",
       "      <th>txt</th>\n",
       "      <th>x_dem_p</th>\n",
       "      <th>length_3m_af</th>\n",
       "      <th>length_3m_be</th>\n",
       "      <th>length_be_dm</th>\n",
       "      <th>length_af_dm</th>\n",
       "      <th>length_dif_dm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2304</td>\n",
       "      <td>1993</td>\n",
       "      <td>X35807</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>PER CURIAM</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1993-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1993-05-04</td>\n",
       "      <td>1993-05-04</td>\n",
       "      <td>-1.099144</td>\n",
       "      <td>per curiam: charo appeals his sentence for co...</td>\n",
       "      <td>0.292902</td>\n",
       "      <td>49.758998</td>\n",
       "      <td>50.858142</td>\n",
       "      <td>-3.877441</td>\n",
       "      <td>-4.976585</td>\n",
       "      <td>-1.099144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2796</td>\n",
       "      <td>1992</td>\n",
       "      <td>X3AD9D</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1992-11-13</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-08-13</td>\n",
       "      <td>1992-08-13</td>\n",
       "      <td>1.663181</td>\n",
       "      <td>logan , circuit judge. the only issue in this...</td>\n",
       "      <td>0.328425</td>\n",
       "      <td>52.334119</td>\n",
       "      <td>50.670939</td>\n",
       "      <td>-5.166051</td>\n",
       "      <td>-3.502871</td>\n",
       "      <td>1.663181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2805</td>\n",
       "      <td>1992</td>\n",
       "      <td>X3ADTB</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>reversed</td>\n",
       "      <td>1992-12-29</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1992-09-29</td>\n",
       "      <td>1992-09-29</td>\n",
       "      <td>-3.563471</td>\n",
       "      <td>logan , circuit judge. defendant bernard d. r...</td>\n",
       "      <td>0.328425</td>\n",
       "      <td>50.282495</td>\n",
       "      <td>53.845966</td>\n",
       "      <td>-1.991025</td>\n",
       "      <td>-5.554496</td>\n",
       "      <td>-3.563471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2814</td>\n",
       "      <td>1997</td>\n",
       "      <td>X37GPR</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>JOHN C</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1997-02-11</td>\n",
       "      <td>PORFILIO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1996-11-11</td>\n",
       "      <td>1996-11-11</td>\n",
       "      <td>0.760434</td>\n",
       "      <td>john c. porfilio , circuit judge. defendant b...</td>\n",
       "      <td>0.360773</td>\n",
       "      <td>51.570342</td>\n",
       "      <td>50.809909</td>\n",
       "      <td>-5.027082</td>\n",
       "      <td>-4.266648</td>\n",
       "      <td>0.760434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2816</td>\n",
       "      <td>1999</td>\n",
       "      <td>X4QAM7</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1998-10-04</td>\n",
       "      <td>1998-10-04</td>\n",
       "      <td>-0.630426</td>\n",
       "      <td>holloway , circuit judge. mr. unser brings th...</td>\n",
       "      <td>0.411135</td>\n",
       "      <td>59.568882</td>\n",
       "      <td>60.199308</td>\n",
       "      <td>4.362317</td>\n",
       "      <td>3.731892</td>\n",
       "      <td>-0.630426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 261 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index  year  caseid  opinion_type  judge_name  decision  \\\n",
       "0           0   2304  1993  X35807  contentMajOp  PER CURIAM  affirmed   \n",
       "1           1   2796  1992  X3AD9D  contentMajOp       LOGAN  affirmed   \n",
       "2           2   2805  1992  X3ADTB  contentMajOp       LOGAN  reversed   \n",
       "3           3   2814  1997  X37GPR  contentMajOp      JOHN C  affirmed   \n",
       "4           4   2816  1999  X4QAM7  contentMajOp    HOLLOWAY  affirmed   \n",
       "\n",
       "         date    Author  Affirmed      ...        month_3m_b   month_3_b  \\\n",
       "0  1993-08-04       NaN       1.0      ...        1993-05-04  1993-05-04   \n",
       "1  1992-11-13     LOGAN       1.0      ...        1992-08-13  1992-08-13   \n",
       "2  1992-12-29     LOGAN       0.0      ...        1992-09-29  1992-09-29   \n",
       "3  1997-02-11  PORFILIO       1.0      ...        1996-11-11  1996-11-11   \n",
       "4  1999-01-04  HOLLOWAY       1.0      ...        1998-10-04  1998-10-04   \n",
       "\n",
       "   length_3m_dif                                                txt   x_dem_p  \\\n",
       "0      -1.099144   per curiam: charo appeals his sentence for co...  0.292902   \n",
       "1       1.663181   logan , circuit judge. the only issue in this...  0.328425   \n",
       "2      -3.563471   logan , circuit judge. defendant bernard d. r...  0.328425   \n",
       "3       0.760434   john c. porfilio , circuit judge. defendant b...  0.360773   \n",
       "4      -0.630426   holloway , circuit judge. mr. unser brings th...  0.411135   \n",
       "\n",
       "   length_3m_af  length_3m_be  length_be_dm  length_af_dm  length_dif_dm  \n",
       "0     49.758998     50.858142     -3.877441     -4.976585      -1.099144  \n",
       "1     52.334119     50.670939     -5.166051     -3.502871       1.663181  \n",
       "2     50.282495     53.845966     -1.991025     -5.554496      -3.563471  \n",
       "3     51.570342     50.809909     -5.027082     -4.266648       0.760434  \n",
       "4     59.568882     60.199308      4.362317      3.731892      -0.630426  \n",
       "\n",
       "[5 rows x 261 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8434 entries, 0 to 8433\n",
      "Columns: 261 entries, Unnamed: 0 to length_dif_dm\n",
      "dtypes: float64(216), int64(19), object(26)\n",
      "memory usage: 16.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'index', 'year', 'caseid', 'opinion_type',\n",
       "       'judge_name', 'decision', 'date', 'Author', 'Affirmed',\n",
       "       'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated',\n",
       "       'VacatedInPart', 'Circuit', 'judgeid1', 'judgeid2', 'judgeid3',\n",
       "       'x_dem_x', 'x_republican_x', 'x_instate_ba_x', 'x_elev_x',\n",
       "       'x_unity_x', 'x_aba_x', 'x_crossa_x', 'x_pfedjdge_x',\n",
       "       'x_pindreg1_x', 'x_plawprof_x', 'x_pscab_x', 'x_pcab_x', 'x_pusa_x',\n",
       "       'x_pssenate_x', 'x_paag_x', 'x_psp_x', 'x_pslc_x', 'x_pssc_x',\n",
       "       'x_pshouse_x', 'x_psg_x', 'x_psgo_x', 'x_psenate_x', 'x_psatty_x',\n",
       "       'x_pprivate_x', 'x_pmayor_x', 'x_plocct_x', 'x_phouse_x',\n",
       "       'x_pgov_x', 'x_pda_x', 'x_pcc_x', 'x_pccoun_x', 'x_pausa_x',\n",
       "       'x_pasatty_x', 'x_pag_x', 'x_pada_x', 'x_pgovt_x', 'x_llm_sjd_x',\n",
       "       'x_protestant_x', 'x_evangelical_x', 'x_mainline_x',\n",
       "       'x_noreligion_x', 'x_catholic_x', 'x_jewish_x', 'x_black_x',\n",
       "       'x_nonwhite_x', 'x_female_x', 'x_jd_public_x', 'x_ba_public_x',\n",
       "       'x_b10s_x', 'x_b20s_x', 'x_b30s_x', 'x_b40s_x', 'x_b50s_x',\n",
       "       'x_pbank_x', 'x_pmag_x', 'x_ageon40s_x', 'x_ageon50s_x',\n",
       "       'x_ageon60s_x', 'x_ageon40orless_x', 'x_ageon70ormore_x',\n",
       "       'x_pago_x', 'x_apptoter_x', 'id_x', 'x_term_x', 'x_hdem_x',\n",
       "       'x_hrep_x', 'x_sdem_x', 'x_srep_x', 'x_hother_x', 'x_sother_x',\n",
       "       'x_agecommi_x', 'fullname_x', 'lastname_x', 'firstname_x',\n",
       "       'middlename_x', 'suffix_x', 'x_dem_y', 'x_republican_y',\n",
       "       'x_instate_ba_y', 'x_elev_y', 'x_unity_y', 'x_aba_y', 'x_crossa_y',\n",
       "       'x_pfedjdge_y', 'x_pindreg1_y', 'x_plawprof_y', 'x_pscab_y',\n",
       "       'x_pcab_y', 'x_pusa_y', 'x_pssenate_y', 'x_paag_y', 'x_psp_y',\n",
       "       'x_pslc_y', 'x_pssc_y', 'x_pshouse_y', 'x_psg_y', 'x_psgo_y',\n",
       "       'x_psenate_y', 'x_psatty_y', 'x_pprivate_y', 'x_pmayor_y',\n",
       "       'x_plocct_y', 'x_phouse_y', 'x_pgov_y', 'x_pda_y', 'x_pcc_y',\n",
       "       'x_pccoun_y', 'x_pausa_y', 'x_pasatty_y', 'x_pag_y', 'x_pada_y',\n",
       "       'x_pgovt_y', 'x_llm_sjd_y', 'x_protestant_y', 'x_evangelical_y',\n",
       "       'x_mainline_y', 'x_noreligion_y', 'x_catholic_y', 'x_jewish_y',\n",
       "       'x_black_y', 'x_nonwhite_y', 'x_female_y', 'x_jd_public_y',\n",
       "       'x_ba_public_y', 'x_b10s_y', 'x_b20s_y', 'x_b30s_y', 'x_b40s_y',\n",
       "       'x_b50s_y', 'x_pbank_y', 'x_pmag_y', 'x_ageon40s_y', 'x_ageon50s_y',\n",
       "       'x_ageon60s_y', 'x_ageon40orless_y', 'x_ageon70ormore_y',\n",
       "       'x_pago_y', 'x_apptoter_y', 'id_y', 'x_term_y', 'x_hdem_y',\n",
       "       'x_hrep_y', 'x_sdem_y', 'x_srep_y', 'x_hother_y', 'x_sother_y',\n",
       "       'x_agecommi_y', 'fullname_y', 'lastname_y', 'firstname_y',\n",
       "       'middlename_y', 'suffix_y', 'x_dem', 'x_republican', 'x_instate_ba',\n",
       "       'x_elev', 'x_unity', 'x_aba', 'x_crossa', 'x_pfedjdge',\n",
       "       'x_pindreg1', 'x_plawprof', 'x_pscab', 'x_pcab', 'x_pusa',\n",
       "       'x_pssenate', 'x_paag', 'x_psp', 'x_pslc', 'x_pssc', 'x_pshouse',\n",
       "       'x_psg', 'x_psgo', 'x_psenate', 'x_psatty', 'x_pprivate',\n",
       "       'x_pmayor', 'x_plocct', 'x_phouse', 'x_pgov', 'x_pda', 'x_pcc',\n",
       "       'x_pccoun', 'x_pausa', 'x_pasatty', 'x_pag', 'x_pada', 'x_pgovt',\n",
       "       'x_llm_sjd', 'x_protestant', 'x_evangelical', 'x_mainline',\n",
       "       'x_noreligion', 'x_catholic', 'x_jewish', 'x_black', 'x_nonwhite',\n",
       "       'x_female', 'x_jd_public', 'x_ba_public', 'x_b10s', 'x_b20s',\n",
       "       'x_b30s', 'x_b40s', 'x_b50s', 'x_pbank', 'x_pmag', 'x_ageon40s',\n",
       "       'x_ageon50s', 'x_ageon60s', 'x_ageon40orless', 'x_ageon70ormore',\n",
       "       'x_pago', 'x_apptoter', 'id', 'x_term', 'x_hdem', 'x_hrep',\n",
       "       'x_sdem', 'x_srep', 'x_hother', 'x_sother', 'x_agecommi',\n",
       "       'fullname', 'lastname', 'firstname', 'middlename', 'suffix',\n",
       "       'month_3', 'length_3m', 'month_6', 'length_6m', 'month_3m_b',\n",
       "       'month_3_b', 'length_3m_dif', 'txt', 'x_dem_p', 'length_3m_af',\n",
       "       'length_3m_be', 'length_be_dm', 'length_af_dm', 'length_dif_dm'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this part, we are going to use Affirmed/Reversed decision of the circuit court, combined with the txt feature of each case to predict the difference in sentencing length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_use = data[['index','Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','txt','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>txt</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2304</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>per curiam: charo appeals his sentence for co...</td>\n",
       "      <td>-1.099144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. the only issue in this...</td>\n",
       "      <td>1.663181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. defendant bernard d. r...</td>\n",
       "      <td>-3.563471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>john c. porfilio , circuit judge. defendant b...</td>\n",
       "      <td>0.760434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2816</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>holloway , circuit judge. mr. unser brings th...</td>\n",
       "      <td>-0.630426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "0   2304       1.0             0.0       0.0             0.0      0.0   \n",
       "1   2796       1.0             0.0       0.0             0.0      0.0   \n",
       "2   2805       0.0             0.0       1.0             0.0      0.0   \n",
       "3   2814       1.0             0.0       0.0             0.0      0.0   \n",
       "4   2816       1.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "   VacatedInPart                                                txt  \\\n",
       "0            0.0   per curiam: charo appeals his sentence for co...   \n",
       "1            0.0   logan , circuit judge. the only issue in this...   \n",
       "2            0.0   logan , circuit judge. defendant bernard d. r...   \n",
       "3            0.0   john c. porfilio , circuit judge. defendant b...   \n",
       "4            0.0   holloway , circuit judge. mr. unser brings th...   \n",
       "\n",
       "   length_3m_dif  \n",
       "0      -1.099144  \n",
       "1       1.663181  \n",
       "2      -3.563471  \n",
       "3       0.760434  \n",
       "4      -0.630426  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_use.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' per curiam: charo appeals his sentence for conspiracy to possess with intent to distribute cocaine and money laundering. he maintains that the district court erred in refusing to grant him a two-level reduction for acceptance of responsibility under u.s.s.g. 3e1.1(a) . he contends that he manifested acceptance of responsibility by admitting his involvement in the offense, showing remorse, and cooperating with law enforcement officials. the government responds that the district court\\'s finding that charo had not accepted responsibility for the offense conduct was not clearly erroneous. the sentencing court\\'s determination that a defendant is not entitled to a reduction for acceptance of responsibility is entitled to great deference and will not be overturned unless it is clearly erroneous. united states v. spraggins , 868 f.2d 1541, 1543 (11th cir.1989). section 3e1.1(a) requires a sentencing court to reduce the offense level by two levels \"if the defendant clearly demonstrates a recognition and affirmative acceptance of personal responsibility for his criminal conduct.\" in determining whether defendant qualifies for an acceptance of responsibility adjustment, the sentencing court may consider whether the defendant made a \"voluntary and truthful admission to authorities of involvement in the offense and related conduct.\" u.s.s.g. 3e1.1 , comment. (n. 1(c)). charo pleaded guilty to one count of conspiracy to possess with intent to distribute cocaine base and one count of money laundering. at the change of plea hearing, he admitted that from june through october of 1981, he transported cocaine base from houston, texas to mobile, alabama on several occasions. he further admitted that he transported approximately one-half a kilogram at a time. charo told the probation officer that he transported approximately one and one-half kilograms of cocaine base on four occasions. thus, the probation officer estimated that he transported four kilograms of cocaine base. at sentencing, he admitted to transporting a total of one and one-half kilograms of cocaine base. charo\\'s statement was not a truthful admission and did not show a clear acceptance of responsibility. the court, therefore, did not err in declining to grant a two-level reduction. the sentence and judgment are affirmed. '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_use['txt'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#train test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6326, 9)\n",
      "(2108, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.5.3/intel/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from numpy.random import RandomState\n",
    "\n",
    "RS1 = RandomState(1)\n",
    "train_data = data_use.sample(frac = 0.75, random_state = 200)\n",
    "test_data = data_use.drop(train_data.index)\n",
    "\n",
    "x_train = train_data['txt']\n",
    "x_test = test_data['txt']\n",
    "Y_train = train_data['length_3m_dif']\n",
    "Y_test = test_data['length_3m_dif']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tokenize the text using tfidf and count vectorizer, select the vectorizer\n",
    "# that perform better for our task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 41s, sys: 3.49 s, total: 3min 45s\n",
      "Wall time: 3min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vectorizer_1gram = CountVectorizer(stop_words = 'english', binary = False)\n",
    "count_vectorizer_2gram = CountVectorizer(stop_words = 'english', ngram_range = (1,2) , binary = False)\n",
    "tfid_vectorizer_1gram = TfidfVectorizer(stop_words = 'english', binary = False)\n",
    "tfid_vectorizer_2gram = TfidfVectorizer(stop_words = 'english', ngram_range= (1,2), binary = False)\n",
    "\n",
    "count_vectorizer_1gram.fit(x_train)\n",
    "count_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "tfid_vectorizer_1gram.fit(x_train)\n",
    "tfid_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "\n",
    "#count_vectorizer\n",
    "count_train_1gram = count_vectorizer_1gram.transform(x_train)\n",
    "count_test_1gram = count_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "count_train_2gram = count_vectorizer_2gram.transform(x_train)\n",
    "count_test_2gram = count_vectorizer_2gram.transform(x_test)\n",
    "\n",
    "#tfid_vectorizer\n",
    "tf_train_1gram = tfid_vectorizer_1gram.transform(x_train)\n",
    "tf_test_1gram = tfid_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "tf_train_2gram = tfid_vectorizer_2gram.transform(x_train)\n",
    "tf_test_2gram = tfid_vectorizer_2gram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#count 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_base_count_1gram = linear_model.LinearRegression()\n",
    "lr_base_count_1gram.fit(count_train_1gram, Y_train)\n",
    "lr_base_count_1gram_pred = lr_base_count_1gram.predict(count_test_1gram)\n",
    "count_1gram_base_mae = mean_absolute_error(Y_test, lr_base_count_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.69964025579824"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_1gram_base_mae_r2 = r2_score(Y_test, lr_base_count_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8035214992717261"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmWVyojAzs0xOFGZmlsmJwszMMv1/TNj0FSC02d8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1ab1ff17b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_count_1gram_pred)\n",
    "plt.xlabel('predicted difference in sentencing length')\n",
    "plt.ylabel('actual difference in sentencing length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#count 2 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_base_count_2gram = linear_model.LinearRegression()\n",
    "lr_base_count_2gram.fit(count_train_2gram, Y_train)\n",
    "lr_base_count_2gram_pred = lr_base_count_2gram.predict(count_test_2gram)\n",
    "count_2gram_base_mae = mean_absolute_error(Y_test, lr_base_count_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7585600238429082"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "count_2gram_base_mae_r2 = r2_score(Y_test, lr_base_count_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.53133179890467885"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1ab213cf28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_count_2gram_pred)\n",
    "plt.xlabel('predicted difference in sentencing length')\n",
    "plt.ylabel('actual difference in sentencing length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tfidf 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_base_tfidf_1gram = linear_model.LinearRegression()\n",
    "lr_base_tfidf_1gram.fit(tf_train_1gram, Y_train)\n",
    "lr_base_tfidf_1gram_pred = lr_base_tfidf_1gram.predict(tf_test_1gram)\n",
    "tfidf_1gram_base_mae = mean_absolute_error(Y_test, lr_base_tfidf_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7085882973015794"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_1gram_base_mae_r2 = r2_score(Y_test, lr_base_tfidf_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37162225423885853"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngWBmZoALBDMzS7lAMDMzAP4/r1z2o544YzUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1ab21be668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_tfidf_1gram_pred)\n",
    "plt.xlabel('predicted difference in sentencing length')\n",
    "plt.ylabel('actual difference in sentencing length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# tfidf 2 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_base_tfidf_2gram = linear_model.LinearRegression()\n",
    "lr_base_tfidf_2gram.fit(tf_train_2gram, Y_train)\n",
    "lr_base_tfidf_2gram_pred = lr_base_tfidf_2gram.predict(tf_test_2gram)\n",
    "tfidf_2gram_base_mae = mean_absolute_error(Y_test, lr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4516721052128747"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tfidf_2gram_base_mae_r2 = r2_score(Y_test, lr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.072263646659945113"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1ab2222470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_tfidf_2gram_pred)\n",
    "plt.xlabel('predicted difference in sentencing length')\n",
    "plt.ylabel('actual difference in sentencing length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "conclusion: \n",
    "from the above experiment, we see that using the same number of grams, TfIdf vectorizer always performs better than count vectorizer according the Mean Absolute error and R square. For the same vectorizer, performance is better for larger range of grams. This intuitively make senese, however, there is a tradeoff between performance and computing resources needed. Based on the analysis, we choose to use TfIdf vectorizer with ngram_range =(1,2) for the following steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#experiment with different algorithm to get an understanding of the predictive power of the text features.\n",
    "#provided in seperate .py file called model_performance.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# fitting the text feature combined with the reversed/affirm decision into a neural network. To see how our data perform in a deep nerual net model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00246613  0.00466438  0.0033177   0.00273369  0.00263453  0.00247129\n",
      "  0.00233313  0.00209636  0.00202032  0.00186841  0.00175678  0.00169007\n",
      "  0.00149295  0.00147398  0.00146348  0.00139788  0.00136532  0.00135417\n",
      "  0.0013412   0.00131931  0.00126959  0.00121461  0.00120349  0.0011521\n",
      "  0.00113406  0.00112939  0.00111907  0.00110137  0.0010816   0.00104323\n",
      "  0.00103074  0.00099091  0.00097298  0.00094979  0.00094327  0.00093012\n",
      "  0.00092094  0.00090753  0.00088763  0.00087595  0.00086402  0.00086237\n",
      "  0.00084706  0.00083933  0.00082278  0.00081538  0.00080988  0.00080583\n",
      "  0.00079532  0.00079216  0.00077237  0.00076855  0.00076194  0.00076138\n",
      "  0.0007418   0.00073655  0.00073447  0.00072773  0.00072099  0.00070834\n",
      "  0.00070705  0.00069969  0.00069347  0.00068781  0.0006811   0.00067255\n",
      "  0.00066227  0.00065802  0.00064985  0.00064482  0.00064278  0.00063622\n",
      "  0.00062344  0.000613    0.00061101  0.00060383  0.00059877  0.0005929\n",
      "  0.00058696  0.00058051]\n",
      "0.0906536781336\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import sparse_random_matrix\n",
    "\n",
    "#input sparse matrix: tf_train_2gram, tf_test_2gram\n",
    "svd = TruncatedSVD(n_components=80, n_iter=7, random_state=42)\n",
    "svd.fit(tf_train_2gram)  \n",
    "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=7,\n",
    "        random_state=42, tol=0.0)\n",
    "\n",
    "svd_train = svd.transform(tf_train_2gram)\n",
    "svd_test = svd.transform(tf_test_2gram)\n",
    "print(svd.explained_variance_ratio_)  \n",
    "print(svd.explained_variance_ratio_.sum())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "##dataframe approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df = pd.DataFrame(data = svd_train, index=train_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_test_df = pd.DataFrame(data = svd_test, index = test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df[['index','Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']] = train_data[['index','Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']]\n",
    "svd_test_df[['index','Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']] = test_data[['index','Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>index</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>0.136688</td>\n",
       "      <td>-0.040999</td>\n",
       "      <td>-0.004375</td>\n",
       "      <td>-0.007436</td>\n",
       "      <td>-0.006002</td>\n",
       "      <td>-0.034239</td>\n",
       "      <td>0.046198</td>\n",
       "      <td>-0.008634</td>\n",
       "      <td>-0.008428</td>\n",
       "      <td>0.013910</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033283</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>35638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.766357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.100533</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>-0.019895</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.014900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>-0.004419</td>\n",
       "      <td>24468</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>0.157620</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>-0.043520</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>-0.039799</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004125</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>39230</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.141970</td>\n",
       "      <td>-0.026448</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-0.054681</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>-0.037343</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>-0.010692</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>22883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.150298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.143912</td>\n",
       "      <td>-0.047580</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>-0.058490</td>\n",
       "      <td>0.036348</td>\n",
       "      <td>-0.016142</td>\n",
       "      <td>0.052118</td>\n",
       "      <td>-0.038125</td>\n",
       "      <td>-0.005855</td>\n",
       "      <td>0.016747</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>-0.002067</td>\n",
       "      <td>16551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.467751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "5577  0.136688 -0.040999 -0.004375 -0.007436 -0.006002 -0.034239  0.046198   \n",
       "2072  0.100533  0.021049  0.007305  0.001948  0.001179 -0.019895 -0.008439   \n",
       "7052  0.157620 -0.026298 -0.043520 -0.057402 -0.039799  0.026700  0.008013   \n",
       "1582  0.141970 -0.026448 -0.080584 -0.054681 -0.016293  0.081749 -0.037343   \n",
       "587   0.143912 -0.047580  0.005257 -0.058490  0.036348 -0.016142  0.052118   \n",
       "\n",
       "             7         8         9      ...              78        79  index  \\\n",
       "5577 -0.008634 -0.008428  0.013910      ...        0.033283  0.000926  35638   \n",
       "2072  0.018857 -0.024885 -0.014900      ...        0.003150 -0.004419  24468   \n",
       "7052 -0.012378 -0.011192 -0.021375      ...       -0.004125  0.002743  39230   \n",
       "1582  0.017813 -0.010692 -0.000912      ...        0.060335 -0.017806  22883   \n",
       "587  -0.038125 -0.005855  0.016747      ...        0.000847 -0.002067  16551   \n",
       "\n",
       "      Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "5577       0.0             0.0       0.0             1.0      0.0   \n",
       "2072       1.0             0.0       0.0             0.0      0.0   \n",
       "7052       1.0             0.0       0.0             0.0      0.0   \n",
       "1582       1.0             0.0       0.0             0.0      0.0   \n",
       "587        0.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "      VacatedInPart  length_3m_dif  \n",
       "5577            0.0      -0.766357  \n",
       "2072            0.0       0.608711  \n",
       "7052            0.0       0.228592  \n",
       "1582            0.0      -3.150298  \n",
       "587             0.0      -2.467751  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>index</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119560</td>\n",
       "      <td>-0.025109</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>-0.007404</td>\n",
       "      <td>0.016002</td>\n",
       "      <td>-0.019776</td>\n",
       "      <td>0.026109</td>\n",
       "      <td>-0.022353</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>0.016800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009664</td>\n",
       "      <td>-0.000136</td>\n",
       "      <td>2805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.563471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.225139</td>\n",
       "      <td>-0.005882</td>\n",
       "      <td>-0.016542</td>\n",
       "      <td>-0.041020</td>\n",
       "      <td>0.020823</td>\n",
       "      <td>-0.080195</td>\n",
       "      <td>-0.008058</td>\n",
       "      <td>0.229446</td>\n",
       "      <td>0.192719</td>\n",
       "      <td>-0.001701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024659</td>\n",
       "      <td>-0.018850</td>\n",
       "      <td>2817</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.591201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.335446</td>\n",
       "      <td>-0.088022</td>\n",
       "      <td>-0.039261</td>\n",
       "      <td>-0.161905</td>\n",
       "      <td>-0.059537</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.040520</td>\n",
       "      <td>0.014652</td>\n",
       "      <td>-0.039186</td>\n",
       "      <td>-0.038314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006725</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>3065</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.099370</td>\n",
       "      <td>-0.008746</td>\n",
       "      <td>0.038461</td>\n",
       "      <td>0.022873</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>-0.033024</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>-0.004343</td>\n",
       "      <td>-0.004816</td>\n",
       "      <td>0.044422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>3392</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.595323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.248678</td>\n",
       "      <td>-0.052280</td>\n",
       "      <td>-0.046642</td>\n",
       "      <td>-0.072317</td>\n",
       "      <td>-0.028221</td>\n",
       "      <td>0.027054</td>\n",
       "      <td>0.023326</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>-0.029729</td>\n",
       "      <td>-0.031930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.002373</td>\n",
       "      <td>3498</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.045180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6  \\\n",
       "2   0.119560 -0.025109  0.006438 -0.007404  0.016002 -0.019776  0.026109   \n",
       "5   0.225139 -0.005882 -0.016542 -0.041020  0.020823 -0.080195 -0.008058   \n",
       "17  0.335446 -0.088022 -0.039261 -0.161905 -0.059537  0.038900  0.040520   \n",
       "18  0.099370 -0.008746  0.038461  0.022873  0.025342 -0.033024  0.035557   \n",
       "23  0.248678 -0.052280 -0.046642 -0.072317 -0.028221  0.027054  0.023326   \n",
       "\n",
       "           7         8         9      ...              78        79  index  \\\n",
       "2  -0.022353 -0.004703  0.016800      ...        0.009664 -0.000136   2805   \n",
       "5   0.229446  0.192719 -0.001701      ...        0.024659 -0.018850   2817   \n",
       "17  0.014652 -0.039186 -0.038314      ...       -0.006725  0.027584   3065   \n",
       "18 -0.004343 -0.004816  0.044422      ...        0.001024  0.012109   3392   \n",
       "23  0.031732 -0.029729 -0.031930      ...        0.002573 -0.002373   3498   \n",
       "\n",
       "    Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "2        0.0             0.0       1.0             0.0      0.0   \n",
       "5        1.0             0.0       0.0             0.0      0.0   \n",
       "17       1.0             0.0       0.0             0.0      0.0   \n",
       "18       1.0             0.0       0.0             0.0      0.0   \n",
       "23       1.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "    VacatedInPart  length_3m_dif  \n",
       "2             0.0      -3.563471  \n",
       "5             0.0       3.591201  \n",
       "17            0.0       0.959357  \n",
       "18            0.0      -1.595323  \n",
       "23            0.0      -2.045180  \n",
       "\n",
       "[5 rows x 88 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_test_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "combined ['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df['res']=svd_train_df['Affirmed']+svd_train_df['AffirmedInPart']+svd_train_df['Reversed']+svd_train_df['ReversedInPart']+svd_train_df['Vacated']+svd_train_df['VacatedInPart']\n",
    "svd_test_df['res']=svd_test_df['Affirmed']+svd_test_df['AffirmedInPart']+svd_test_df['Reversed']+svd_test_df['ReversedInPart']+svd_test_df['Vacated']+svd_test_df['VacatedInPart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#keep only row with res ==1\n",
    "svd_train_df=svd_train_df[svd_train_df['res']==1]\n",
    "svd_test_df=svd_test_df[svd_test_df['res']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def combine_reverse(row):\n",
    "    if row['Reversed']==1:\n",
    "        return 2\n",
    "    \n",
    "    elif row['Vacated']==1:\n",
    "        return 2\n",
    "    \n",
    "    elif row['Affirmed']==1:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df['Res_binary'] = svd_train_df.apply(combine_reverse, axis=1)\n",
    "svd_test_df['Res_binary'] = svd_test_df.apply(combine_reverse, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df=svd_train_df[(svd_train_df['Res_binary']==1) | (svd_train_df['Res_binary']==2)]\n",
    "svd_test_df=svd_test_df[(svd_test_df['Res_binary']==1) | (svd_test_df['Res_binary']==2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_df= svd_train_df.drop(['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','res'],axis=1)\n",
    "svd_test_df= svd_test_df.drop(['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','res'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_nn_df = svd_train_df.iloc[:,:]\n",
    "svd_test_nn_df = svd_test_df.iloc[:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_nn_df['y'] = svd_train_df['length_3m_dif']\n",
    "svd_test_nn_df['y'] = svd_test_df['length_3m_dif']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "svd_train_nn_df.drop(['length_3m_dif','index'],axis=1, inplace=True)\n",
    "svd_test_nn_df.drop(['length_3m_dif','index'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>Res_binary</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.100533</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>-0.019895</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.014900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>-0.016751</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>-0.004419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>0.157620</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>-0.043520</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>-0.039799</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>-0.008680</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>-0.027626</td>\n",
       "      <td>-0.004125</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.141970</td>\n",
       "      <td>-0.026448</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-0.054681</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>-0.037343</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>-0.010692</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>-0.019105</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.109131</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.150298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>0.190061</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>0.053561</td>\n",
       "      <td>0.034232</td>\n",
       "      <td>0.072467</td>\n",
       "      <td>0.050863</td>\n",
       "      <td>-0.012928</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030851</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>0.009954</td>\n",
       "      <td>-0.044379</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.195514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>0.228796</td>\n",
       "      <td>-0.007014</td>\n",
       "      <td>0.070923</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.030451</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.019736</td>\n",
       "      <td>-0.031986</td>\n",
       "      <td>-0.012740</td>\n",
       "      <td>0.044786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>-0.010504</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>-0.016495</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.082692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "2072  0.100533  0.021049  0.007305  0.001948  0.001179 -0.019895 -0.008439   \n",
       "7052  0.157620 -0.026298 -0.043520 -0.057402 -0.039799  0.026700  0.008013   \n",
       "1582  0.141970 -0.026448 -0.080584 -0.054681 -0.016293  0.081749 -0.037343   \n",
       "2054  0.190061  0.104727  0.008652  0.053561  0.034232  0.072467  0.050863   \n",
       "6602  0.228796 -0.007014  0.070923 -0.003444 -0.030451 -0.000764  0.019736   \n",
       "\n",
       "             7         8         9    ...           72        73        74  \\\n",
       "2072  0.018857 -0.024885 -0.014900    ...     0.007469 -0.002709  0.018256   \n",
       "7052 -0.012378 -0.011192 -0.021375    ...     0.003264  0.016863 -0.008680   \n",
       "1582  0.017813 -0.010692 -0.000912    ...     0.018319  0.009916  0.033197   \n",
       "2054 -0.012928 -0.008381  0.014233    ...     0.030851 -0.009701  0.009954   \n",
       "6602 -0.031986 -0.012740  0.044786    ...     0.029470 -0.000433  0.028144   \n",
       "\n",
       "            75        76        77        78        79  Res_binary         y  \n",
       "2072  0.000921 -0.016751 -0.000313  0.003150 -0.004419         1.0  0.608711  \n",
       "7052  0.017289 -0.012610 -0.027626 -0.004125  0.002743         1.0  0.228592  \n",
       "1582 -0.019105  0.019378  0.109131  0.060335 -0.017806         1.0 -3.150298  \n",
       "2054 -0.044379 -0.006101 -0.011122 -0.019106 -0.000104         1.0 -1.195514  \n",
       "6602 -0.002859  0.019277 -0.010504  0.002817 -0.016495         2.0 -0.082692  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_train_nn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#save the data (after dimension reduction)\n",
    "svd_train_nn_df.to_csv(\"train_nn.csv\", sep=',')\n",
    "svd_test_nn_df.to_csv(\"test_nn.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#svd_train_nn_df = pd.read_csv(\"nn_prepared_svd_train.csv\", sep=',')\n",
    "#svd_test_nn_df = pd.read_csv(\"nn_prepared_svd_test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#basic nn architecture:\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#used for batch training:\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "xy_train = np.loadtxt(\"train_nn.csv\", delimiter = ',' ,skiprows=1, dtype= np.float64 )\n",
    "xy_test = np.loadtxt(\"test_nn.csv\", delimiter = ',', skiprows =1 ,dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5533, 83)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "#########################\n",
    "#prepare data for direct training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(xy_test[:,1:-1])\n",
    "y_test = torch.from_numpy(xy_test[:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_test = Variable(x_test.float())\n",
    "y_test = Variable(y_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_validation = torch.from_numpy(xy_train[:1265,1:-1])\n",
    "y_validation = torch.from_numpy(xy_train[:1265,[-1]])\n",
    "x_train = torch.from_numpy(xy_train[1265:,1:-1])\n",
    "y_train = torch.from_numpy(xy_train[1265:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x_validation = Variable(x_validation.float())\n",
    "y_validation = Variable(y_validation.float())\n",
    "x_train = Variable(x_train.float())\n",
    "y_train = Variable(y_train.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#define our own nn:\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden=torch.nn.Linear(n_features,n_hidden) \n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.predict=torch.nn.Linear(n_hidden,1) \n",
    "    #forward:\n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.hidden(x)) \n",
    "        x=self.hidden2(x)\n",
    "        x=self.predict(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 1.2059e-01  7.8051e-02 -4.7009e-02  ...  -5.0610e-04 -2.6548e-02  1.0000e+00\n",
       " 1.3144e-01 -5.8033e-03  5.1376e-02  ...  -6.2423e-03 -1.4356e-02  2.0000e+00\n",
       " 9.8614e-02 -6.2820e-02 -6.5716e-02  ...   1.0115e-02 -1.3742e-02  1.0000e+00\n",
       "                ...                   â‹±                   ...                \n",
       " 2.0341e-01 -4.0337e-02 -1.0333e-01  ...  -4.9230e-02  2.6367e-02  1.0000e+00\n",
       " 1.9376e-01  1.0681e-01  1.4803e-02  ...  -6.4434e-03 -4.5955e-02  1.0000e+00\n",
       " 8.8230e-02 -1.2538e-02  7.0084e-02  ...   1.1423e-03 -9.6509e-03  1.0000e+00\n",
       "[torch.FloatTensor of size 4268x81]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#initialize the net:\n",
    "n_features = 81\n",
    "net = Net(81, 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (hidden): Linear (81 -> 40)\n",
      "  (hidden2): Linear (40 -> 40)\n",
      "  (predict): Linear (40 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.03)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "loss_func_MAe=torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#batch training:\n",
    "Train_BATCH_SIZE = 200\n",
    "Validate_BATCH_SIZE = 126\n",
    "\n",
    "x_validation = torch.from_numpy(xy_train[:1260,1:-1])\n",
    "y_validation = torch.from_numpy(xy_train[:1260,[-1]])\n",
    "x_train = torch.from_numpy(xy_train[1260:,1:-1])\n",
    "y_train = torch.from_numpy(xy_train[1260:,[-1]])\n",
    "\n",
    "torch_dataset_train = Data.TensorDataset(data_tensor = x_train, target_tensor = y_train)\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset_train,\n",
    "    batch_size = Train_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,)\n",
    "\n",
    "torch_dataset_validate = Data.TensorDataset(data_tensor = x_validation, target_tensor = y_validation)\n",
    "validate_loader = Data.DataLoader(\n",
    "    dataset = torch_dataset_validate,\n",
    "    batch_size = Validate_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step:  0\n",
      "training loss is:  3.069209337234497\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  1\n",
      "training loss is:  3.3174920082092285\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  2\n",
      "training loss is:  3.025050163269043\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  3\n",
      "training loss is:  3.0972373485565186\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  4\n",
      "training loss is:  2.6433212757110596\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  5\n",
      "training loss is:  2.3488526344299316\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  6\n",
      "training loss is:  2.921114444732666\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  7\n",
      "training loss is:  4.0220441818237305\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  8\n",
      "training loss is:  4.0584516525268555\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  9\n",
      "training loss is:  3.4626169204711914\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  10\n",
      "training loss is:  3.464083194732666\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  11\n",
      "training loss is:  2.6654350757598877\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  12\n",
      "training loss is:  3.618847608566284\n",
      "\n",
      "\n",
      "validation loss is 3.0065675735473634\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  13\n",
      "training loss is:  2.9463608264923096\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  14\n",
      "training loss is:  5.31777286529541\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  15\n",
      "training loss is:  2.992253065109253\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  16\n",
      "training loss is:  3.089477777481079\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  17\n",
      "training loss is:  2.9976770877838135\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  18\n",
      "training loss is:  2.966973304748535\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  19\n",
      "training loss is:  2.7250778675079346\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  20\n",
      "training loss is:  2.7345633506774902\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  21\n",
      "training loss is:  2.565333843231201\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  0\n",
      "training loss is:  3.364041805267334\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  1\n",
      "training loss is:  3.0833497047424316\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  2\n",
      "training loss is:  3.3204047679901123\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  3\n",
      "training loss is:  2.7558679580688477\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  4\n",
      "training loss is:  2.883113384246826\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  5\n",
      "training loss is:  3.999866008758545\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  6\n",
      "training loss is:  3.2706351280212402\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  7\n",
      "training loss is:  2.8932013511657715\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  8\n",
      "training loss is:  3.2740066051483154\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  9\n",
      "training loss is:  2.7542214393615723\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  10\n",
      "training loss is:  3.5643017292022705\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  11\n",
      "training loss is:  2.8108270168304443\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  12\n",
      "training loss is:  2.848984956741333\n",
      "\n",
      "\n",
      "validation loss is 2.997226428985596\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  13\n",
      "training loss is:  3.0995073318481445\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  14\n",
      "training loss is:  2.5102689266204834\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  15\n",
      "training loss is:  2.6440603733062744\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  16\n",
      "training loss is:  2.961669683456421\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  17\n",
      "training loss is:  3.6027541160583496\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  18\n",
      "training loss is:  5.268794536590576\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  19\n",
      "training loss is:  3.305596113204956\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  20\n",
      "training loss is:  3.0492889881134033\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  21\n",
      "training loss is:  2.453369617462158\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  0\n",
      "training loss is:  3.140902519226074\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  1\n",
      "training loss is:  3.100977897644043\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  2\n",
      "training loss is:  2.967583656311035\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  3\n",
      "training loss is:  3.203533411026001\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  4\n",
      "training loss is:  2.859743356704712\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  5\n",
      "training loss is:  2.802370309829712\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  6\n",
      "training loss is:  3.163376569747925\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  7\n",
      "training loss is:  2.758725166320801\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  8\n",
      "training loss is:  3.270810604095459\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  9\n",
      "training loss is:  2.8107879161834717\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  10\n",
      "training loss is:  3.3816144466400146\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  11\n",
      "training loss is:  4.039182662963867\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  12\n",
      "training loss is:  5.121941089630127\n",
      "\n",
      "\n",
      "validation loss is 2.997050714492798\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  13\n",
      "training loss is:  2.7688474655151367\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  14\n",
      "training loss is:  2.533504009246826\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  15\n",
      "training loss is:  3.237802743911743\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  16\n",
      "training loss is:  4.287548065185547\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  17\n",
      "training loss is:  2.5412843227386475\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  18\n",
      "training loss is:  3.1893837451934814\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  19\n",
      "training loss is:  3.23382568359375\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  20\n",
      "training loss is:  2.889702796936035\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  21\n",
      "training loss is:  2.3125669956207275\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  0\n",
      "training loss is:  4.183619976043701\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  1\n",
      "training loss is:  3.7359366416931152\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  2\n",
      "training loss is:  2.673457622528076\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  3\n",
      "training loss is:  3.577263593673706\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  4\n",
      "training loss is:  2.572227716445923\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  5\n",
      "training loss is:  2.9149880409240723\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  6\n",
      "training loss is:  2.332399606704712\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  7\n",
      "training loss is:  3.1298935413360596\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  8\n",
      "training loss is:  2.8498735427856445\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  9\n",
      "training loss is:  3.114854335784912\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  10\n",
      "training loss is:  2.980694532394409\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  11\n",
      "training loss is:  2.973952293395996\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  12\n",
      "training loss is:  2.7239022254943848\n",
      "\n",
      "\n",
      "validation loss is 2.996923899650574\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  13\n",
      "training loss is:  2.459491014480591\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  14\n",
      "training loss is:  3.1247894763946533\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  15\n",
      "training loss is:  2.8198015689849854\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  16\n",
      "training loss is:  5.5021257400512695\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  17\n",
      "training loss is:  3.5360770225524902\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  18\n",
      "training loss is:  2.5644421577453613\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  19\n",
      "training loss is:  3.7728965282440186\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  20\n",
      "training loss is:  3.541961669921875\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  21\n",
      "training loss is:  3.0174970626831055\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  0\n",
      "training loss is:  3.1383216381073\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  1\n",
      "training loss is:  3.256406545639038\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  2\n",
      "training loss is:  2.523327112197876\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  3\n",
      "training loss is:  2.9636080265045166\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  4\n",
      "training loss is:  4.060062408447266\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  5\n",
      "training loss is:  2.7322471141815186\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  6\n",
      "training loss is:  3.287205219268799\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  7\n",
      "training loss is:  3.536221504211426\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  8\n",
      "training loss is:  2.881274461746216\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  9\n",
      "training loss is:  2.5313422679901123\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  10\n",
      "training loss is:  2.7939743995666504\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  11\n",
      "training loss is:  5.430534839630127\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  12\n",
      "training loss is:  3.6861155033111572\n",
      "\n",
      "\n",
      "validation loss is 2.9987701892852785\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  13\n",
      "training loss is:  2.7410614490509033\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  14\n",
      "training loss is:  2.7162158489227295\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  15\n",
      "training loss is:  3.5579144954681396\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  16\n",
      "training loss is:  3.073167324066162\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  17\n",
      "training loss is:  3.267073154449463\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  18\n",
      "training loss is:  2.621133327484131\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  19\n",
      "training loss is:  2.193014621734619\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  20\n",
      "training loss is:  3.6149110794067383\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  21\n",
      "training loss is:  4.222121238708496\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  0\n",
      "training loss is:  2.7786343097686768\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  1\n",
      "training loss is:  2.260284662246704\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  2\n",
      "training loss is:  2.9623074531555176\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  3\n",
      "training loss is:  2.7778735160827637\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  4\n",
      "training loss is:  2.9307701587677\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  5\n",
      "training loss is:  2.921966791152954\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  6\n",
      "training loss is:  3.272477388381958\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  7\n",
      "training loss is:  3.09885311126709\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  8\n",
      "training loss is:  3.5795934200286865\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  9\n",
      "training loss is:  3.0393738746643066\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  10\n",
      "training loss is:  2.601252555847168\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  11\n",
      "training loss is:  3.0008349418640137\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  12\n",
      "training loss is:  2.9117612838745117\n",
      "\n",
      "\n",
      "validation loss is 2.998925805091858\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  13\n",
      "training loss is:  3.198145866394043\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  14\n",
      "training loss is:  2.6843178272247314\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  15\n",
      "training loss is:  4.381169319152832\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  16\n",
      "training loss is:  4.1526594161987305\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  17\n",
      "training loss is:  3.4127919673919678\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  18\n",
      "training loss is:  5.785537242889404\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  19\n",
      "training loss is:  3.0984039306640625\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  20\n",
      "training loss is:  2.2366995811462402\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  21\n",
      "training loss is:  2.919839382171631\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  0\n",
      "training loss is:  2.7129878997802734\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  1\n",
      "training loss is:  3.4896934032440186\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  2\n",
      "training loss is:  3.265348196029663\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  3\n",
      "training loss is:  2.9891462326049805\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  4\n",
      "training loss is:  3.169381618499756\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  5\n",
      "training loss is:  5.30795955657959\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  6\n",
      "training loss is:  2.8543307781219482\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  7\n",
      "training loss is:  2.76676082611084\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  8\n",
      "training loss is:  3.646812677383423\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  9\n",
      "training loss is:  3.3330609798431396\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  10\n",
      "training loss is:  3.4207828044891357\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  11\n",
      "training loss is:  3.100537061691284\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  12\n",
      "training loss is:  2.4606165885925293\n",
      "\n",
      "\n",
      "validation loss is 2.9965404748916624\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  13\n",
      "training loss is:  2.6822774410247803\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  14\n",
      "training loss is:  2.749723196029663\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  15\n",
      "training loss is:  3.038630485534668\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  16\n",
      "training loss is:  3.3103339672088623\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  17\n",
      "training loss is:  3.572235345840454\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  18\n",
      "training loss is:  2.5972483158111572\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  19\n",
      "training loss is:  2.4221127033233643\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  20\n",
      "training loss is:  4.104865550994873\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  21\n",
      "training loss is:  3.1576709747314453\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  0\n",
      "training loss is:  3.0232882499694824\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  1\n",
      "training loss is:  6.195652008056641\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  2\n",
      "training loss is:  3.1040945053100586\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  3\n",
      "training loss is:  2.5596184730529785\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  4\n",
      "training loss is:  2.928269386291504\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  5\n",
      "training loss is:  2.3267791271209717\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  6\n",
      "training loss is:  3.7392148971557617\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  7\n",
      "training loss is:  2.8908982276916504\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  8\n",
      "training loss is:  2.922658920288086\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  9\n",
      "training loss is:  3.212740182876587\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  10\n",
      "training loss is:  2.667184352874756\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  11\n",
      "training loss is:  3.030642509460449\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  12\n",
      "training loss is:  3.051783561706543\n",
      "\n",
      "\n",
      "validation loss is 2.996732568740845\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  13\n",
      "training loss is:  2.5868473052978516\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  14\n",
      "training loss is:  2.8272690773010254\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  15\n",
      "training loss is:  3.3746869564056396\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  16\n",
      "training loss is:  3.3575448989868164\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  17\n",
      "training loss is:  3.0912816524505615\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  18\n",
      "training loss is:  3.264077663421631\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  19\n",
      "training loss is:  2.6520516872406006\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  20\n",
      "training loss is:  3.7255568504333496\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  21\n",
      "training loss is:  4.385158538818359\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  0\n",
      "training loss is:  2.4467737674713135\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  1\n",
      "training loss is:  2.926297664642334\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  2\n",
      "training loss is:  2.7633602619171143\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  3\n",
      "training loss is:  3.445664167404175\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  4\n",
      "training loss is:  3.7310662269592285\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  5\n",
      "training loss is:  2.893176317214966\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  6\n",
      "training loss is:  2.811917781829834\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  7\n",
      "training loss is:  2.9541938304901123\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  8\n",
      "training loss is:  4.602868556976318\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  9\n",
      "training loss is:  3.2038705348968506\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  10\n",
      "training loss is:  4.645529747009277\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  11\n",
      "training loss is:  2.624032497406006\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  12\n",
      "training loss is:  2.7340080738067627\n",
      "\n",
      "\n",
      "validation loss is 2.997018909454346\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  13\n",
      "training loss is:  3.033869743347168\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  14\n",
      "training loss is:  2.8163347244262695\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  15\n",
      "training loss is:  2.9785752296447754\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  16\n",
      "training loss is:  3.1249730587005615\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  17\n",
      "training loss is:  2.9890575408935547\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  18\n",
      "training loss is:  3.278827428817749\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  19\n",
      "training loss is:  3.2588372230529785\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  20\n",
      "training loss is:  4.056253433227539\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  21\n",
      "training loss is:  2.2437381744384766\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  0\n",
      "training loss is:  2.7526352405548096\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  1\n",
      "training loss is:  2.7679684162139893\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  2\n",
      "training loss is:  2.5665981769561768\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  3\n",
      "training loss is:  5.164578437805176\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  4\n",
      "training loss is:  4.056959629058838\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  5\n",
      "training loss is:  3.700122356414795\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  6\n",
      "training loss is:  2.511939525604248\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  7\n",
      "training loss is:  3.394516944885254\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  8\n",
      "training loss is:  2.622012138366699\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  9\n",
      "training loss is:  2.439119815826416\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  10\n",
      "training loss is:  2.8270304203033447\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  11\n",
      "training loss is:  2.736604690551758\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  12\n",
      "training loss is:  3.411628484725952\n",
      "\n",
      "\n",
      "validation loss is 2.9965610265731812\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  13\n",
      "training loss is:  3.260481834411621\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  14\n",
      "training loss is:  3.3088018894195557\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  15\n",
      "training loss is:  2.9362857341766357\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  16\n",
      "training loss is:  2.5322742462158203\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  17\n",
      "training loss is:  2.8059754371643066\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  18\n",
      "training loss is:  3.813194513320923\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  19\n",
      "training loss is:  2.758687734603882\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  20\n",
      "training loss is:  4.225122928619385\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  21\n",
      "training loss is:  4.27924919128418\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  0\n",
      "training loss is:  2.8984529972076416\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  1\n",
      "training loss is:  3.3638830184936523\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  2\n",
      "training loss is:  3.225632667541504\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  3\n",
      "training loss is:  2.7962894439697266\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  4\n",
      "training loss is:  3.0981171131134033\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  5\n",
      "training loss is:  3.089906692504883\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  6\n",
      "training loss is:  2.7637805938720703\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  7\n",
      "training loss is:  2.7571685314178467\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  8\n",
      "training loss is:  2.889171838760376\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  9\n",
      "training loss is:  2.6262331008911133\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  10\n",
      "training loss is:  3.4518070220947266\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  11\n",
      "training loss is:  5.602900505065918\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  12\n",
      "training loss is:  4.859321594238281\n",
      "\n",
      "\n",
      "validation loss is 2.998999333381653\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  13\n",
      "training loss is:  2.964829921722412\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  14\n",
      "training loss is:  3.2244513034820557\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  15\n",
      "training loss is:  2.8441662788391113\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  16\n",
      "training loss is:  2.6082687377929688\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  17\n",
      "training loss is:  2.922456979751587\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  18\n",
      "training loss is:  2.8623669147491455\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  19\n",
      "training loss is:  3.1384665966033936\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  20\n",
      "training loss is:  2.4515798091888428\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  21\n",
      "training loss is:  4.686627388000488\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  0\n",
      "training loss is:  5.511045932769775\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  1\n",
      "training loss is:  2.5617175102233887\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  2\n",
      "training loss is:  2.5235536098480225\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  3\n",
      "training loss is:  3.107192277908325\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  4\n",
      "training loss is:  3.7220189571380615\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  5\n",
      "training loss is:  3.1926887035369873\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  6\n",
      "training loss is:  3.5972070693969727\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  7\n",
      "training loss is:  2.900423526763916\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  8\n",
      "training loss is:  3.620030403137207\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  9\n",
      "training loss is:  3.170867681503296\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  10\n",
      "training loss is:  3.4788668155670166\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  11\n",
      "training loss is:  2.9785947799682617\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  12\n",
      "training loss is:  2.7740087509155273\n",
      "\n",
      "\n",
      "validation loss is 2.997383189201355\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  13\n",
      "training loss is:  3.0077266693115234\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  14\n",
      "training loss is:  3.080209970474243\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  15\n",
      "training loss is:  2.43701171875\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  16\n",
      "training loss is:  2.785766363143921\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  17\n",
      "training loss is:  2.889324426651001\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  18\n",
      "training loss is:  2.9172089099884033\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  19\n",
      "training loss is:  3.1450138092041016\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  20\n",
      "training loss is:  3.3985660076141357\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  21\n",
      "training loss is:  3.4818778038024902\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  0\n",
      "training loss is:  2.9772613048553467\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  1\n",
      "training loss is:  2.90372371673584\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  2\n",
      "training loss is:  3.20772647857666\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  3\n",
      "training loss is:  3.157353401184082\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  4\n",
      "training loss is:  2.776787519454956\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  5\n",
      "training loss is:  3.8332014083862305\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  6\n",
      "training loss is:  3.1564176082611084\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  7\n",
      "training loss is:  3.282799005508423\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  8\n",
      "training loss is:  3.318018913269043\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  9\n",
      "training loss is:  2.6995251178741455\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  10\n",
      "training loss is:  2.417543888092041\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  11\n",
      "training loss is:  2.5819895267486572\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  12\n",
      "training loss is:  5.493279933929443\n",
      "\n",
      "\n",
      "validation loss is 2.995875024795532\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  13\n",
      "training loss is:  2.8364923000335693\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  14\n",
      "training loss is:  3.947908639907837\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  15\n",
      "training loss is:  3.051778793334961\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  16\n",
      "training loss is:  3.4818646907806396\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  17\n",
      "training loss is:  2.341909408569336\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  18\n",
      "training loss is:  3.3951416015625\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  19\n",
      "training loss is:  3.293766975402832\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  20\n",
      "training loss is:  2.7094202041625977\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  21\n",
      "training loss is:  3.581850290298462\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  0\n",
      "training loss is:  3.0448315143585205\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  1\n",
      "training loss is:  2.7949936389923096\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  2\n",
      "training loss is:  6.583680629730225\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  3\n",
      "training loss is:  3.221379280090332\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  4\n",
      "training loss is:  2.7465410232543945\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  5\n",
      "training loss is:  3.560002088546753\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  6\n",
      "training loss is:  2.5710608959198\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  7\n",
      "training loss is:  3.272876024246216\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  8\n",
      "training loss is:  3.3201186656951904\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  9\n",
      "training loss is:  2.9078118801116943\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  10\n",
      "training loss is:  2.892537832260132\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  11\n",
      "training loss is:  2.9077210426330566\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  12\n",
      "training loss is:  3.1785287857055664\n",
      "\n",
      "\n",
      "validation loss is 2.9981078147888183\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  13\n",
      "training loss is:  2.9198763370513916\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  14\n",
      "training loss is:  2.8102283477783203\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  15\n",
      "training loss is:  2.874342441558838\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  16\n",
      "training loss is:  2.7213635444641113\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  17\n",
      "training loss is:  3.147200584411621\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  18\n",
      "training loss is:  2.4678351879119873\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  19\n",
      "training loss is:  3.647383213043213\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  20\n",
      "training loss is:  3.551481246948242\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  21\n",
      "training loss is:  2.7620632648468018\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  0\n",
      "training loss is:  2.769787073135376\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  1\n",
      "training loss is:  2.8483943939208984\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  2\n",
      "training loss is:  2.7070462703704834\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  3\n",
      "training loss is:  3.465937614440918\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  4\n",
      "training loss is:  5.276829242706299\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  5\n",
      "training loss is:  2.6895036697387695\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  6\n",
      "training loss is:  3.0834226608276367\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  7\n",
      "training loss is:  2.8480496406555176\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  8\n",
      "training loss is:  3.292433977127075\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  9\n",
      "training loss is:  3.423569917678833\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  10\n",
      "training loss is:  2.3787052631378174\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  11\n",
      "training loss is:  3.534498691558838\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  12\n",
      "training loss is:  2.9664664268493652\n",
      "\n",
      "\n",
      "validation loss is 2.9962998151779177\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  13\n",
      "training loss is:  3.428102731704712\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  14\n",
      "training loss is:  2.913816452026367\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  15\n",
      "training loss is:  2.9755446910858154\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  16\n",
      "training loss is:  3.6403939723968506\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  17\n",
      "training loss is:  3.2818403244018555\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  18\n",
      "training loss is:  3.039733648300171\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  19\n",
      "training loss is:  2.871840715408325\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  20\n",
      "training loss is:  3.3212738037109375\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  21\n",
      "training loss is:  3.7475645542144775\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  0\n",
      "training loss is:  3.128941535949707\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  1\n",
      "training loss is:  2.811279535293579\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  2\n",
      "training loss is:  3.5254459381103516\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  3\n",
      "training loss is:  2.684776544570923\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  4\n",
      "training loss is:  3.5289535522460938\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  5\n",
      "training loss is:  2.6894025802612305\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  6\n",
      "training loss is:  3.0588531494140625\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  7\n",
      "training loss is:  2.8646230697631836\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  8\n",
      "training loss is:  3.0418930053710938\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  9\n",
      "training loss is:  2.9499781131744385\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  10\n",
      "training loss is:  3.0328452587127686\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  11\n",
      "training loss is:  5.150867938995361\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  12\n",
      "training loss is:  3.1192445755004883\n",
      "\n",
      "\n",
      "validation loss is 2.995670294761658\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  13\n",
      "training loss is:  3.7199130058288574\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  14\n",
      "training loss is:  2.8526716232299805\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  15\n",
      "training loss is:  2.9513344764709473\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  16\n",
      "training loss is:  2.6542751789093018\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  17\n",
      "training loss is:  3.710951328277588\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  18\n",
      "training loss is:  3.623767614364624\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  19\n",
      "training loss is:  3.0461392402648926\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  20\n",
      "training loss is:  3.2137813568115234\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  21\n",
      "training loss is:  2.1288487911224365\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  0\n",
      "training loss is:  2.257636308670044\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  1\n",
      "training loss is:  3.7431693077087402\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  2\n",
      "training loss is:  3.0516488552093506\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  3\n",
      "training loss is:  3.884345054626465\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  4\n",
      "training loss is:  5.819935321807861\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  5\n",
      "training loss is:  2.8589513301849365\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  6\n",
      "training loss is:  2.8128914833068848\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  7\n",
      "training loss is:  3.547214984893799\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  8\n",
      "training loss is:  3.689789056777954\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  9\n",
      "training loss is:  2.6898107528686523\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  10\n",
      "training loss is:  2.563382148742676\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  11\n",
      "training loss is:  3.4283535480499268\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  12\n",
      "training loss is:  2.3642547130584717\n",
      "\n",
      "\n",
      "validation loss is 2.9956655502319336\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  13\n",
      "training loss is:  2.970736026763916\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  14\n",
      "training loss is:  3.0863866806030273\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  15\n",
      "training loss is:  2.851726770401001\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  16\n",
      "training loss is:  2.5559308528900146\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  17\n",
      "training loss is:  2.7761168479919434\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  18\n",
      "training loss is:  3.2310471534729004\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  19\n",
      "training loss is:  3.556363821029663\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  20\n",
      "training loss is:  3.1624274253845215\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  21\n",
      "training loss is:  3.3588078022003174\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  0\n",
      "training loss is:  3.0857620239257812\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  1\n",
      "training loss is:  3.0563454627990723\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  2\n",
      "training loss is:  2.2537946701049805\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  3\n",
      "training loss is:  3.281761407852173\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  4\n",
      "training loss is:  3.026531934738159\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  5\n",
      "training loss is:  3.4021637439727783\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  6\n",
      "training loss is:  3.4284255504608154\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  7\n",
      "training loss is:  3.7191877365112305\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  8\n",
      "training loss is:  2.492265462875366\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  9\n",
      "training loss is:  2.9577438831329346\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  10\n",
      "training loss is:  3.1232528686523438\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  11\n",
      "training loss is:  5.364256381988525\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  12\n",
      "training loss is:  2.9134092330932617\n",
      "\n",
      "\n",
      "validation loss is 2.9955846309661864\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  13\n",
      "training loss is:  3.308539390563965\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  14\n",
      "training loss is:  2.800480604171753\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  15\n",
      "training loss is:  3.078733444213867\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  16\n",
      "training loss is:  3.2723805904388428\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  17\n",
      "training loss is:  4.048213958740234\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  18\n",
      "training loss is:  2.9805779457092285\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  19\n",
      "training loss is:  2.382627010345459\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  20\n",
      "training loss is:  3.3519577980041504\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  21\n",
      "training loss is:  2.1746814250946045\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  0\n",
      "training loss is:  3.0302515029907227\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  1\n",
      "training loss is:  3.5221776962280273\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  2\n",
      "training loss is:  2.8733294010162354\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  3\n",
      "training loss is:  5.883975028991699\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  4\n",
      "training loss is:  2.944758892059326\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  5\n",
      "training loss is:  2.6621804237365723\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  6\n",
      "training loss is:  3.031916856765747\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  7\n",
      "training loss is:  4.236159801483154\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  8\n",
      "training loss is:  3.6354458332061768\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  9\n",
      "training loss is:  2.8064568042755127\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  10\n",
      "training loss is:  3.3667004108428955\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  11\n",
      "training loss is:  3.0807855129241943\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  12\n",
      "training loss is:  2.886495590209961\n",
      "\n",
      "\n",
      "validation loss is 2.996066856384277\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  13\n",
      "training loss is:  3.294368267059326\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  14\n",
      "training loss is:  2.546220064163208\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  15\n",
      "training loss is:  2.4756722450256348\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  16\n",
      "training loss is:  3.815704584121704\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  17\n",
      "training loss is:  2.7359418869018555\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  18\n",
      "training loss is:  3.0019147396087646\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  19\n",
      "training loss is:  2.8005928993225098\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  20\n",
      "training loss is:  2.513286828994751\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  21\n",
      "training loss is:  2.689424991607666\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  0\n",
      "training loss is:  3.1030590534210205\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  1\n",
      "training loss is:  3.1362030506134033\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  2\n",
      "training loss is:  3.9175713062286377\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  3\n",
      "training loss is:  2.951397657394409\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  4\n",
      "training loss is:  2.9262990951538086\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  5\n",
      "training loss is:  3.1143507957458496\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  6\n",
      "training loss is:  2.6169159412384033\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  7\n",
      "training loss is:  2.581076145172119\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  8\n",
      "training loss is:  3.1958866119384766\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  9\n",
      "training loss is:  3.2332241535186768\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  10\n",
      "training loss is:  3.4552996158599854\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  11\n",
      "training loss is:  3.590097427368164\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  12\n",
      "training loss is:  3.0129668712615967\n",
      "\n",
      "\n",
      "validation loss is 2.995478868484497\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  13\n",
      "training loss is:  2.7196128368377686\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  14\n",
      "training loss is:  2.9079699516296387\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  15\n",
      "training loss is:  5.083980083465576\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  16\n",
      "training loss is:  3.4695606231689453\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  17\n",
      "training loss is:  3.4596433639526367\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  18\n",
      "training loss is:  2.245511531829834\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  19\n",
      "training loss is:  2.256293535232544\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  20\n",
      "training loss is:  4.151159763336182\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  21\n",
      "training loss is:  2.6240508556365967\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  0\n",
      "training loss is:  2.1862306594848633\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  1\n",
      "training loss is:  3.0022687911987305\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  2\n",
      "training loss is:  2.8324267864227295\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  3\n",
      "training loss is:  3.688676118850708\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  4\n",
      "training loss is:  3.1476807594299316\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  5\n",
      "training loss is:  2.9397242069244385\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  6\n",
      "training loss is:  3.0208852291107178\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  7\n",
      "training loss is:  3.3474886417388916\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  8\n",
      "training loss is:  2.99467134475708\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  9\n",
      "training loss is:  3.2710230350494385\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  10\n",
      "training loss is:  5.5569586753845215\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  11\n",
      "training loss is:  3.0574417114257812\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  12\n",
      "training loss is:  3.0428173542022705\n",
      "\n",
      "\n",
      "validation loss is 2.995937371253967\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  13\n",
      "training loss is:  3.4543638229370117\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  14\n",
      "training loss is:  3.4123525619506836\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  15\n",
      "training loss is:  2.7187106609344482\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  16\n",
      "training loss is:  2.5565459728240967\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  17\n",
      "training loss is:  3.368351936340332\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  18\n",
      "training loss is:  2.7073895931243896\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  19\n",
      "training loss is:  2.5855960845947266\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  20\n",
      "training loss is:  3.706282377243042\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  21\n",
      "training loss is:  4.166022777557373\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  0\n",
      "training loss is:  5.529189586639404\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  1\n",
      "training loss is:  2.9955878257751465\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  2\n",
      "training loss is:  3.32047700881958\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  3\n",
      "training loss is:  2.49900484085083\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  4\n",
      "training loss is:  3.4939095973968506\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  5\n",
      "training loss is:  3.7659127712249756\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  6\n",
      "training loss is:  2.460559129714966\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  7\n",
      "training loss is:  2.6719841957092285\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  8\n",
      "training loss is:  3.2876477241516113\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  9\n",
      "training loss is:  3.1310667991638184\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  10\n",
      "training loss is:  3.232635259628296\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  11\n",
      "training loss is:  2.952939510345459\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  12\n",
      "training loss is:  2.7275099754333496\n",
      "\n",
      "\n",
      "validation loss is 2.9983670234680178\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  13\n",
      "training loss is:  3.027608633041382\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  14\n",
      "training loss is:  3.2149124145507812\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  15\n",
      "training loss is:  3.0209238529205322\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  16\n",
      "training loss is:  3.5649707317352295\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  17\n",
      "training loss is:  2.9006035327911377\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  18\n",
      "training loss is:  3.548853874206543\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  19\n",
      "training loss is:  2.314225196838379\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  20\n",
      "training loss is:  3.459656000137329\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  21\n",
      "training loss is:  2.7468695640563965\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  0\n",
      "training loss is:  3.1752331256866455\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  1\n",
      "training loss is:  3.4899070262908936\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  2\n",
      "training loss is:  4.90815544128418\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  3\n",
      "training loss is:  3.1535634994506836\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  4\n",
      "training loss is:  3.7448947429656982\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  5\n",
      "training loss is:  3.194291591644287\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  6\n",
      "training loss is:  3.4611968994140625\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  7\n",
      "training loss is:  2.5860683917999268\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  8\n",
      "training loss is:  3.645841360092163\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  9\n",
      "training loss is:  2.88742733001709\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  10\n",
      "training loss is:  2.639655351638794\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  11\n",
      "training loss is:  3.43986177444458\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  12\n",
      "training loss is:  2.514916181564331\n",
      "\n",
      "\n",
      "validation loss is 2.995412516593933\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  13\n",
      "training loss is:  3.1890530586242676\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  14\n",
      "training loss is:  2.639923334121704\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  15\n",
      "training loss is:  3.062466621398926\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  16\n",
      "training loss is:  2.8571856021881104\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  17\n",
      "training loss is:  2.994741201400757\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  18\n",
      "training loss is:  2.943916082382202\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  19\n",
      "training loss is:  3.674009323120117\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  20\n",
      "training loss is:  2.941450595855713\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  21\n",
      "training loss is:  2.690345048904419\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  0\n",
      "training loss is:  2.951664447784424\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  1\n",
      "training loss is:  2.760862350463867\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  2\n",
      "training loss is:  3.0558855533599854\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  3\n",
      "training loss is:  3.354762315750122\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  4\n",
      "training loss is:  2.898045539855957\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  5\n",
      "training loss is:  5.202066421508789\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  6\n",
      "training loss is:  3.140845537185669\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  7\n",
      "training loss is:  2.9658823013305664\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  8\n",
      "training loss is:  3.9606268405914307\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  9\n",
      "training loss is:  2.894031047821045\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  10\n",
      "training loss is:  2.141953706741333\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  11\n",
      "training loss is:  3.017007350921631\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  12\n",
      "training loss is:  3.6624443531036377\n",
      "\n",
      "\n",
      "validation loss is 2.9954002141952514\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  13\n",
      "training loss is:  3.612802743911743\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  14\n",
      "training loss is:  3.048433542251587\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  15\n",
      "training loss is:  2.625675916671753\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  16\n",
      "training loss is:  2.3786113262176514\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  17\n",
      "training loss is:  3.1171581745147705\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  18\n",
      "training loss is:  2.7771120071411133\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  19\n",
      "training loss is:  2.8457295894622803\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  20\n",
      "training loss is:  3.979862928390503\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  21\n",
      "training loss is:  4.730938911437988\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  0\n",
      "training loss is:  2.6894214153289795\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  1\n",
      "training loss is:  3.480062246322632\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  2\n",
      "training loss is:  3.1054775714874268\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  3\n",
      "training loss is:  2.837127923965454\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  4\n",
      "training loss is:  2.4070398807525635\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  5\n",
      "training loss is:  4.269369602203369\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  6\n",
      "training loss is:  2.8172857761383057\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  7\n",
      "training loss is:  3.5065059661865234\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  8\n",
      "training loss is:  4.842768669128418\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  9\n",
      "training loss is:  3.1255130767822266\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  10\n",
      "training loss is:  2.744797468185425\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  11\n",
      "training loss is:  3.1757194995880127\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  12\n",
      "training loss is:  2.9539685249328613\n",
      "\n",
      "\n",
      "validation loss is 2.9982319831848145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  13\n",
      "training loss is:  3.3063430786132812\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  14\n",
      "training loss is:  2.852928876876831\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  15\n",
      "training loss is:  2.6178040504455566\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  16\n",
      "training loss is:  3.144869327545166\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  17\n",
      "training loss is:  3.385416269302368\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  18\n",
      "training loss is:  3.3301455974578857\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  19\n",
      "training loss is:  2.9479260444641113\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  20\n",
      "training loss is:  3.1790857315063477\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  21\n",
      "training loss is:  3.762343645095825\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  0\n",
      "training loss is:  2.914137840270996\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  1\n",
      "training loss is:  2.7389612197875977\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  2\n",
      "training loss is:  3.0478293895721436\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  3\n",
      "training loss is:  3.0323050022125244\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  4\n",
      "training loss is:  2.823012351989746\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  5\n",
      "training loss is:  3.7805702686309814\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  6\n",
      "training loss is:  3.0683298110961914\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  7\n",
      "training loss is:  2.344836473464966\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  8\n",
      "training loss is:  6.3422627449035645\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  9\n",
      "training loss is:  3.190537214279175\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  10\n",
      "training loss is:  3.387376070022583\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  11\n",
      "training loss is:  2.8060574531555176\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  12\n",
      "training loss is:  3.1360201835632324\n",
      "\n",
      "\n",
      "validation loss is 2.9952141523361204\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  13\n",
      "training loss is:  3.2937378883361816\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  14\n",
      "training loss is:  2.4801459312438965\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  15\n",
      "training loss is:  2.554330587387085\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  16\n",
      "training loss is:  3.9652271270751953\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  17\n",
      "training loss is:  2.4584450721740723\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  18\n",
      "training loss is:  3.0015125274658203\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  19\n",
      "training loss is:  3.539740800857544\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  20\n",
      "training loss is:  2.9386422634124756\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  21\n",
      "training loss is:  3.3792736530303955\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  0\n",
      "training loss is:  2.7079977989196777\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  1\n",
      "training loss is:  3.031167984008789\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  2\n",
      "training loss is:  3.113406181335449\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  3\n",
      "training loss is:  3.9343206882476807\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  4\n",
      "training loss is:  3.527717351913452\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  5\n",
      "training loss is:  2.813570261001587\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  6\n",
      "training loss is:  3.0070688724517822\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  7\n",
      "training loss is:  3.0090339183807373\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  8\n",
      "training loss is:  2.659059524536133\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  9\n",
      "training loss is:  3.560426950454712\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  10\n",
      "training loss is:  3.8747763633728027\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  11\n",
      "training loss is:  3.0236732959747314\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  12\n",
      "training loss is:  2.5905518531799316\n",
      "\n",
      "\n",
      "validation loss is 2.9951726675033568\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  13\n",
      "training loss is:  2.7821309566497803\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  14\n",
      "training loss is:  5.9116291999816895\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  15\n",
      "training loss is:  3.2080512046813965\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  16\n",
      "training loss is:  2.8756091594696045\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  17\n",
      "training loss is:  2.295686721801758\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  18\n",
      "training loss is:  3.097719430923462\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  19\n",
      "training loss is:  3.026991367340088\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  20\n",
      "training loss is:  2.500444173812866\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  21\n",
      "training loss is:  4.180819511413574\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  0\n",
      "training loss is:  3.9302186965942383\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  1\n",
      "training loss is:  2.491304397583008\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  2\n",
      "training loss is:  2.3766586780548096\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  3\n",
      "training loss is:  2.7945809364318848\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  4\n",
      "training loss is:  2.359342336654663\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  5\n",
      "training loss is:  3.303623914718628\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  6\n",
      "training loss is:  3.0894837379455566\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  7\n",
      "training loss is:  2.4304680824279785\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  8\n",
      "training loss is:  3.1691370010375977\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  9\n",
      "training loss is:  3.3160738945007324\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  10\n",
      "training loss is:  2.714462995529175\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  11\n",
      "training loss is:  3.0863797664642334\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  12\n",
      "training loss is:  2.9821624755859375\n",
      "\n",
      "\n",
      "validation loss is 2.996881628036499\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  13\n",
      "training loss is:  2.46199893951416\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  14\n",
      "training loss is:  3.07973051071167\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  15\n",
      "training loss is:  3.456569194793701\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  16\n",
      "training loss is:  3.800990104675293\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  17\n",
      "training loss is:  3.5674002170562744\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  18\n",
      "training loss is:  6.4718756675720215\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  19\n",
      "training loss is:  3.034677028656006\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  20\n",
      "training loss is:  2.825254440307617\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  21\n",
      "training loss is:  3.7476985454559326\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  0\n",
      "training loss is:  3.4458236694335938\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  1\n",
      "training loss is:  3.804166316986084\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  2\n",
      "training loss is:  2.9831552505493164\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  3\n",
      "training loss is:  2.8091330528259277\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  4\n",
      "training loss is:  2.874464511871338\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  5\n",
      "training loss is:  2.875276803970337\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  6\n",
      "training loss is:  3.1964123249053955\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  7\n",
      "training loss is:  5.21688985824585\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  8\n",
      "training loss is:  2.4083149433135986\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  9\n",
      "training loss is:  2.6324198246002197\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  10\n",
      "training loss is:  2.801861524581909\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  11\n",
      "training loss is:  3.008880615234375\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  12\n",
      "training loss is:  2.992398738861084\n",
      "\n",
      "\n",
      "validation loss is 2.9954816579818724\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  13\n",
      "training loss is:  3.6163887977600098\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  14\n",
      "training loss is:  3.401278495788574\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  15\n",
      "training loss is:  2.7341625690460205\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  16\n",
      "training loss is:  2.6276466846466064\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  17\n",
      "training loss is:  3.186861515045166\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  18\n",
      "training loss is:  3.982741594314575\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  19\n",
      "training loss is:  3.379377841949463\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  20\n",
      "training loss is:  3.018521785736084\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  21\n",
      "training loss is:  2.9956932067871094\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  0\n",
      "training loss is:  2.636293888092041\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  1\n",
      "training loss is:  2.929112434387207\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  2\n",
      "training loss is:  2.943777561187744\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  3\n",
      "training loss is:  3.429354190826416\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  4\n",
      "training loss is:  3.441495418548584\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  5\n",
      "training loss is:  3.0968706607818604\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  6\n",
      "training loss is:  3.4692118167877197\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  7\n",
      "training loss is:  3.048257827758789\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  8\n",
      "training loss is:  5.05570650100708\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  9\n",
      "training loss is:  3.0263748168945312\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  10\n",
      "training loss is:  3.5049569606781006\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  11\n",
      "training loss is:  3.1147525310516357\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  12\n",
      "training loss is:  2.9138410091400146\n",
      "\n",
      "\n",
      "validation loss is 2.9963048696517944\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  13\n",
      "training loss is:  2.9246926307678223\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  14\n",
      "training loss is:  3.28178071975708\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  15\n",
      "training loss is:  3.9348061084747314\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  16\n",
      "training loss is:  2.3276076316833496\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  17\n",
      "training loss is:  2.70062255859375\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  18\n",
      "training loss is:  3.5619702339172363\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  19\n",
      "training loss is:  2.693535804748535\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  20\n",
      "training loss is:  3.0998709201812744\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  21\n",
      "training loss is:  2.5956146717071533\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  0\n",
      "training loss is:  2.512788772583008\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  1\n",
      "training loss is:  3.6528429985046387\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  2\n",
      "training loss is:  2.7658379077911377\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  3\n",
      "training loss is:  2.6010642051696777\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  4\n",
      "training loss is:  2.573549270629883\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  5\n",
      "training loss is:  3.134615182876587\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  6\n",
      "training loss is:  3.7851381301879883\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  7\n",
      "training loss is:  3.6708428859710693\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  8\n",
      "training loss is:  3.070155143737793\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  9\n",
      "training loss is:  2.5469675064086914\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  10\n",
      "training loss is:  3.313772678375244\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  11\n",
      "training loss is:  3.683161973953247\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  12\n",
      "training loss is:  3.1307718753814697\n",
      "\n",
      "\n",
      "validation loss is 2.995782494544983\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  13\n",
      "training loss is:  3.075883150100708\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  14\n",
      "training loss is:  2.536090135574341\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  15\n",
      "training loss is:  3.207876682281494\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  16\n",
      "training loss is:  2.9530420303344727\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  17\n",
      "training loss is:  3.332669734954834\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  18\n",
      "training loss is:  2.8686177730560303\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  19\n",
      "training loss is:  2.5381510257720947\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  20\n",
      "training loss is:  3.5387210845947266\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  21\n",
      "training loss is:  9.911684036254883\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  0\n",
      "training loss is:  3.984908103942871\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  1\n",
      "training loss is:  3.9153406620025635\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  2\n",
      "training loss is:  2.618940830230713\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  3\n",
      "training loss is:  3.468193292617798\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  4\n",
      "training loss is:  4.8795671463012695\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  5\n",
      "training loss is:  3.0390071868896484\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  6\n",
      "training loss is:  2.9288089275360107\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  7\n",
      "training loss is:  2.8543241024017334\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  8\n",
      "training loss is:  3.3635425567626953\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  9\n",
      "training loss is:  2.5637824535369873\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  10\n",
      "training loss is:  3.041729688644409\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  11\n",
      "training loss is:  2.829627752304077\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  12\n",
      "training loss is:  3.2464141845703125\n",
      "\n",
      "\n",
      "validation loss is 2.995186996459961\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  13\n",
      "training loss is:  2.9073896408081055\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  14\n",
      "training loss is:  3.050204277038574\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  15\n",
      "training loss is:  3.9768612384796143\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  16\n",
      "training loss is:  2.9478518962860107\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  17\n",
      "training loss is:  3.2435309886932373\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  18\n",
      "training loss is:  2.9910216331481934\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  19\n",
      "training loss is:  2.4795753955841064\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  20\n",
      "training loss is:  2.8075170516967773\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  21\n",
      "training loss is:  2.5829272270202637\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  0\n",
      "training loss is:  3.6118357181549072\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  1\n",
      "training loss is:  3.1488425731658936\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  2\n",
      "training loss is:  2.3362114429473877\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  3\n",
      "training loss is:  3.084531307220459\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  4\n",
      "training loss is:  3.2242279052734375\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  5\n",
      "training loss is:  4.268430709838867\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  6\n",
      "training loss is:  2.6268820762634277\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  7\n",
      "training loss is:  2.39886474609375\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  8\n",
      "training loss is:  2.7239296436309814\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  9\n",
      "training loss is:  2.7983667850494385\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  10\n",
      "training loss is:  2.508039712905884\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  11\n",
      "training loss is:  3.3172316551208496\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  12\n",
      "training loss is:  2.923217535018921\n",
      "\n",
      "\n",
      "validation loss is 2.994998002052307\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  13\n",
      "training loss is:  3.045186996459961\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  14\n",
      "training loss is:  3.616835117340088\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  15\n",
      "training loss is:  2.8006980419158936\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  16\n",
      "training loss is:  2.862589120864868\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  17\n",
      "training loss is:  3.797426462173462\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  18\n",
      "training loss is:  3.072274684906006\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  19\n",
      "training loss is:  4.686678886413574\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  20\n",
      "training loss is:  3.92634916305542\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  21\n",
      "training loss is:  3.517971992492676\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  0\n",
      "training loss is:  3.186328411102295\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  1\n",
      "training loss is:  2.9595186710357666\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  2\n",
      "training loss is:  2.9852254390716553\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  3\n",
      "training loss is:  3.65266752243042\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  4\n",
      "training loss is:  2.9666635990142822\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  5\n",
      "training loss is:  3.961820602416992\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  6\n",
      "training loss is:  3.194716691970825\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  7\n",
      "training loss is:  3.999399185180664\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  8\n",
      "training loss is:  2.6690878868103027\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  9\n",
      "training loss is:  3.233518123626709\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  10\n",
      "training loss is:  2.5407285690307617\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  11\n",
      "training loss is:  3.294658899307251\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  12\n",
      "training loss is:  3.332975387573242\n",
      "\n",
      "\n",
      "validation loss is 2.995277738571167\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  13\n",
      "training loss is:  3.318913459777832\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  14\n",
      "training loss is:  2.891867160797119\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  15\n",
      "training loss is:  3.4182333946228027\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  16\n",
      "training loss is:  2.3002305030822754\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  17\n",
      "training loss is:  2.579061985015869\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  18\n",
      "training loss is:  5.208986282348633\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  19\n",
      "training loss is:  2.459684133529663\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  20\n",
      "training loss is:  3.104576826095581\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  21\n",
      "training loss is:  2.2112996578216553\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  0\n",
      "training loss is:  3.7248473167419434\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  1\n",
      "training loss is:  3.1362414360046387\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  2\n",
      "training loss is:  3.334956169128418\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  3\n",
      "training loss is:  3.327197551727295\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  4\n",
      "training loss is:  3.404118537902832\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  5\n",
      "training loss is:  3.3445258140563965\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  6\n",
      "training loss is:  2.345503807067871\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  7\n",
      "training loss is:  2.6045982837677\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  8\n",
      "training loss is:  2.524860143661499\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  9\n",
      "training loss is:  2.539733648300171\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  10\n",
      "training loss is:  2.902261734008789\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  11\n",
      "training loss is:  2.864556074142456\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  12\n",
      "training loss is:  2.8745152950286865\n",
      "\n",
      "\n",
      "validation loss is 2.995510625839233\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  13\n",
      "training loss is:  2.949491500854492\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  14\n",
      "training loss is:  2.8289499282836914\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  15\n",
      "training loss is:  3.591052770614624\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  16\n",
      "training loss is:  3.3381762504577637\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  17\n",
      "training loss is:  3.6654601097106934\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  18\n",
      "training loss is:  2.92482852935791\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  19\n",
      "training loss is:  2.8229684829711914\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  20\n",
      "training loss is:  6.005537509918213\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  21\n",
      "training loss is:  2.811830997467041\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  0\n",
      "training loss is:  5.973371505737305\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  1\n",
      "training loss is:  2.9295592308044434\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  2\n",
      "training loss is:  3.0238640308380127\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  3\n",
      "training loss is:  3.614893913269043\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  4\n",
      "training loss is:  4.714728832244873\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  5\n",
      "training loss is:  3.1390182971954346\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  6\n",
      "training loss is:  3.4388537406921387\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  7\n",
      "training loss is:  3.419086217880249\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  8\n",
      "training loss is:  2.940375566482544\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  9\n",
      "training loss is:  2.6200103759765625\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  10\n",
      "training loss is:  2.6783864498138428\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  11\n",
      "training loss is:  2.392364263534546\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  12\n",
      "training loss is:  2.8704819679260254\n",
      "\n",
      "\n",
      "validation loss is 2.9989626169204713\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  13\n",
      "training loss is:  3.481968402862549\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  14\n",
      "training loss is:  3.3753812313079834\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  15\n",
      "training loss is:  3.2599782943725586\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  16\n",
      "training loss is:  2.934140682220459\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  17\n",
      "training loss is:  2.6309783458709717\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  18\n",
      "training loss is:  2.758216619491577\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  19\n",
      "training loss is:  2.25634503364563\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  20\n",
      "training loss is:  2.7304461002349854\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  21\n",
      "training loss is:  2.392984628677368\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  0\n",
      "training loss is:  3.271733283996582\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  1\n",
      "training loss is:  3.3154685497283936\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  2\n",
      "training loss is:  2.485273599624634\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  3\n",
      "training loss is:  3.737609624862671\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  4\n",
      "training loss is:  3.71830677986145\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  5\n",
      "training loss is:  3.6124117374420166\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  6\n",
      "training loss is:  3.102170467376709\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  7\n",
      "training loss is:  2.4363088607788086\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  8\n",
      "training loss is:  3.0857675075531006\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  9\n",
      "training loss is:  2.6639819145202637\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  10\n",
      "training loss is:  2.971235752105713\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  11\n",
      "training loss is:  3.337256669998169\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  12\n",
      "training loss is:  2.9934475421905518\n",
      "\n",
      "\n",
      "validation loss is 2.9950034856796264\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  13\n",
      "training loss is:  5.781156063079834\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  14\n",
      "training loss is:  3.041078567504883\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  15\n",
      "training loss is:  2.694291353225708\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  16\n",
      "training loss is:  2.668912649154663\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  17\n",
      "training loss is:  2.9264743328094482\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  18\n",
      "training loss is:  3.004640817642212\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  19\n",
      "training loss is:  3.5234134197235107\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  20\n",
      "training loss is:  2.9909284114837646\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  21\n",
      "training loss is:  1.976853847503662\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  0\n",
      "training loss is:  2.729177474975586\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  1\n",
      "training loss is:  2.736290216445923\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  2\n",
      "training loss is:  3.219453811645508\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  3\n",
      "training loss is:  3.6134698390960693\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  4\n",
      "training loss is:  4.439792633056641\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  5\n",
      "training loss is:  3.429086685180664\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  6\n",
      "training loss is:  5.594317436218262\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  7\n",
      "training loss is:  3.381950616836548\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  8\n",
      "training loss is:  2.951725482940674\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  9\n",
      "training loss is:  2.8524467945098877\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  10\n",
      "training loss is:  2.6545560359954834\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  11\n",
      "training loss is:  2.8343236446380615\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  12\n",
      "training loss is:  3.3635551929473877\n",
      "\n",
      "\n",
      "validation loss is 2.994939684867859\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  13\n",
      "training loss is:  3.002192974090576\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  14\n",
      "training loss is:  2.9664783477783203\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  15\n",
      "training loss is:  3.1446282863616943\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  16\n",
      "training loss is:  2.485356330871582\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  17\n",
      "training loss is:  2.582035779953003\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  18\n",
      "training loss is:  2.8613040447235107\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  19\n",
      "training loss is:  2.8965299129486084\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  20\n",
      "training loss is:  3.0616729259490967\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  21\n",
      "training loss is:  3.4494385719299316\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  0\n",
      "training loss is:  2.818385124206543\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  1\n",
      "training loss is:  3.730391263961792\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  2\n",
      "training loss is:  2.423607349395752\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  3\n",
      "training loss is:  2.5996921062469482\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  4\n",
      "training loss is:  2.612023115158081\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  5\n",
      "training loss is:  5.759699821472168\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  6\n",
      "training loss is:  2.4379453659057617\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  7\n",
      "training loss is:  3.7235336303710938\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  8\n",
      "training loss is:  4.534403324127197\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  9\n",
      "training loss is:  3.270448923110962\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  10\n",
      "training loss is:  2.8045413494110107\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  11\n",
      "training loss is:  2.8499197959899902\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  12\n",
      "training loss is:  3.185312509536743\n",
      "\n",
      "\n",
      "validation loss is 2.9958799362182615\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  13\n",
      "training loss is:  3.9904065132141113\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  14\n",
      "training loss is:  2.654297113418579\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  15\n",
      "training loss is:  3.2672479152679443\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  16\n",
      "training loss is:  3.0966124534606934\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  17\n",
      "training loss is:  3.0373663902282715\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  18\n",
      "training loss is:  2.93379282951355\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  19\n",
      "training loss is:  2.6054608821868896\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  20\n",
      "training loss is:  2.6808528900146484\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  21\n",
      "training loss is:  2.8108837604522705\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  0\n",
      "training loss is:  2.8497314453125\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  1\n",
      "training loss is:  3.9951767921447754\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  2\n",
      "training loss is:  2.959273099899292\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  3\n",
      "training loss is:  2.694425344467163\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  4\n",
      "training loss is:  2.649290084838867\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  5\n",
      "training loss is:  2.7905826568603516\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  6\n",
      "training loss is:  3.135312080383301\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  7\n",
      "training loss is:  2.9641895294189453\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  8\n",
      "training loss is:  2.543090581893921\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  9\n",
      "training loss is:  3.3868753910064697\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  10\n",
      "training loss is:  2.4282097816467285\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  11\n",
      "training loss is:  3.641139268875122\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  12\n",
      "training loss is:  3.325376033782959\n",
      "\n",
      "\n",
      "validation loss is 2.9954708099365233\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  13\n",
      "training loss is:  3.853679895401001\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  14\n",
      "training loss is:  2.329768180847168\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  15\n",
      "training loss is:  3.351228952407837\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  16\n",
      "training loss is:  5.014632701873779\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  17\n",
      "training loss is:  3.071690082550049\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  18\n",
      "training loss is:  3.048100233078003\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  19\n",
      "training loss is:  3.4501214027404785\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  20\n",
      "training loss is:  3.090834617614746\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  21\n",
      "training loss is:  4.148360729217529\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  0\n",
      "training loss is:  3.1724393367767334\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  1\n",
      "training loss is:  2.6337087154388428\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  2\n",
      "training loss is:  2.8017187118530273\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  3\n",
      "training loss is:  2.7162134647369385\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  4\n",
      "training loss is:  5.191967010498047\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  5\n",
      "training loss is:  3.477119207382202\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  6\n",
      "training loss is:  3.1642391681671143\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  7\n",
      "training loss is:  3.4031641483306885\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  8\n",
      "training loss is:  2.8774030208587646\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  9\n",
      "training loss is:  3.9998319149017334\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  10\n",
      "training loss is:  3.446486711502075\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  11\n",
      "training loss is:  3.293572425842285\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  12\n",
      "training loss is:  2.3032734394073486\n",
      "\n",
      "\n",
      "validation loss is 2.9947306632995607\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  13\n",
      "training loss is:  3.384626865386963\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  14\n",
      "training loss is:  3.1168267726898193\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  15\n",
      "training loss is:  3.043781042098999\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  16\n",
      "training loss is:  2.7424850463867188\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  17\n",
      "training loss is:  3.644152879714966\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  18\n",
      "training loss is:  2.1940832138061523\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  19\n",
      "training loss is:  2.6932549476623535\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  20\n",
      "training loss is:  3.1096060276031494\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  21\n",
      "training loss is:  4.551868438720703\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  0\n",
      "training loss is:  3.3373968601226807\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  1\n",
      "training loss is:  3.0713369846343994\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  2\n",
      "training loss is:  3.425114154815674\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  3\n",
      "training loss is:  3.982922315597534\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  4\n",
      "training loss is:  2.632805109024048\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  5\n",
      "training loss is:  3.0970165729522705\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  6\n",
      "training loss is:  2.808135986328125\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  7\n",
      "training loss is:  3.0238735675811768\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  8\n",
      "training loss is:  2.447930335998535\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  9\n",
      "training loss is:  4.900639533996582\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  10\n",
      "training loss is:  2.3952248096466064\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  11\n",
      "training loss is:  3.883126735687256\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  12\n",
      "training loss is:  3.617733955383301\n",
      "\n",
      "\n",
      "validation loss is 2.9952189207077025\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  13\n",
      "training loss is:  3.1003451347351074\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  14\n",
      "training loss is:  3.5310730934143066\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  15\n",
      "training loss is:  3.0099103450775146\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  16\n",
      "training loss is:  2.785916328430176\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  17\n",
      "training loss is:  2.7532148361206055\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  18\n",
      "training loss is:  3.6120874881744385\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  19\n",
      "training loss is:  2.9103505611419678\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  20\n",
      "training loss is:  2.4872493743896484\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  21\n",
      "training loss is:  3.3557865619659424\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  0\n",
      "training loss is:  3.0786221027374268\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  1\n",
      "training loss is:  2.7099249362945557\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  2\n",
      "training loss is:  3.2340455055236816\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  3\n",
      "training loss is:  2.94563364982605\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  4\n",
      "training loss is:  2.699491024017334\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  5\n",
      "training loss is:  3.1092724800109863\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  6\n",
      "training loss is:  5.5728888511657715\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  7\n",
      "training loss is:  3.1163339614868164\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  8\n",
      "training loss is:  2.651458978652954\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  9\n",
      "training loss is:  3.5542972087860107\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  10\n",
      "training loss is:  3.6516876220703125\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  11\n",
      "training loss is:  2.693655014038086\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  12\n",
      "training loss is:  2.9611077308654785\n",
      "\n",
      "\n",
      "validation loss is 2.994870114326477\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  13\n",
      "training loss is:  2.938420057296753\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  14\n",
      "training loss is:  3.569167137145996\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  15\n",
      "training loss is:  3.234623908996582\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  16\n",
      "training loss is:  3.2241828441619873\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  17\n",
      "training loss is:  2.7244949340820312\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  18\n",
      "training loss is:  2.7097790241241455\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  19\n",
      "training loss is:  3.716564655303955\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  20\n",
      "training loss is:  2.677812099456787\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  21\n",
      "training loss is:  3.556389331817627\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  0\n",
      "training loss is:  3.2547147274017334\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  1\n",
      "training loss is:  2.402629852294922\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  2\n",
      "training loss is:  2.742769718170166\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  3\n",
      "training loss is:  2.7012903690338135\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  4\n",
      "training loss is:  2.8295319080352783\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  5\n",
      "training loss is:  5.366706371307373\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  6\n",
      "training loss is:  3.598187208175659\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  7\n",
      "training loss is:  2.7335317134857178\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  8\n",
      "training loss is:  3.3091440200805664\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  9\n",
      "training loss is:  3.680737257003784\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  10\n",
      "training loss is:  3.253976345062256\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  11\n",
      "training loss is:  3.150764226913452\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  12\n",
      "training loss is:  2.8273684978485107\n",
      "\n",
      "\n",
      "validation loss is 2.9946439266204834\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  13\n",
      "training loss is:  3.1098897457122803\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  14\n",
      "training loss is:  2.563601016998291\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  15\n",
      "training loss is:  2.6895458698272705\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  16\n",
      "training loss is:  3.6535332202911377\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  17\n",
      "training loss is:  2.5155951976776123\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  18\n",
      "training loss is:  3.743903875350952\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  19\n",
      "training loss is:  3.268230676651001\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  20\n",
      "training loss is:  2.9589011669158936\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  21\n",
      "training loss is:  4.658116817474365\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  0\n",
      "training loss is:  2.620626211166382\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  1\n",
      "training loss is:  3.5448641777038574\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  2\n",
      "training loss is:  4.383747577667236\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  3\n",
      "training loss is:  3.2399468421936035\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  4\n",
      "training loss is:  2.8873131275177\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  5\n",
      "training loss is:  3.4753403663635254\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  6\n",
      "training loss is:  2.784597396850586\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  7\n",
      "training loss is:  2.5614125728607178\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  8\n",
      "training loss is:  3.074960947036743\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  9\n",
      "training loss is:  2.9025344848632812\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  10\n",
      "training loss is:  3.245457649230957\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  11\n",
      "training loss is:  3.2772932052612305\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  12\n",
      "training loss is:  3.299356460571289\n",
      "\n",
      "\n",
      "validation loss is 2.9945991039276123\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  13\n",
      "training loss is:  3.1373300552368164\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  14\n",
      "training loss is:  2.623779058456421\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  15\n",
      "training loss is:  2.7211196422576904\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  16\n",
      "training loss is:  4.045446872711182\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  17\n",
      "training loss is:  2.5949618816375732\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  18\n",
      "training loss is:  2.6180691719055176\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  19\n",
      "training loss is:  3.1465964317321777\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  20\n",
      "training loss is:  5.051150798797607\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  21\n",
      "training loss is:  2.1999473571777344\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  0\n",
      "training loss is:  3.863492727279663\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  1\n",
      "training loss is:  3.0132226943969727\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  2\n",
      "training loss is:  2.6419973373413086\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  3\n",
      "training loss is:  3.055851697921753\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  4\n",
      "training loss is:  3.08145809173584\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  5\n",
      "training loss is:  2.3886311054229736\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  6\n",
      "training loss is:  2.5943143367767334\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  7\n",
      "training loss is:  2.658087968826294\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  8\n",
      "training loss is:  3.6400034427642822\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  9\n",
      "training loss is:  2.5385520458221436\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  10\n",
      "training loss is:  3.327529191970825\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  11\n",
      "training loss is:  2.4880824089050293\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  12\n",
      "training loss is:  3.042471408843994\n",
      "\n",
      "\n",
      "validation loss is 2.994778561592102\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  13\n",
      "training loss is:  2.542522668838501\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  14\n",
      "training loss is:  3.935828924179077\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  15\n",
      "training loss is:  2.9245455265045166\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  16\n",
      "training loss is:  2.91935658454895\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  17\n",
      "training loss is:  6.502665996551514\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  18\n",
      "training loss is:  3.4772543907165527\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  19\n",
      "training loss is:  2.6178643703460693\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  20\n",
      "training loss is:  3.3322525024414062\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  21\n",
      "training loss is:  3.9842910766601562\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  0\n",
      "training loss is:  2.8483922481536865\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  1\n",
      "training loss is:  2.8876824378967285\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  2\n",
      "training loss is:  3.738779306411743\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  3\n",
      "training loss is:  3.3142778873443604\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  4\n",
      "training loss is:  2.7320709228515625\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  5\n",
      "training loss is:  3.0396649837493896\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  6\n",
      "training loss is:  2.767033338546753\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  7\n",
      "training loss is:  2.6654510498046875\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  8\n",
      "training loss is:  2.8400824069976807\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  9\n",
      "training loss is:  2.8907711505889893\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  10\n",
      "training loss is:  3.3470089435577393\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  11\n",
      "training loss is:  3.168499708175659\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  12\n",
      "training loss is:  4.663141250610352\n",
      "\n",
      "\n",
      "validation loss is 2.996555495262146\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  13\n",
      "training loss is:  3.043433904647827\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  14\n",
      "training loss is:  3.1225533485412598\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  15\n",
      "training loss is:  5.344415664672852\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  16\n",
      "training loss is:  2.5261118412017822\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  17\n",
      "training loss is:  3.281073808670044\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  18\n",
      "training loss is:  2.8108510971069336\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  19\n",
      "training loss is:  2.984015464782715\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  20\n",
      "training loss is:  3.1522860527038574\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  21\n",
      "training loss is:  2.4819042682647705\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  0\n",
      "training loss is:  3.3081881999969482\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  1\n",
      "training loss is:  2.989788770675659\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  2\n",
      "training loss is:  3.0190060138702393\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  3\n",
      "training loss is:  2.9334847927093506\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  4\n",
      "training loss is:  2.1831018924713135\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  5\n",
      "training loss is:  2.4314801692962646\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  6\n",
      "training loss is:  3.1190106868743896\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  7\n",
      "training loss is:  3.186666488647461\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  8\n",
      "training loss is:  2.8237531185150146\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  9\n",
      "training loss is:  3.602908134460449\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  10\n",
      "training loss is:  3.100740909576416\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  11\n",
      "training loss is:  2.866208791732788\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  12\n",
      "training loss is:  3.9920380115509033\n",
      "\n",
      "\n",
      "validation loss is 2.99533896446228\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  13\n",
      "training loss is:  2.926851511001587\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  14\n",
      "training loss is:  3.810878276824951\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  15\n",
      "training loss is:  3.1101160049438477\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  16\n",
      "training loss is:  2.455904006958008\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  17\n",
      "training loss is:  4.927218437194824\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  18\n",
      "training loss is:  2.938216209411621\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  19\n",
      "training loss is:  3.431433916091919\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  20\n",
      "training loss is:  3.994530439376831\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  21\n",
      "training loss is:  2.3946616649627686\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  0\n",
      "training loss is:  3.1730923652648926\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  1\n",
      "training loss is:  3.1425249576568604\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  2\n",
      "training loss is:  5.461009502410889\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  3\n",
      "training loss is:  3.7155861854553223\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  4\n",
      "training loss is:  2.7789623737335205\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  5\n",
      "training loss is:  3.370913505554199\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  6\n",
      "training loss is:  3.7252113819122314\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  7\n",
      "training loss is:  3.2473092079162598\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  8\n",
      "training loss is:  2.4271674156188965\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  9\n",
      "training loss is:  3.3469061851501465\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  10\n",
      "training loss is:  3.673832893371582\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  11\n",
      "training loss is:  2.558377981185913\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  12\n",
      "training loss is:  2.666227102279663\n",
      "\n",
      "\n",
      "validation loss is 2.995235300064087\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  13\n",
      "training loss is:  2.286189556121826\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  14\n",
      "training loss is:  3.222853899002075\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  15\n",
      "training loss is:  2.514533042907715\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  16\n",
      "training loss is:  3.0049166679382324\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  17\n",
      "training loss is:  3.0793423652648926\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  18\n",
      "training loss is:  3.893906354904175\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  19\n",
      "training loss is:  3.3500614166259766\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  20\n",
      "training loss is:  2.3363752365112305\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  21\n",
      "training loss is:  2.9525673389434814\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  0\n",
      "training loss is:  3.54591703414917\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  1\n",
      "training loss is:  3.1267929077148438\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  2\n",
      "training loss is:  3.1183602809906006\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  3\n",
      "training loss is:  2.460514783859253\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  4\n",
      "training loss is:  2.6694223880767822\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  5\n",
      "training loss is:  2.97430419921875\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  6\n",
      "training loss is:  3.168567180633545\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  7\n",
      "training loss is:  2.8570117950439453\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  8\n",
      "training loss is:  2.751290798187256\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  9\n",
      "training loss is:  3.1112122535705566\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  10\n",
      "training loss is:  2.2199320793151855\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  11\n",
      "training loss is:  3.900968074798584\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  12\n",
      "training loss is:  2.515031099319458\n",
      "\n",
      "\n",
      "validation loss is 2.9961757898330688\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  13\n",
      "training loss is:  3.1911392211914062\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  14\n",
      "training loss is:  3.4364240169525146\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  15\n",
      "training loss is:  3.1456050872802734\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  16\n",
      "training loss is:  3.1910178661346436\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  17\n",
      "training loss is:  2.779557228088379\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  18\n",
      "training loss is:  3.7386116981506348\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  19\n",
      "training loss is:  3.000072956085205\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  20\n",
      "training loss is:  3.687912702560425\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  21\n",
      "training loss is:  9.456220626831055\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  0\n",
      "training loss is:  2.5716159343719482\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  1\n",
      "training loss is:  3.7904651165008545\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  2\n",
      "training loss is:  3.209707021713257\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  3\n",
      "training loss is:  3.5645647048950195\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  4\n",
      "training loss is:  2.7945239543914795\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  5\n",
      "training loss is:  2.80778169631958\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  6\n",
      "training loss is:  2.9142754077911377\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  7\n",
      "training loss is:  3.0347495079040527\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  8\n",
      "training loss is:  2.6247522830963135\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  9\n",
      "training loss is:  2.9289655685424805\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  10\n",
      "training loss is:  5.825568199157715\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  11\n",
      "training loss is:  3.828195571899414\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  12\n",
      "training loss is:  3.2373862266540527\n",
      "\n",
      "\n",
      "validation loss is 2.99545156955719\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  13\n",
      "training loss is:  3.9069314002990723\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  14\n",
      "training loss is:  2.4556760787963867\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  15\n",
      "training loss is:  2.812699317932129\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  16\n",
      "training loss is:  2.758923292160034\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  17\n",
      "training loss is:  3.0498833656311035\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  18\n",
      "training loss is:  3.110491991043091\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  19\n",
      "training loss is:  3.4277472496032715\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  20\n",
      "training loss is:  2.563763380050659\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  21\n",
      "training loss is:  2.3389527797698975\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  0\n",
      "training loss is:  3.1375844478607178\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  1\n",
      "training loss is:  5.65847635269165\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  2\n",
      "training loss is:  3.4029510021209717\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  3\n",
      "training loss is:  4.199836730957031\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  4\n",
      "training loss is:  2.8637189865112305\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  5\n",
      "training loss is:  2.8818061351776123\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  6\n",
      "training loss is:  2.778803825378418\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  7\n",
      "training loss is:  2.503828525543213\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  8\n",
      "training loss is:  2.988032817840576\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  9\n",
      "training loss is:  2.6108264923095703\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  10\n",
      "training loss is:  3.642993450164795\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  11\n",
      "training loss is:  2.527364730834961\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  12\n",
      "training loss is:  2.5816118717193604\n",
      "\n",
      "\n",
      "validation loss is 2.9951330423355103\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  13\n",
      "training loss is:  2.7396903038024902\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  14\n",
      "training loss is:  3.1496763229370117\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  15\n",
      "training loss is:  3.420729637145996\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  16\n",
      "training loss is:  3.374511480331421\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  17\n",
      "training loss is:  2.471270799636841\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  18\n",
      "training loss is:  3.02522611618042\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  19\n",
      "training loss is:  3.9776740074157715\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  20\n",
      "training loss is:  2.5894339084625244\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  21\n",
      "training loss is:  4.175816059112549\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  0\n",
      "training loss is:  3.4921507835388184\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  1\n",
      "training loss is:  3.661344051361084\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  2\n",
      "training loss is:  3.4429569244384766\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  3\n",
      "training loss is:  5.509530067443848\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  4\n",
      "training loss is:  3.443983793258667\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  5\n",
      "training loss is:  3.696598529815674\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  6\n",
      "training loss is:  3.045694589614868\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  7\n",
      "training loss is:  2.314035415649414\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  8\n",
      "training loss is:  2.958493947982788\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  9\n",
      "training loss is:  3.102466106414795\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  10\n",
      "training loss is:  2.7331795692443848\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  11\n",
      "training loss is:  2.76267671585083\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  12\n",
      "training loss is:  2.8825151920318604\n",
      "\n",
      "\n",
      "validation loss is 2.995228099822998\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  13\n",
      "training loss is:  2.9866623878479004\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  14\n",
      "training loss is:  2.6987836360931396\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  15\n",
      "training loss is:  3.2799484729766846\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  16\n",
      "training loss is:  3.0806379318237305\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  17\n",
      "training loss is:  3.3066484928131104\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  18\n",
      "training loss is:  2.8512685298919678\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  19\n",
      "training loss is:  3.4341046810150146\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  20\n",
      "training loss is:  2.461796760559082\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  21\n",
      "training loss is:  2.4184200763702393\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  0\n",
      "training loss is:  3.0308218002319336\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  1\n",
      "training loss is:  2.9307188987731934\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  2\n",
      "training loss is:  3.086757183074951\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  3\n",
      "training loss is:  3.6562812328338623\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  4\n",
      "training loss is:  2.8535938262939453\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  5\n",
      "training loss is:  3.2744369506835938\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  6\n",
      "training loss is:  2.5494251251220703\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  7\n",
      "training loss is:  2.3782336711883545\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  8\n",
      "training loss is:  3.1440911293029785\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  9\n",
      "training loss is:  3.358966588973999\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  10\n",
      "training loss is:  3.797656297683716\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  11\n",
      "training loss is:  4.141068458557129\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  12\n",
      "training loss is:  2.7857325077056885\n",
      "\n",
      "\n",
      "validation loss is 2.995958161354065\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  13\n",
      "training loss is:  3.2966911792755127\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  14\n",
      "training loss is:  2.897226333618164\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  15\n",
      "training loss is:  2.4711356163024902\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  16\n",
      "training loss is:  5.109698295593262\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  17\n",
      "training loss is:  2.9287216663360596\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  18\n",
      "training loss is:  3.0023727416992188\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  19\n",
      "training loss is:  3.3419864177703857\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  20\n",
      "training loss is:  3.0618276596069336\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  21\n",
      "training loss is:  2.5131497383117676\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  0\n",
      "training loss is:  3.2274584770202637\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  1\n",
      "training loss is:  2.594697952270508\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  2\n",
      "training loss is:  4.960275173187256\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  3\n",
      "training loss is:  3.1610724925994873\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  4\n",
      "training loss is:  4.0343918800354\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  5\n",
      "training loss is:  3.1023495197296143\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  6\n",
      "training loss is:  2.662567138671875\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  7\n",
      "training loss is:  3.826028347015381\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  8\n",
      "training loss is:  2.876948595046997\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  9\n",
      "training loss is:  3.291736364364624\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  10\n",
      "training loss is:  3.0319159030914307\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  11\n",
      "training loss is:  3.0296883583068848\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  12\n",
      "training loss is:  3.040472984313965\n",
      "\n",
      "\n",
      "validation loss is 2.9946974754333495\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  13\n",
      "training loss is:  3.193997383117676\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  14\n",
      "training loss is:  3.0500566959381104\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  15\n",
      "training loss is:  3.019526481628418\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  16\n",
      "training loss is:  2.9984214305877686\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  17\n",
      "training loss is:  2.879030466079712\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  18\n",
      "training loss is:  2.437807559967041\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  19\n",
      "training loss is:  2.959294080734253\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  20\n",
      "training loss is:  3.225945472717285\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  21\n",
      "training loss is:  3.8902504444122314\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  0\n",
      "training loss is:  2.963611364364624\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  1\n",
      "training loss is:  2.3808975219726562\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  2\n",
      "training loss is:  3.1938281059265137\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  3\n",
      "training loss is:  3.1452550888061523\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  4\n",
      "training loss is:  2.620375633239746\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  5\n",
      "training loss is:  2.565512180328369\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  6\n",
      "training loss is:  2.8619225025177\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  7\n",
      "training loss is:  4.44908332824707\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  8\n",
      "training loss is:  2.7708652019500732\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  9\n",
      "training loss is:  2.8873958587646484\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  10\n",
      "training loss is:  4.014629364013672\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  11\n",
      "training loss is:  3.108114004135132\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  12\n",
      "training loss is:  2.740934133529663\n",
      "\n",
      "\n",
      "validation loss is 2.9960781812667845\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  13\n",
      "training loss is:  2.9295296669006348\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  14\n",
      "training loss is:  3.577031135559082\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  15\n",
      "training loss is:  3.798433542251587\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  16\n",
      "training loss is:  5.463929653167725\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  17\n",
      "training loss is:  2.659635305404663\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  18\n",
      "training loss is:  2.9782776832580566\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  19\n",
      "training loss is:  3.225362539291382\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  20\n",
      "training loss is:  2.824369192123413\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  21\n",
      "training loss is:  2.4985029697418213\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  0\n",
      "training loss is:  2.228736162185669\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  1\n",
      "training loss is:  3.238312005996704\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  2\n",
      "training loss is:  5.417751312255859\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  3\n",
      "training loss is:  3.1342334747314453\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  4\n",
      "training loss is:  2.892951250076294\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  5\n",
      "training loss is:  2.999006748199463\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  6\n",
      "training loss is:  2.7195510864257812\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  7\n",
      "training loss is:  3.7955243587493896\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  8\n",
      "training loss is:  3.3522608280181885\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  9\n",
      "training loss is:  3.365786552429199\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  10\n",
      "training loss is:  2.6223065853118896\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  11\n",
      "training loss is:  2.8823845386505127\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  12\n",
      "training loss is:  3.5780749320983887\n",
      "\n",
      "\n",
      "validation loss is 2.9948140144348145\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  13\n",
      "training loss is:  2.745288372039795\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  14\n",
      "training loss is:  2.6329474449157715\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  15\n",
      "training loss is:  2.677156448364258\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  16\n",
      "training loss is:  3.1508398056030273\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  17\n",
      "training loss is:  2.826458692550659\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  18\n",
      "training loss is:  2.8249263763427734\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  19\n",
      "training loss is:  2.8794350624084473\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  20\n",
      "training loss is:  3.7535464763641357\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  21\n",
      "training loss is:  6.37910795211792\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  0\n",
      "training loss is:  2.873911142349243\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  1\n",
      "training loss is:  2.552361249923706\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  2\n",
      "training loss is:  2.7039437294006348\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  3\n",
      "training loss is:  3.0852723121643066\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  4\n",
      "training loss is:  3.099538803100586\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  5\n",
      "training loss is:  2.976642370223999\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  6\n",
      "training loss is:  2.975648880004883\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  7\n",
      "training loss is:  2.4933922290802\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  8\n",
      "training loss is:  5.0871052742004395\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  9\n",
      "training loss is:  3.246067523956299\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  10\n",
      "training loss is:  2.773900032043457\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  11\n",
      "training loss is:  3.054718255996704\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  12\n",
      "training loss is:  3.0767040252685547\n",
      "\n",
      "\n",
      "validation loss is 2.995863914489746\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  13\n",
      "training loss is:  3.412689447402954\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  14\n",
      "training loss is:  2.8201851844787598\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  15\n",
      "training loss is:  3.194830894470215\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  16\n",
      "training loss is:  3.033912420272827\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  17\n",
      "training loss is:  2.9375393390655518\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  18\n",
      "training loss is:  4.912136077880859\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  19\n",
      "training loss is:  3.5223031044006348\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  20\n",
      "training loss is:  3.527841806411743\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  21\n",
      "training loss is:  1.9289640188217163\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  0\n",
      "training loss is:  2.925276756286621\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  1\n",
      "training loss is:  3.9860033988952637\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  2\n",
      "training loss is:  3.2839441299438477\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  3\n",
      "training loss is:  2.4041666984558105\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  4\n",
      "training loss is:  2.8634095191955566\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  5\n",
      "training loss is:  2.9364571571350098\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  6\n",
      "training loss is:  2.4835927486419678\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  7\n",
      "training loss is:  2.930114984512329\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  8\n",
      "training loss is:  3.494051218032837\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  9\n",
      "training loss is:  5.085745811462402\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  10\n",
      "training loss is:  2.753960371017456\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  11\n",
      "training loss is:  3.3828604221343994\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  12\n",
      "training loss is:  3.0177154541015625\n",
      "\n",
      "\n",
      "validation loss is 2.996198034286499\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  13\n",
      "training loss is:  3.6476995944976807\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  14\n",
      "training loss is:  3.170668601989746\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  15\n",
      "training loss is:  3.212625503540039\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  16\n",
      "training loss is:  3.3057990074157715\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  17\n",
      "training loss is:  2.4599993228912354\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  18\n",
      "training loss is:  2.996044635772705\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  19\n",
      "training loss is:  2.8109307289123535\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  20\n",
      "training loss is:  3.3826465606689453\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  21\n",
      "training loss is:  4.049163818359375\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  0\n",
      "training loss is:  3.4057180881500244\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  1\n",
      "training loss is:  3.0295822620391846\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  2\n",
      "training loss is:  2.739788293838501\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  3\n",
      "training loss is:  3.4625911712646484\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  4\n",
      "training loss is:  3.122202157974243\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  5\n",
      "training loss is:  3.3874568939208984\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  6\n",
      "training loss is:  3.6371254920959473\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  7\n",
      "training loss is:  2.8221676349639893\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  8\n",
      "training loss is:  3.526397466659546\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  9\n",
      "training loss is:  2.9751901626586914\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  10\n",
      "training loss is:  3.157137155532837\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  11\n",
      "training loss is:  2.9973950386047363\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  12\n",
      "training loss is:  4.976953029632568\n",
      "\n",
      "\n",
      "validation loss is 2.994468331336975\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  13\n",
      "training loss is:  3.181698799133301\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  14\n",
      "training loss is:  3.086867570877075\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  15\n",
      "training loss is:  2.7038071155548096\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  16\n",
      "training loss is:  2.9419450759887695\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  17\n",
      "training loss is:  2.616283655166626\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  18\n",
      "training loss is:  2.726196527481079\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  19\n",
      "training loss is:  2.562955379486084\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  20\n",
      "training loss is:  4.1100873947143555\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  21\n",
      "training loss is:  2.367678165435791\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  0\n",
      "training loss is:  2.723609685897827\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  1\n",
      "training loss is:  2.9840469360351562\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  2\n",
      "training loss is:  2.548543691635132\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  3\n",
      "training loss is:  2.346433639526367\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  4\n",
      "training loss is:  3.1766717433929443\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  5\n",
      "training loss is:  3.8619625568389893\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  6\n",
      "training loss is:  2.907470464706421\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  7\n",
      "training loss is:  3.7374372482299805\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  8\n",
      "training loss is:  3.5164425373077393\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  9\n",
      "training loss is:  2.7044496536254883\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  10\n",
      "training loss is:  2.8874988555908203\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  11\n",
      "training loss is:  3.1122727394104004\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  12\n",
      "training loss is:  2.6098618507385254\n",
      "\n",
      "\n",
      "validation loss is 2.9957650184631346\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  13\n",
      "training loss is:  3.457277774810791\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  14\n",
      "training loss is:  5.327566623687744\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  15\n",
      "training loss is:  2.7450714111328125\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  16\n",
      "training loss is:  3.9381375312805176\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  17\n",
      "training loss is:  3.2655487060546875\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  18\n",
      "training loss is:  2.5230159759521484\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  19\n",
      "training loss is:  3.423017978668213\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  20\n",
      "training loss is:  3.2068281173706055\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  21\n",
      "training loss is:  2.8760743141174316\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  0\n",
      "training loss is:  3.208677053451538\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  1\n",
      "training loss is:  2.7793033123016357\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  2\n",
      "training loss is:  4.093173027038574\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  3\n",
      "training loss is:  3.6413564682006836\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  4\n",
      "training loss is:  2.894458055496216\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  5\n",
      "training loss is:  3.337801218032837\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  6\n",
      "training loss is:  4.262507438659668\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  7\n",
      "training loss is:  2.319579601287842\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  8\n",
      "training loss is:  3.0783157348632812\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  9\n",
      "training loss is:  2.571150779724121\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  10\n",
      "training loss is:  3.2925631999969482\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  11\n",
      "training loss is:  2.7450037002563477\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  12\n",
      "training loss is:  4.959135055541992\n",
      "\n",
      "\n",
      "validation loss is 2.996703028678894\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  13\n",
      "training loss is:  2.2653214931488037\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  14\n",
      "training loss is:  2.9319846630096436\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  15\n",
      "training loss is:  3.067758798599243\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  16\n",
      "training loss is:  3.2246968746185303\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  17\n",
      "training loss is:  3.1509456634521484\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  18\n",
      "training loss is:  3.613265037536621\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  19\n",
      "training loss is:  2.6551625728607178\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  20\n",
      "training loss is:  3.0828428268432617\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  21\n",
      "training loss is:  2.2194666862487793\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  0\n",
      "training loss is:  3.403935432434082\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  1\n",
      "training loss is:  2.7319247722625732\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  2\n",
      "training loss is:  3.005673885345459\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  3\n",
      "training loss is:  3.9545016288757324\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  4\n",
      "training loss is:  2.8459203243255615\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  5\n",
      "training loss is:  3.265256881713867\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  6\n",
      "training loss is:  2.5512125492095947\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  7\n",
      "training loss is:  2.7882065773010254\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  8\n",
      "training loss is:  3.4451780319213867\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  9\n",
      "training loss is:  3.231207847595215\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  10\n",
      "training loss is:  2.7740132808685303\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  11\n",
      "training loss is:  3.4569385051727295\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  12\n",
      "training loss is:  3.4835119247436523\n",
      "\n",
      "\n",
      "validation loss is 2.9989199161529543\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  13\n",
      "training loss is:  2.9014174938201904\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  14\n",
      "training loss is:  2.700674533843994\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  15\n",
      "training loss is:  3.102074384689331\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  16\n",
      "training loss is:  3.072355270385742\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  17\n",
      "training loss is:  2.984450101852417\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  18\n",
      "training loss is:  5.4548563957214355\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  19\n",
      "training loss is:  2.732332229614258\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  20\n",
      "training loss is:  3.5061187744140625\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  21\n",
      "training loss is:  1.6727941036224365\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  0\n",
      "training loss is:  2.892418146133423\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  1\n",
      "training loss is:  2.553230047225952\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  2\n",
      "training loss is:  2.9460930824279785\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  3\n",
      "training loss is:  3.2713124752044678\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  4\n",
      "training loss is:  2.737460136413574\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  5\n",
      "training loss is:  3.7467620372772217\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  6\n",
      "training loss is:  3.209005117416382\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  7\n",
      "training loss is:  3.3573195934295654\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  8\n",
      "training loss is:  2.5028812885284424\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  9\n",
      "training loss is:  2.8160669803619385\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  10\n",
      "training loss is:  2.354248523712158\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  11\n",
      "training loss is:  3.2281057834625244\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  12\n",
      "training loss is:  2.6263275146484375\n",
      "\n",
      "\n",
      "validation loss is 2.994788312911987\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  13\n",
      "training loss is:  3.5267629623413086\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  14\n",
      "training loss is:  2.925854206085205\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  15\n",
      "training loss is:  2.4247772693634033\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  16\n",
      "training loss is:  3.8390555381774902\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  17\n",
      "training loss is:  2.5507514476776123\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  18\n",
      "training loss is:  3.6538732051849365\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  19\n",
      "training loss is:  6.480005264282227\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  20\n",
      "training loss is:  3.351142168045044\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  21\n",
      "training loss is:  2.807831287384033\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  0\n",
      "training loss is:  3.1815860271453857\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  1\n",
      "training loss is:  3.313002824783325\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  2\n",
      "training loss is:  2.750166654586792\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  3\n",
      "training loss is:  3.3449981212615967\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  4\n",
      "training loss is:  3.6822142601013184\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  5\n",
      "training loss is:  2.6598949432373047\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  6\n",
      "training loss is:  2.9975147247314453\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  7\n",
      "training loss is:  3.2895736694335938\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  8\n",
      "training loss is:  3.2696950435638428\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  9\n",
      "training loss is:  2.9384424686431885\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  10\n",
      "training loss is:  3.3888814449310303\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  11\n",
      "training loss is:  2.422917604446411\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  12\n",
      "training loss is:  3.759420156478882\n",
      "\n",
      "\n",
      "validation loss is 2.994884419441223\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  13\n",
      "training loss is:  2.1757636070251465\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  14\n",
      "training loss is:  3.1277215480804443\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  15\n",
      "training loss is:  2.9563043117523193\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  16\n",
      "training loss is:  2.9719409942626953\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  17\n",
      "training loss is:  3.6443986892700195\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  18\n",
      "training loss is:  2.24959135055542\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  19\n",
      "training loss is:  5.5205464363098145\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  20\n",
      "training loss is:  3.2433547973632812\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  21\n",
      "training loss is:  3.098663806915283\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  0\n",
      "training loss is:  5.672371864318848\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  1\n",
      "training loss is:  2.777639865875244\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  2\n",
      "training loss is:  2.8651556968688965\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  3\n",
      "training loss is:  3.0743203163146973\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  4\n",
      "training loss is:  3.139415979385376\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  5\n",
      "training loss is:  4.126344203948975\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  6\n",
      "training loss is:  2.963268995285034\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  7\n",
      "training loss is:  3.9783272743225098\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  8\n",
      "training loss is:  3.0765979290008545\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  9\n",
      "training loss is:  3.986309766769409\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  10\n",
      "training loss is:  3.480538845062256\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  11\n",
      "training loss is:  2.437161922454834\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  12\n",
      "training loss is:  2.8925130367279053\n",
      "\n",
      "\n",
      "validation loss is 2.9950596570968626\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  13\n",
      "training loss is:  2.8541100025177\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  14\n",
      "training loss is:  3.9442098140716553\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  15\n",
      "training loss is:  2.9170010089874268\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  16\n",
      "training loss is:  2.416996479034424\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  17\n",
      "training loss is:  2.5673699378967285\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  18\n",
      "training loss is:  2.5181190967559814\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  19\n",
      "training loss is:  2.4089560508728027\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  20\n",
      "training loss is:  2.9850988388061523\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  21\n",
      "training loss is:  2.610628128051758\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  0\n",
      "training loss is:  2.508941650390625\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  1\n",
      "training loss is:  3.0112619400024414\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  2\n",
      "training loss is:  3.026303291320801\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  3\n",
      "training loss is:  2.467653274536133\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  4\n",
      "training loss is:  2.492537498474121\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  5\n",
      "training loss is:  3.1535277366638184\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  6\n",
      "training loss is:  2.524674415588379\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  7\n",
      "training loss is:  3.3670432567596436\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  8\n",
      "training loss is:  2.695910930633545\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  9\n",
      "training loss is:  5.35107421875\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  10\n",
      "training loss is:  3.1532912254333496\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  11\n",
      "training loss is:  3.100559711456299\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  12\n",
      "training loss is:  2.88214111328125\n",
      "\n",
      "\n",
      "validation loss is 3.0001930236816405\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  13\n",
      "training loss is:  2.618988275527954\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  14\n",
      "training loss is:  3.105289936065674\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  15\n",
      "training loss is:  3.058425188064575\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  16\n",
      "training loss is:  3.316816806793213\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  17\n",
      "training loss is:  2.658200740814209\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  18\n",
      "training loss is:  4.5174946784973145\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  19\n",
      "training loss is:  3.5643694400787354\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  20\n",
      "training loss is:  3.4469215869903564\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  21\n",
      "training loss is:  5.494128704071045\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  0\n",
      "training loss is:  4.081542491912842\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  1\n",
      "training loss is:  2.629077196121216\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  2\n",
      "training loss is:  2.5806827545166016\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  3\n",
      "training loss is:  3.2233121395111084\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  4\n",
      "training loss is:  2.8312385082244873\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  5\n",
      "training loss is:  3.0869314670562744\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  6\n",
      "training loss is:  2.7215499877929688\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  7\n",
      "training loss is:  3.235900640487671\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  8\n",
      "training loss is:  2.539504051208496\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  9\n",
      "training loss is:  2.165071725845337\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  10\n",
      "training loss is:  6.232195854187012\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  11\n",
      "training loss is:  3.517991065979004\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  12\n",
      "training loss is:  3.5654544830322266\n",
      "\n",
      "\n",
      "validation loss is 2.9943742990493774\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  13\n",
      "training loss is:  3.2639498710632324\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  14\n",
      "training loss is:  3.669605016708374\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  15\n",
      "training loss is:  3.1854279041290283\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  16\n",
      "training loss is:  3.0542802810668945\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  17\n",
      "training loss is:  3.1691348552703857\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  18\n",
      "training loss is:  3.051846504211426\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  19\n",
      "training loss is:  2.5896308422088623\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  20\n",
      "training loss is:  2.6205084323883057\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  21\n",
      "training loss is:  2.70999813079834\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  0\n",
      "training loss is:  4.470808982849121\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  1\n",
      "training loss is:  2.7578139305114746\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  2\n",
      "training loss is:  2.9489059448242188\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  3\n",
      "training loss is:  2.4419946670532227\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  4\n",
      "training loss is:  3.390977144241333\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  5\n",
      "training loss is:  5.459204196929932\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  6\n",
      "training loss is:  2.713291645050049\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  7\n",
      "training loss is:  2.8506557941436768\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  8\n",
      "training loss is:  3.0599563121795654\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  9\n",
      "training loss is:  2.8488516807556152\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  10\n",
      "training loss is:  3.3732616901397705\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  11\n",
      "training loss is:  2.622103214263916\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  12\n",
      "training loss is:  3.09969425201416\n",
      "\n",
      "\n",
      "validation loss is 2.9952962398529053\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  13\n",
      "training loss is:  3.4570159912109375\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  14\n",
      "training loss is:  2.3352136611938477\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  15\n",
      "training loss is:  3.9939587116241455\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  16\n",
      "training loss is:  2.655027389526367\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  17\n",
      "training loss is:  3.1029553413391113\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  18\n",
      "training loss is:  2.9998061656951904\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  19\n",
      "training loss is:  2.920302152633667\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  20\n",
      "training loss is:  3.6172821521759033\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  21\n",
      "training loss is:  2.449244737625122\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  0\n",
      "training loss is:  2.576603651046753\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  1\n",
      "training loss is:  3.2543556690216064\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  2\n",
      "training loss is:  3.5441393852233887\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  3\n",
      "training loss is:  2.8702988624572754\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  4\n",
      "training loss is:  3.0246059894561768\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  5\n",
      "training loss is:  5.6103620529174805\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  6\n",
      "training loss is:  3.2276389598846436\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  7\n",
      "training loss is:  3.4697988033294678\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  8\n",
      "training loss is:  2.955510139465332\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  9\n",
      "training loss is:  3.9603054523468018\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  10\n",
      "training loss is:  2.557126045227051\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  11\n",
      "training loss is:  2.610507011413574\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  12\n",
      "training loss is:  3.9428107738494873\n",
      "\n",
      "\n",
      "validation loss is 2.9944960117340087\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  13\n",
      "training loss is:  2.4175961017608643\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  14\n",
      "training loss is:  3.1217334270477295\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  15\n",
      "training loss is:  2.7718405723571777\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  16\n",
      "training loss is:  3.0134758949279785\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  17\n",
      "training loss is:  2.9646694660186768\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  18\n",
      "training loss is:  3.1801841259002686\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  19\n",
      "training loss is:  2.7221901416778564\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  20\n",
      "training loss is:  2.689990758895874\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  21\n",
      "training loss is:  4.237536430358887\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  0\n",
      "training loss is:  2.7898261547088623\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  1\n",
      "training loss is:  3.142258882522583\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  2\n",
      "training loss is:  2.7089548110961914\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  3\n",
      "training loss is:  2.7143044471740723\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  4\n",
      "training loss is:  3.644991874694824\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  5\n",
      "training loss is:  3.1194679737091064\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  6\n",
      "training loss is:  2.762580156326294\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  7\n",
      "training loss is:  3.0845611095428467\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  8\n",
      "training loss is:  3.3978676795959473\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  9\n",
      "training loss is:  4.1179518699646\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  10\n",
      "training loss is:  2.4417431354522705\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  11\n",
      "training loss is:  4.222139835357666\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  12\n",
      "training loss is:  2.7186331748962402\n",
      "\n",
      "\n",
      "validation loss is 2.994768571853638\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  13\n",
      "training loss is:  2.244093179702759\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  14\n",
      "training loss is:  2.8862099647521973\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  15\n",
      "training loss is:  3.3087968826293945\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  16\n",
      "training loss is:  3.0120530128479004\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  17\n",
      "training loss is:  3.348289728164673\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  18\n",
      "training loss is:  2.5781610012054443\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  19\n",
      "training loss is:  3.419658899307251\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  20\n",
      "training loss is:  5.40715217590332\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  21\n",
      "training loss is:  2.4775638580322266\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  0\n",
      "training loss is:  2.776740074157715\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  1\n",
      "training loss is:  3.07722806930542\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  2\n",
      "training loss is:  2.9704790115356445\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  3\n",
      "training loss is:  2.7728660106658936\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  4\n",
      "training loss is:  3.0448074340820312\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  5\n",
      "training loss is:  3.2271649837493896\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  6\n",
      "training loss is:  4.27143669128418\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  7\n",
      "training loss is:  5.13165283203125\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  8\n",
      "training loss is:  2.634012222290039\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  9\n",
      "training loss is:  2.975184440612793\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  10\n",
      "training loss is:  3.9912095069885254\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  11\n",
      "training loss is:  2.7239224910736084\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  12\n",
      "training loss is:  3.2631049156188965\n",
      "\n",
      "\n",
      "validation loss is 2.9942739725112917\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  13\n",
      "training loss is:  3.525449275970459\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  14\n",
      "training loss is:  3.219571113586426\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  15\n",
      "training loss is:  3.5321297645568848\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  16\n",
      "training loss is:  2.435274124145508\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  17\n",
      "training loss is:  2.6877245903015137\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  18\n",
      "training loss is:  2.7589242458343506\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  19\n",
      "training loss is:  2.8675241470336914\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  20\n",
      "training loss is:  3.14404296875\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  21\n",
      "training loss is:  2.708508014678955\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  0\n",
      "training loss is:  5.181292533874512\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  1\n",
      "training loss is:  2.773970603942871\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  2\n",
      "training loss is:  3.2062294483184814\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  3\n",
      "training loss is:  2.8179755210876465\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  4\n",
      "training loss is:  2.7360756397247314\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  5\n",
      "training loss is:  3.071087121963501\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  6\n",
      "training loss is:  3.1174721717834473\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  7\n",
      "training loss is:  3.7332711219787598\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  8\n",
      "training loss is:  2.9446029663085938\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  9\n",
      "training loss is:  2.726574659347534\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  10\n",
      "training loss is:  4.155187606811523\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  11\n",
      "training loss is:  2.728485345840454\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  12\n",
      "training loss is:  2.326075792312622\n",
      "\n",
      "\n",
      "validation loss is 2.995988368988037\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  13\n",
      "training loss is:  2.64666748046875\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  14\n",
      "training loss is:  2.5682642459869385\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  15\n",
      "training loss is:  3.3900928497314453\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  16\n",
      "training loss is:  3.0529961585998535\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  17\n",
      "training loss is:  3.8781235218048096\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  18\n",
      "training loss is:  2.942880153656006\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  19\n",
      "training loss is:  3.804398775100708\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  20\n",
      "training loss is:  3.2124478816986084\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  21\n",
      "training loss is:  2.5910229682922363\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  0\n",
      "training loss is:  2.807631015777588\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  1\n",
      "training loss is:  2.9028005599975586\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  2\n",
      "training loss is:  2.998547315597534\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  3\n",
      "training loss is:  3.0862302780151367\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  4\n",
      "training loss is:  3.347011089324951\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  5\n",
      "training loss is:  3.1144888401031494\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  6\n",
      "training loss is:  3.0302574634552\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  7\n",
      "training loss is:  2.829610586166382\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  8\n",
      "training loss is:  2.9199066162109375\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  9\n",
      "training loss is:  3.0518569946289062\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  10\n",
      "training loss is:  2.3385422229766846\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  11\n",
      "training loss is:  5.264293670654297\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  12\n",
      "training loss is:  2.5684778690338135\n",
      "\n",
      "\n",
      "validation loss is 2.9951485633850097\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  13\n",
      "training loss is:  3.9185354709625244\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  14\n",
      "training loss is:  4.585442066192627\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  15\n",
      "training loss is:  2.9066214561462402\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  16\n",
      "training loss is:  2.708345890045166\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  17\n",
      "training loss is:  2.6570796966552734\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  18\n",
      "training loss is:  2.767465114593506\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  19\n",
      "training loss is:  3.4857285022735596\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  20\n",
      "training loss is:  3.59782075881958\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  21\n",
      "training loss is:  2.881441116333008\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  0\n",
      "training loss is:  3.5347466468811035\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  1\n",
      "training loss is:  2.6343369483947754\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  2\n",
      "training loss is:  2.709683895111084\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  3\n",
      "training loss is:  4.096144676208496\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  4\n",
      "training loss is:  2.830754041671753\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  5\n",
      "training loss is:  3.6195449829101562\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  6\n",
      "training loss is:  3.381100654602051\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  7\n",
      "training loss is:  2.5187807083129883\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  8\n",
      "training loss is:  3.7578811645507812\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  9\n",
      "training loss is:  3.3240599632263184\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  10\n",
      "training loss is:  2.7317686080932617\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  11\n",
      "training loss is:  2.888711452484131\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  12\n",
      "training loss is:  3.0072057247161865\n",
      "\n",
      "\n",
      "validation loss is 2.9945760011672973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  13\n",
      "training loss is:  2.8255345821380615\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  14\n",
      "training loss is:  2.6120989322662354\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  15\n",
      "training loss is:  3.125915288925171\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  16\n",
      "training loss is:  3.0316131114959717\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  17\n",
      "training loss is:  2.5272884368896484\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  18\n",
      "training loss is:  3.442092180252075\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  19\n",
      "training loss is:  2.672499895095825\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  20\n",
      "training loss is:  5.908732891082764\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  21\n",
      "training loss is:  2.261136293411255\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  0\n",
      "training loss is:  3.6963186264038086\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  1\n",
      "training loss is:  3.241513729095459\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  2\n",
      "training loss is:  2.779878616333008\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  3\n",
      "training loss is:  3.1640841960906982\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  4\n",
      "training loss is:  3.1866588592529297\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  5\n",
      "training loss is:  2.780444622039795\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  6\n",
      "training loss is:  2.516319990158081\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  7\n",
      "training loss is:  3.5439453125\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  8\n",
      "training loss is:  3.5281872749328613\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  9\n",
      "training loss is:  5.8033366203308105\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  10\n",
      "training loss is:  2.735125780105591\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  11\n",
      "training loss is:  2.954392671585083\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  12\n",
      "training loss is:  3.4294793605804443\n",
      "\n",
      "\n",
      "validation loss is 2.994345784187317\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  13\n",
      "training loss is:  2.6657204627990723\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  14\n",
      "training loss is:  2.4218950271606445\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  15\n",
      "training loss is:  3.8206100463867188\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  16\n",
      "training loss is:  2.8855483531951904\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  17\n",
      "training loss is:  2.5376877784729004\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  18\n",
      "training loss is:  3.016413927078247\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  19\n",
      "training loss is:  2.4232683181762695\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  20\n",
      "training loss is:  3.689220905303955\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  21\n",
      "training loss is:  3.3037424087524414\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  0\n",
      "training loss is:  2.730001449584961\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  1\n",
      "training loss is:  3.3215081691741943\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  2\n",
      "training loss is:  2.321997880935669\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  3\n",
      "training loss is:  3.138047218322754\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  4\n",
      "training loss is:  2.9992151260375977\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  5\n",
      "training loss is:  2.7384023666381836\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  6\n",
      "training loss is:  3.0742177963256836\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  7\n",
      "training loss is:  2.794095039367676\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  8\n",
      "training loss is:  3.94572377204895\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  9\n",
      "training loss is:  3.9563541412353516\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  10\n",
      "training loss is:  2.8387978076934814\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  11\n",
      "training loss is:  2.5851988792419434\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  12\n",
      "training loss is:  2.8913040161132812\n",
      "\n",
      "\n",
      "validation loss is 2.9943172216415403\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  13\n",
      "training loss is:  6.082174301147461\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  14\n",
      "training loss is:  2.5382676124572754\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  15\n",
      "training loss is:  3.3688735961914062\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  16\n",
      "training loss is:  2.3652749061584473\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  17\n",
      "training loss is:  3.4851999282836914\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  18\n",
      "training loss is:  3.610398292541504\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  19\n",
      "training loss is:  3.419619083404541\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  20\n",
      "training loss is:  2.9012694358825684\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  21\n",
      "training loss is:  2.570787191390991\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  0\n",
      "training loss is:  2.45707368850708\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  1\n",
      "training loss is:  3.5323493480682373\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  2\n",
      "training loss is:  2.509478807449341\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  3\n",
      "training loss is:  3.0022335052490234\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  4\n",
      "training loss is:  3.1577489376068115\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  5\n",
      "training loss is:  3.3385322093963623\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  6\n",
      "training loss is:  2.9925754070281982\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  7\n",
      "training loss is:  3.1619458198547363\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  8\n",
      "training loss is:  3.8723037242889404\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  9\n",
      "training loss is:  2.8882498741149902\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  10\n",
      "training loss is:  5.239887237548828\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  11\n",
      "training loss is:  2.9466326236724854\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  12\n",
      "training loss is:  2.891300916671753\n",
      "\n",
      "\n",
      "validation loss is 2.9943902015686037\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  13\n",
      "training loss is:  3.077557325363159\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  14\n",
      "training loss is:  3.542904853820801\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  15\n",
      "training loss is:  3.5727035999298096\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  16\n",
      "training loss is:  3.0999526977539062\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  17\n",
      "training loss is:  2.4598748683929443\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  18\n",
      "training loss is:  3.9830756187438965\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  19\n",
      "training loss is:  2.5858511924743652\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  20\n",
      "training loss is:  2.9013047218322754\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  21\n",
      "training loss is:  2.005580186843872\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  0\n",
      "training loss is:  3.4919662475585938\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  1\n",
      "training loss is:  2.7771668434143066\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  2\n",
      "training loss is:  3.979689359664917\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  3\n",
      "training loss is:  2.587149143218994\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  4\n",
      "training loss is:  2.9288647174835205\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  5\n",
      "training loss is:  2.9546608924865723\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  6\n",
      "training loss is:  2.377439260482788\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  7\n",
      "training loss is:  2.9937736988067627\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  8\n",
      "training loss is:  2.719609260559082\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  9\n",
      "training loss is:  3.6410117149353027\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  10\n",
      "training loss is:  3.0238568782806396\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  11\n",
      "training loss is:  3.26762318611145\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  12\n",
      "training loss is:  2.808542490005493\n",
      "\n",
      "\n",
      "validation loss is 2.995139515399933\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  13\n",
      "training loss is:  3.0718514919281006\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  14\n",
      "training loss is:  3.250201463699341\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  15\n",
      "training loss is:  3.243856430053711\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  16\n",
      "training loss is:  3.6028618812561035\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  17\n",
      "training loss is:  3.227328300476074\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  18\n",
      "training loss is:  2.4662492275238037\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  19\n",
      "training loss is:  3.275693416595459\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  20\n",
      "training loss is:  5.692902088165283\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  21\n",
      "training loss is:  1.8224190473556519\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  0\n",
      "training loss is:  2.668984889984131\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  1\n",
      "training loss is:  3.4988205432891846\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  2\n",
      "training loss is:  3.6092312335968018\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  3\n",
      "training loss is:  2.9414291381835938\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  4\n",
      "training loss is:  3.149303913116455\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  5\n",
      "training loss is:  2.4438037872314453\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  6\n",
      "training loss is:  3.8295938968658447\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  7\n",
      "training loss is:  2.876873254776001\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  8\n",
      "training loss is:  2.565272569656372\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  9\n",
      "training loss is:  2.7973814010620117\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  10\n",
      "training loss is:  3.5553476810455322\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  11\n",
      "training loss is:  2.8671159744262695\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  12\n",
      "training loss is:  2.8985135555267334\n",
      "\n",
      "\n",
      "validation loss is 2.994481587409973\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  13\n",
      "training loss is:  5.853763580322266\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  14\n",
      "training loss is:  3.729663610458374\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  15\n",
      "training loss is:  2.697155714035034\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  16\n",
      "training loss is:  2.6280953884124756\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  17\n",
      "training loss is:  2.503143310546875\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  18\n",
      "training loss is:  3.223026752471924\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  19\n",
      "training loss is:  3.44755220413208\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  20\n",
      "training loss is:  3.327319860458374\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  21\n",
      "training loss is:  2.4426748752593994\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  0\n",
      "training loss is:  3.154805898666382\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  1\n",
      "training loss is:  3.4042720794677734\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  2\n",
      "training loss is:  2.819488763809204\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  3\n",
      "training loss is:  2.9829955101013184\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  4\n",
      "training loss is:  2.610548734664917\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  5\n",
      "training loss is:  5.139059066772461\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  6\n",
      "training loss is:  2.7540090084075928\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  7\n",
      "training loss is:  2.646716356277466\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  8\n",
      "training loss is:  2.7016103267669678\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  9\n",
      "training loss is:  3.128373146057129\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  10\n",
      "training loss is:  3.3350095748901367\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  11\n",
      "training loss is:  3.6253888607025146\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  12\n",
      "training loss is:  2.5980029106140137\n",
      "\n",
      "\n",
      "validation loss is 2.99461715221405\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  13\n",
      "training loss is:  3.227278470993042\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  14\n",
      "training loss is:  3.125107526779175\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  15\n",
      "training loss is:  2.7465505599975586\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  16\n",
      "training loss is:  3.0349721908569336\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  17\n",
      "training loss is:  2.9085254669189453\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  18\n",
      "training loss is:  2.852916955947876\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  19\n",
      "training loss is:  4.140591621398926\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  20\n",
      "training loss is:  3.9422311782836914\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  21\n",
      "training loss is:  3.0246591567993164\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  0\n",
      "training loss is:  5.535366058349609\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  1\n",
      "training loss is:  2.7060234546661377\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  2\n",
      "training loss is:  2.234253168106079\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  3\n",
      "training loss is:  2.714902639389038\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  4\n",
      "training loss is:  4.255577564239502\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  5\n",
      "training loss is:  3.2922720909118652\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  6\n",
      "training loss is:  3.3864004611968994\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  7\n",
      "training loss is:  3.958317518234253\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  8\n",
      "training loss is:  2.8960089683532715\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  9\n",
      "training loss is:  3.0925323963165283\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  10\n",
      "training loss is:  3.6329174041748047\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  11\n",
      "training loss is:  2.374485731124878\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  12\n",
      "training loss is:  2.773103713989258\n",
      "\n",
      "\n",
      "validation loss is 2.9954241037368776\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  13\n",
      "training loss is:  3.1043055057525635\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  14\n",
      "training loss is:  2.8243062496185303\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  15\n",
      "training loss is:  2.7952933311462402\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  16\n",
      "training loss is:  3.3557348251342773\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  17\n",
      "training loss is:  2.6656460762023926\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  18\n",
      "training loss is:  2.8804755210876465\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  19\n",
      "training loss is:  2.982428550720215\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  20\n",
      "training loss is:  3.224074125289917\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  21\n",
      "training loss is:  3.6339850425720215\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  0\n",
      "training loss is:  2.6847290992736816\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  1\n",
      "training loss is:  2.8367257118225098\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  2\n",
      "training loss is:  2.7406094074249268\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  3\n",
      "training loss is:  3.691805362701416\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  4\n",
      "training loss is:  2.995266914367676\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  5\n",
      "training loss is:  2.7350683212280273\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  6\n",
      "training loss is:  4.6201677322387695\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  7\n",
      "training loss is:  2.5296547412872314\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  8\n",
      "training loss is:  2.44880747795105\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  9\n",
      "training loss is:  2.6792471408843994\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  10\n",
      "training loss is:  3.874582290649414\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  11\n",
      "training loss is:  2.6334424018859863\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  12\n",
      "training loss is:  2.69514536857605\n",
      "\n",
      "\n",
      "validation loss is 2.9942808151245117\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  13\n",
      "training loss is:  3.221670627593994\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  14\n",
      "training loss is:  3.5963051319122314\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  15\n",
      "training loss is:  3.309440851211548\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  16\n",
      "training loss is:  2.8037421703338623\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  17\n",
      "training loss is:  3.498293876647949\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  18\n",
      "training loss is:  2.9552059173583984\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  19\n",
      "training loss is:  2.705167293548584\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  20\n",
      "training loss is:  3.066932439804077\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  21\n",
      "training loss is:  9.961141586303711\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  0\n",
      "training loss is:  3.0587170124053955\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  1\n",
      "training loss is:  2.7915778160095215\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  2\n",
      "training loss is:  4.619147300720215\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  3\n",
      "training loss is:  3.0773301124572754\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  4\n",
      "training loss is:  2.9463138580322266\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  5\n",
      "training loss is:  2.788590669631958\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  6\n",
      "training loss is:  2.7967889308929443\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  7\n",
      "training loss is:  3.244157075881958\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  8\n",
      "training loss is:  2.9584572315216064\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  9\n",
      "training loss is:  3.704453945159912\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  10\n",
      "training loss is:  2.574146032333374\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  11\n",
      "training loss is:  3.1211249828338623\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  12\n",
      "training loss is:  3.6234307289123535\n",
      "\n",
      "\n",
      "validation loss is 3.0005130052566527\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  13\n",
      "training loss is:  2.992769718170166\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  14\n",
      "training loss is:  3.3747754096984863\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  15\n",
      "training loss is:  3.1170413494110107\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  16\n",
      "training loss is:  3.114581346511841\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  17\n",
      "training loss is:  4.046050548553467\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  18\n",
      "training loss is:  2.8696062564849854\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  19\n",
      "training loss is:  3.4104433059692383\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  20\n",
      "training loss is:  2.690789222717285\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  21\n",
      "training loss is:  2.804403305053711\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  0\n",
      "training loss is:  3.2881219387054443\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  1\n",
      "training loss is:  3.406778573989868\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  2\n",
      "training loss is:  2.9443907737731934\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  3\n",
      "training loss is:  3.1311278343200684\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  4\n",
      "training loss is:  3.182495355606079\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  5\n",
      "training loss is:  2.662381172180176\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  6\n",
      "training loss is:  2.9188199043273926\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  7\n",
      "training loss is:  2.879530429840088\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  8\n",
      "training loss is:  3.3080289363861084\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  9\n",
      "training loss is:  3.1959316730499268\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  10\n",
      "training loss is:  5.714634895324707\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  11\n",
      "training loss is:  3.8901681900024414\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  12\n",
      "training loss is:  3.5287222862243652\n",
      "\n",
      "\n",
      "validation loss is 2.9941005229949953\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  13\n",
      "training loss is:  2.585477590560913\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  14\n",
      "training loss is:  3.0197129249572754\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  15\n",
      "training loss is:  2.8754842281341553\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  16\n",
      "training loss is:  2.0650699138641357\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  17\n",
      "training loss is:  2.6892309188842773\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  18\n",
      "training loss is:  2.9219372272491455\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  19\n",
      "training loss is:  3.087585210800171\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  20\n",
      "training loss is:  3.539841651916504\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  21\n",
      "training loss is:  3.1415016651153564\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  0\n",
      "training loss is:  3.007357120513916\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  1\n",
      "training loss is:  2.6864235401153564\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  2\n",
      "training loss is:  3.0571701526641846\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  3\n",
      "training loss is:  3.022258996963501\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  4\n",
      "training loss is:  3.1985232830047607\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  5\n",
      "training loss is:  5.777280330657959\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  6\n",
      "training loss is:  3.4464995861053467\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  7\n",
      "training loss is:  2.9607532024383545\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  8\n",
      "training loss is:  3.0714516639709473\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  9\n",
      "training loss is:  2.8619773387908936\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  10\n",
      "training loss is:  3.645171880722046\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  11\n",
      "training loss is:  2.6679527759552\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  12\n",
      "training loss is:  2.84824800491333\n",
      "\n",
      "\n",
      "validation loss is 2.995008659362793\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  13\n",
      "training loss is:  3.337521553039551\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  14\n",
      "training loss is:  3.109295606613159\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  15\n",
      "training loss is:  3.4654955863952637\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  16\n",
      "training loss is:  2.9845428466796875\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  17\n",
      "training loss is:  2.62087345123291\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  18\n",
      "training loss is:  3.00791072845459\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  19\n",
      "training loss is:  2.778597116470337\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  20\n",
      "training loss is:  3.475550651550293\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  21\n",
      "training loss is:  2.570185661315918\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  0\n",
      "training loss is:  2.788527488708496\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  1\n",
      "training loss is:  2.9122705459594727\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  2\n",
      "training loss is:  2.9024932384490967\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  3\n",
      "training loss is:  3.0047173500061035\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  4\n",
      "training loss is:  2.9448423385620117\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  5\n",
      "training loss is:  3.1089816093444824\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  6\n",
      "training loss is:  3.5763025283813477\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  7\n",
      "training loss is:  5.233734607696533\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  8\n",
      "training loss is:  3.187227725982666\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  9\n",
      "training loss is:  2.820582389831543\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  10\n",
      "training loss is:  3.6678454875946045\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  11\n",
      "training loss is:  4.513869762420654\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  12\n",
      "training loss is:  2.6916723251342773\n",
      "\n",
      "\n",
      "validation loss is 2.9941575288772584\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  13\n",
      "training loss is:  3.0683302879333496\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  14\n",
      "training loss is:  2.680215358734131\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  15\n",
      "training loss is:  3.313610792160034\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  16\n",
      "training loss is:  2.8052661418914795\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  17\n",
      "training loss is:  2.831261396408081\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  18\n",
      "training loss is:  2.5635318756103516\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  19\n",
      "training loss is:  3.5126302242279053\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  20\n",
      "training loss is:  2.3926358222961426\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  21\n",
      "training loss is:  4.0117268562316895\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  0\n",
      "training loss is:  3.054605007171631\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  1\n",
      "training loss is:  3.1135072708129883\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  2\n",
      "training loss is:  2.7520275115966797\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  3\n",
      "training loss is:  3.4539794921875\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  4\n",
      "training loss is:  3.364539861679077\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  5\n",
      "training loss is:  3.385047674179077\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  6\n",
      "training loss is:  3.052276611328125\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  7\n",
      "training loss is:  3.383974313735962\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  8\n",
      "training loss is:  2.4434621334075928\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  9\n",
      "training loss is:  3.116933822631836\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  10\n",
      "training loss is:  3.3385794162750244\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  11\n",
      "training loss is:  2.781492233276367\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  12\n",
      "training loss is:  2.632722854614258\n",
      "\n",
      "\n",
      "validation loss is 3.0027832269668577\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  13\n",
      "training loss is:  2.8947207927703857\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  14\n",
      "training loss is:  3.0527279376983643\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  15\n",
      "training loss is:  3.3777217864990234\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  16\n",
      "training loss is:  3.7609894275665283\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  17\n",
      "training loss is:  2.9378316402435303\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  18\n",
      "training loss is:  2.978505849838257\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  19\n",
      "training loss is:  3.202780246734619\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  20\n",
      "training loss is:  4.942884922027588\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  21\n",
      "training loss is:  2.5593795776367188\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  0\n",
      "training loss is:  3.032416343688965\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  1\n",
      "training loss is:  2.511003017425537\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  2\n",
      "training loss is:  2.763347864151001\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  3\n",
      "training loss is:  3.566542387008667\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  4\n",
      "training loss is:  2.8546130657196045\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  5\n",
      "training loss is:  3.268051862716675\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  6\n",
      "training loss is:  3.648341417312622\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  7\n",
      "training loss is:  3.0552093982696533\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  8\n",
      "training loss is:  2.4952874183654785\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  9\n",
      "training loss is:  3.379171848297119\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  10\n",
      "training loss is:  3.0766332149505615\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  11\n",
      "training loss is:  2.862426519393921\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  12\n",
      "training loss is:  3.8590197563171387\n",
      "\n",
      "\n",
      "validation loss is 2.994086241722107\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  13\n",
      "training loss is:  5.663145065307617\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  14\n",
      "training loss is:  2.518625497817993\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  15\n",
      "training loss is:  3.24495792388916\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  16\n",
      "training loss is:  2.9056050777435303\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  17\n",
      "training loss is:  2.676901340484619\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  18\n",
      "training loss is:  3.4931750297546387\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  19\n",
      "training loss is:  3.2259714603424072\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  20\n",
      "training loss is:  2.8709917068481445\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  21\n",
      "training loss is:  2.734025478363037\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  0\n",
      "training loss is:  3.1623756885528564\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  1\n",
      "training loss is:  3.4280831813812256\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  2\n",
      "training loss is:  4.437339782714844\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  3\n",
      "training loss is:  3.0635244846343994\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  4\n",
      "training loss is:  3.3012735843658447\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  5\n",
      "training loss is:  2.8247265815734863\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  6\n",
      "training loss is:  3.389972448348999\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  7\n",
      "training loss is:  2.404900550842285\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  8\n",
      "training loss is:  3.6657941341400146\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  9\n",
      "training loss is:  2.61971378326416\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  10\n",
      "training loss is:  2.701305866241455\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  11\n",
      "training loss is:  2.355853796005249\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  12\n",
      "training loss is:  3.2541909217834473\n",
      "\n",
      "\n",
      "validation loss is 2.99709951877594\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  13\n",
      "training loss is:  3.1500210762023926\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  14\n",
      "training loss is:  3.559730291366577\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  15\n",
      "training loss is:  2.3574976921081543\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  16\n",
      "training loss is:  3.2480247020721436\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  17\n",
      "training loss is:  4.879851818084717\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  18\n",
      "training loss is:  2.9884836673736572\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  19\n",
      "training loss is:  3.182180881500244\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  20\n",
      "training loss is:  2.953486442565918\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  21\n",
      "training loss is:  2.8714921474456787\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  0\n",
      "training loss is:  3.1683664321899414\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  1\n",
      "training loss is:  2.704221725463867\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  2\n",
      "training loss is:  3.3038742542266846\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  3\n",
      "training loss is:  2.5757389068603516\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  4\n",
      "training loss is:  2.8957431316375732\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  5\n",
      "training loss is:  3.737412452697754\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  6\n",
      "training loss is:  3.1238718032836914\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  7\n",
      "training loss is:  3.027043104171753\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  8\n",
      "training loss is:  3.4597995281219482\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  9\n",
      "training loss is:  3.190899133682251\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  10\n",
      "training loss is:  3.574282169342041\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  11\n",
      "training loss is:  2.9425582885742188\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  12\n",
      "training loss is:  2.9728598594665527\n",
      "\n",
      "\n",
      "validation loss is 2.9961519956588747\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  13\n",
      "training loss is:  3.0979275703430176\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  14\n",
      "training loss is:  3.194962501525879\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  15\n",
      "training loss is:  2.974656343460083\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  16\n",
      "training loss is:  3.390082359313965\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  17\n",
      "training loss is:  5.592081069946289\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  18\n",
      "training loss is:  2.8113467693328857\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  19\n",
      "training loss is:  2.848886728286743\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  20\n",
      "training loss is:  2.3802273273468018\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  21\n",
      "training loss is:  2.7129979133605957\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  0\n",
      "training loss is:  4.806411266326904\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  1\n",
      "training loss is:  3.1059296131134033\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  2\n",
      "training loss is:  3.639763832092285\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  3\n",
      "training loss is:  3.416231632232666\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  4\n",
      "training loss is:  2.8477096557617188\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  5\n",
      "training loss is:  2.995835542678833\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  6\n",
      "training loss is:  2.972823143005371\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  7\n",
      "training loss is:  2.9811956882476807\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  8\n",
      "training loss is:  3.100048542022705\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  9\n",
      "training loss is:  2.5186045169830322\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  10\n",
      "training loss is:  2.774146318435669\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  11\n",
      "training loss is:  2.7477598190307617\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  12\n",
      "training loss is:  2.8831098079681396\n",
      "\n",
      "\n",
      "validation loss is 2.9951467990875242\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  13\n",
      "training loss is:  2.791630744934082\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  14\n",
      "training loss is:  3.235635280609131\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  15\n",
      "training loss is:  3.120201826095581\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  16\n",
      "training loss is:  2.3803255558013916\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  17\n",
      "training loss is:  3.06302547454834\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  18\n",
      "training loss is:  5.095930099487305\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  19\n",
      "training loss is:  3.3149173259735107\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  20\n",
      "training loss is:  3.177258253097534\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  21\n",
      "training loss is:  2.7147114276885986\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  0\n",
      "training loss is:  2.499274492263794\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  1\n",
      "training loss is:  4.930479049682617\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  2\n",
      "training loss is:  3.17620849609375\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  3\n",
      "training loss is:  4.118002891540527\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  4\n",
      "training loss is:  3.1157636642456055\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  5\n",
      "training loss is:  3.4657998085021973\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  6\n",
      "training loss is:  3.4794461727142334\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  7\n",
      "training loss is:  2.8231513500213623\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  8\n",
      "training loss is:  3.1434664726257324\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  9\n",
      "training loss is:  2.917288303375244\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  10\n",
      "training loss is:  3.2341039180755615\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  11\n",
      "training loss is:  3.189797878265381\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  12\n",
      "training loss is:  2.3875997066497803\n",
      "\n",
      "\n",
      "validation loss is 2.994746541976929\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  13\n",
      "training loss is:  3.0191309452056885\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  14\n",
      "training loss is:  3.148434638977051\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  15\n",
      "training loss is:  3.1203064918518066\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  16\n",
      "training loss is:  4.174142360687256\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  17\n",
      "training loss is:  3.252476692199707\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  18\n",
      "training loss is:  2.629103899002075\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  19\n",
      "training loss is:  2.185840606689453\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  20\n",
      "training loss is:  2.837920904159546\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  21\n",
      "training loss is:  3.1519174575805664\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  0\n",
      "training loss is:  3.0216493606567383\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  1\n",
      "training loss is:  3.120662212371826\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  2\n",
      "training loss is:  2.878833055496216\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  3\n",
      "training loss is:  2.8439462184906006\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  4\n",
      "training loss is:  3.3316962718963623\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  5\n",
      "training loss is:  2.8021650314331055\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  6\n",
      "training loss is:  2.6892757415771484\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  7\n",
      "training loss is:  3.0945475101470947\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  8\n",
      "training loss is:  2.4824135303497314\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  9\n",
      "training loss is:  2.7462878227233887\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  10\n",
      "training loss is:  2.980055809020996\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  11\n",
      "training loss is:  3.544135093688965\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  12\n",
      "training loss is:  3.0353786945343018\n",
      "\n",
      "\n",
      "validation loss is 2.9941943645477296\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  13\n",
      "training loss is:  3.0488481521606445\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  14\n",
      "training loss is:  5.601858615875244\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  15\n",
      "training loss is:  2.3718488216400146\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  16\n",
      "training loss is:  4.595953941345215\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  17\n",
      "training loss is:  3.0510146617889404\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  18\n",
      "training loss is:  4.450993537902832\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  19\n",
      "training loss is:  2.171052932739258\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  20\n",
      "training loss is:  3.222954750061035\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  21\n",
      "training loss is:  2.390324115753174\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  0\n",
      "training loss is:  3.6286966800689697\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  1\n",
      "training loss is:  2.913287878036499\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  2\n",
      "training loss is:  3.203167676925659\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  3\n",
      "training loss is:  2.8524787425994873\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  4\n",
      "training loss is:  2.950047254562378\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  5\n",
      "training loss is:  3.6839187145233154\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  6\n",
      "training loss is:  2.8128910064697266\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  7\n",
      "training loss is:  2.2528817653656006\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  8\n",
      "training loss is:  3.4694864749908447\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  9\n",
      "training loss is:  3.592254400253296\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  10\n",
      "training loss is:  2.800438165664673\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  11\n",
      "training loss is:  2.9275577068328857\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  12\n",
      "training loss is:  3.6646718978881836\n",
      "\n",
      "\n",
      "validation loss is 2.994345760345459\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  13\n",
      "training loss is:  3.315169334411621\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  14\n",
      "training loss is:  3.138308048248291\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  15\n",
      "training loss is:  2.9027132987976074\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  16\n",
      "training loss is:  6.1189422607421875\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  17\n",
      "training loss is:  2.655104398727417\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  18\n",
      "training loss is:  2.192359685897827\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  19\n",
      "training loss is:  3.025287389755249\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  20\n",
      "training loss is:  2.6079952716827393\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  21\n",
      "training loss is:  3.422787666320801\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  0\n",
      "training loss is:  2.9890778064727783\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  1\n",
      "training loss is:  3.3328189849853516\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  2\n",
      "training loss is:  3.0647132396698\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  3\n",
      "training loss is:  2.7549493312835693\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  4\n",
      "training loss is:  2.978597402572632\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  5\n",
      "training loss is:  2.685555934906006\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  6\n",
      "training loss is:  3.3029417991638184\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  7\n",
      "training loss is:  3.332932233810425\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  8\n",
      "training loss is:  2.658430576324463\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  9\n",
      "training loss is:  2.726877212524414\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  10\n",
      "training loss is:  2.8305110931396484\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  11\n",
      "training loss is:  2.7171859741210938\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  12\n",
      "training loss is:  3.2655997276306152\n",
      "\n",
      "\n",
      "validation loss is 2.9947226524353026\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  13\n",
      "training loss is:  4.192949295043945\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  14\n",
      "training loss is:  5.831173896789551\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  15\n",
      "training loss is:  2.521531105041504\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  16\n",
      "training loss is:  2.5542819499969482\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  17\n",
      "training loss is:  3.3313629627227783\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  18\n",
      "training loss is:  3.052748680114746\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  19\n",
      "training loss is:  3.043625593185425\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  20\n",
      "training loss is:  3.5305259227752686\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  21\n",
      "training loss is:  3.394782781600952\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  0\n",
      "training loss is:  3.4321787357330322\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  1\n",
      "training loss is:  3.2346510887145996\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  2\n",
      "training loss is:  2.5284786224365234\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  3\n",
      "training loss is:  2.9216811656951904\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  4\n",
      "training loss is:  3.945580244064331\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  5\n",
      "training loss is:  3.146627902984619\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  6\n",
      "training loss is:  4.0035624504089355\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  7\n",
      "training loss is:  2.7920913696289062\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  8\n",
      "training loss is:  3.1341171264648438\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  9\n",
      "training loss is:  3.1589243412017822\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  10\n",
      "training loss is:  3.2284069061279297\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  11\n",
      "training loss is:  3.0257773399353027\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  12\n",
      "training loss is:  2.610109329223633\n",
      "\n",
      "\n",
      "validation loss is 2.9960736989974976\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  13\n",
      "training loss is:  2.6696486473083496\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  14\n",
      "training loss is:  2.867594003677368\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  15\n",
      "training loss is:  2.745027780532837\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  16\n",
      "training loss is:  2.9237678050994873\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  17\n",
      "training loss is:  5.801395893096924\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  18\n",
      "training loss is:  2.762030839920044\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  19\n",
      "training loss is:  2.618142604827881\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  20\n",
      "training loss is:  3.341691017150879\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  21\n",
      "training loss is:  2.8674964904785156\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  0\n",
      "training loss is:  3.049290180206299\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  1\n",
      "training loss is:  3.295442819595337\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  2\n",
      "training loss is:  2.7978134155273438\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  3\n",
      "training loss is:  4.0071001052856445\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  4\n",
      "training loss is:  2.5529232025146484\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  5\n",
      "training loss is:  3.205474853515625\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  6\n",
      "training loss is:  3.2769811153411865\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  7\n",
      "training loss is:  2.642124891281128\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  8\n",
      "training loss is:  2.9481568336486816\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  9\n",
      "training loss is:  2.7720391750335693\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  10\n",
      "training loss is:  5.110368728637695\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  11\n",
      "training loss is:  3.820425510406494\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  12\n",
      "training loss is:  2.8803510665893555\n",
      "\n",
      "\n",
      "validation loss is 2.9953834772109986\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  13\n",
      "training loss is:  3.588134527206421\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  14\n",
      "training loss is:  2.97636079788208\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  15\n",
      "training loss is:  3.576702833175659\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  16\n",
      "training loss is:  2.735363721847534\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  17\n",
      "training loss is:  2.8737752437591553\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  18\n",
      "training loss is:  2.684725761413574\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  19\n",
      "training loss is:  3.07072114944458\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  20\n",
      "training loss is:  3.0249710083007812\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  21\n",
      "training loss is:  2.8907461166381836\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  0\n",
      "training loss is:  2.3660097122192383\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  1\n",
      "training loss is:  2.6879804134368896\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  2\n",
      "training loss is:  2.8627076148986816\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  3\n",
      "training loss is:  2.850484848022461\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  4\n",
      "training loss is:  4.420019149780273\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  5\n",
      "training loss is:  5.4085564613342285\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  6\n",
      "training loss is:  3.9345788955688477\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  7\n",
      "training loss is:  2.5966858863830566\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  8\n",
      "training loss is:  3.5213708877563477\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  9\n",
      "training loss is:  2.669644355773926\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  10\n",
      "training loss is:  3.2088141441345215\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  11\n",
      "training loss is:  2.9514007568359375\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  12\n",
      "training loss is:  2.4263319969177246\n",
      "\n",
      "\n",
      "validation loss is 2.9939548492431642\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  13\n",
      "training loss is:  2.7333645820617676\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  14\n",
      "training loss is:  2.8826987743377686\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  15\n",
      "training loss is:  3.2791876792907715\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  16\n",
      "training loss is:  2.670733690261841\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  17\n",
      "training loss is:  4.059373378753662\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  18\n",
      "training loss is:  3.9952259063720703\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  19\n",
      "training loss is:  2.8149304389953613\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  20\n",
      "training loss is:  2.9892165660858154\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  21\n",
      "training loss is:  1.6540412902832031\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  0\n",
      "training loss is:  2.8098928928375244\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  1\n",
      "training loss is:  2.5198960304260254\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  2\n",
      "training loss is:  3.3729488849639893\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  3\n",
      "training loss is:  2.860898971557617\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  4\n",
      "training loss is:  3.4501874446868896\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  5\n",
      "training loss is:  3.0868749618530273\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  6\n",
      "training loss is:  2.7205727100372314\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  7\n",
      "training loss is:  3.396726131439209\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  8\n",
      "training loss is:  3.3790292739868164\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  9\n",
      "training loss is:  3.185153007507324\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  10\n",
      "training loss is:  3.3108108043670654\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  11\n",
      "training loss is:  3.089874505996704\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  12\n",
      "training loss is:  3.1167991161346436\n",
      "\n",
      "\n",
      "validation loss is 2.9940348148345945\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  13\n",
      "training loss is:  3.0149264335632324\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  14\n",
      "training loss is:  2.9859704971313477\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  15\n",
      "training loss is:  2.5435380935668945\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  16\n",
      "training loss is:  4.089686870574951\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  17\n",
      "training loss is:  3.0508229732513428\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  18\n",
      "training loss is:  4.823218822479248\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  19\n",
      "training loss is:  2.858795166015625\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  20\n",
      "training loss is:  3.3835158348083496\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  21\n",
      "training loss is:  2.400697946548462\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training:\n",
    "#train too many steps will leads to overfit\n",
    "#how to determine what is the stopping criterion of the training:\n",
    "#---- one solution is \n",
    "validation_loss = []\n",
    "for epoch in range(100):\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        print('Epoch:', epoch, '| Step: ', step)\n",
    "        \n",
    "        b_x = Variable(batch_x.float())\n",
    "        b_y = Variable(batch_y.float())\n",
    "        \n",
    "        prediction = net(b_x)\n",
    "        train_loss = loss_func(prediction, b_y)\n",
    "        print('training loss is: ',train_loss.data[0])\n",
    "        print('\\n')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #validation loss:\n",
    "        if (step+1) % 13 ==0:\n",
    "            sum_validation_loss = 0\n",
    "            count = 0\n",
    "            for s, (batch_x_validate, batch_y_validate) in enumerate(validate_loader):\n",
    "                batch_x_validate = Variable(batch_x_validate.float())\n",
    "                batch_y_validate = Variable(batch_y_validate.float())\n",
    "                validation_output = net(batch_x_validate)\n",
    "                #print(loss_func(validation_output, batch_y_validate).data[0])\n",
    "                sum_validation_loss += loss_func(validation_output, batch_y_validate).data[0]\n",
    "                count+=1\n",
    "            validation_loss.append(sum_validation_loss/count)\n",
    "            print(\"validation loss is {}\".format(validation_loss[-1]))\n",
    "            print('\\n')\n",
    "            print('\\n')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find where the validation error is the at minimum:\n",
    "np.argmin(validation_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#so the optimal training steps are 98* 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step:  0\n",
      "training loss is:  3.1945407390594482\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  1\n",
      "training loss is:  2.735752820968628\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  2\n",
      "training loss is:  2.872418165206909\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  3\n",
      "training loss is:  2.879749059677124\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  4\n",
      "training loss is:  2.6705987453460693\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  5\n",
      "training loss is:  2.557917356491089\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  6\n",
      "training loss is:  3.2785110473632812\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  7\n",
      "training loss is:  3.6813132762908936\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  8\n",
      "training loss is:  2.8625879287719727\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  9\n",
      "training loss is:  2.767526865005493\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  10\n",
      "training loss is:  2.572329521179199\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  11\n",
      "training loss is:  4.862743377685547\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  12\n",
      "training loss is:  3.2131075859069824\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  13\n",
      "training loss is:  3.3542590141296387\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  14\n",
      "training loss is:  2.701188087463379\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  15\n",
      "training loss is:  2.9821484088897705\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  16\n",
      "training loss is:  3.1816132068634033\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  17\n",
      "training loss is:  2.3170783519744873\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  18\n",
      "training loss is:  3.2670929431915283\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  19\n",
      "training loss is:  3.597860097885132\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  20\n",
      "training loss is:  4.882688999176025\n",
      "\n",
      "\n",
      "Epoch: 0 | Step:  21\n",
      "training loss is:  2.1126205921173096\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  0\n",
      "training loss is:  2.7929046154022217\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  1\n",
      "training loss is:  6.0512003898620605\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  2\n",
      "training loss is:  4.197967052459717\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  3\n",
      "training loss is:  3.144564628601074\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  4\n",
      "training loss is:  2.92368221282959\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  5\n",
      "training loss is:  2.5920848846435547\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  6\n",
      "training loss is:  3.302264451980591\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  7\n",
      "training loss is:  2.9328274726867676\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  8\n",
      "training loss is:  3.771369695663452\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  9\n",
      "training loss is:  2.5479772090911865\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  10\n",
      "training loss is:  3.3287618160247803\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  11\n",
      "training loss is:  2.9227170944213867\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  12\n",
      "training loss is:  2.933673620223999\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  13\n",
      "training loss is:  3.2934842109680176\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  14\n",
      "training loss is:  3.116854667663574\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  15\n",
      "training loss is:  2.823864221572876\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  16\n",
      "training loss is:  3.243155002593994\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  17\n",
      "training loss is:  2.6007742881774902\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  18\n",
      "training loss is:  2.0604426860809326\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  19\n",
      "training loss is:  3.6624207496643066\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  20\n",
      "training loss is:  2.2907309532165527\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  21\n",
      "training loss is:  2.4532134532928467\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  0\n",
      "training loss is:  3.1664371490478516\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  1\n",
      "training loss is:  2.776974678039551\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  2\n",
      "training loss is:  3.3448898792266846\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  3\n",
      "training loss is:  2.9404969215393066\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  4\n",
      "training loss is:  2.728231191635132\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  5\n",
      "training loss is:  2.764176845550537\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  6\n",
      "training loss is:  2.597476005554199\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  7\n",
      "training loss is:  2.4794280529022217\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  8\n",
      "training loss is:  3.539727210998535\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  9\n",
      "training loss is:  3.4573352336883545\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  10\n",
      "training loss is:  3.4395315647125244\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  11\n",
      "training loss is:  5.092099189758301\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  12\n",
      "training loss is:  2.545330762863159\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  13\n",
      "training loss is:  2.6308910846710205\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  14\n",
      "training loss is:  2.7340099811553955\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  15\n",
      "training loss is:  3.4515721797943115\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  16\n",
      "training loss is:  3.8509507179260254\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  17\n",
      "training loss is:  3.1983535289764404\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  18\n",
      "training loss is:  3.2908825874328613\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  19\n",
      "training loss is:  3.288142442703247\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  20\n",
      "training loss is:  2.6981563568115234\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  21\n",
      "training loss is:  3.474807024002075\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  0\n",
      "training loss is:  2.331953525543213\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  1\n",
      "training loss is:  2.649179458618164\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  2\n",
      "training loss is:  2.9935712814331055\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  3\n",
      "training loss is:  5.552723407745361\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  4\n",
      "training loss is:  4.10819149017334\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  5\n",
      "training loss is:  2.7628774642944336\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  6\n",
      "training loss is:  2.470367670059204\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  7\n",
      "training loss is:  3.068056106567383\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  8\n",
      "training loss is:  3.6825268268585205\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  9\n",
      "training loss is:  3.0645647048950195\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  10\n",
      "training loss is:  3.390531539916992\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  11\n",
      "training loss is:  3.3410985469818115\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  12\n",
      "training loss is:  3.2731170654296875\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  13\n",
      "training loss is:  2.642698287963867\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  14\n",
      "training loss is:  3.3529739379882812\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  15\n",
      "training loss is:  3.472625732421875\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  16\n",
      "training loss is:  2.394779920578003\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  17\n",
      "training loss is:  2.6137051582336426\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  18\n",
      "training loss is:  3.1921048164367676\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  19\n",
      "training loss is:  3.1529858112335205\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  20\n",
      "training loss is:  2.8636279106140137\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  21\n",
      "training loss is:  3.0170249938964844\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  0\n",
      "training loss is:  3.290217638015747\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  1\n",
      "training loss is:  3.1897077560424805\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  2\n",
      "training loss is:  2.7996201515197754\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  3\n",
      "training loss is:  4.292206287384033\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  4\n",
      "training loss is:  3.6326303482055664\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  5\n",
      "training loss is:  2.907806158065796\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  6\n",
      "training loss is:  2.9431939125061035\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  7\n",
      "training loss is:  3.2730462551116943\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  8\n",
      "training loss is:  2.4890828132629395\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  9\n",
      "training loss is:  2.801933526992798\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  10\n",
      "training loss is:  3.078442096710205\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  11\n",
      "training loss is:  2.906282663345337\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  12\n",
      "training loss is:  3.104398250579834\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  13\n",
      "training loss is:  2.4798386096954346\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  14\n",
      "training loss is:  2.7516696453094482\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  15\n",
      "training loss is:  5.97486686706543\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  16\n",
      "training loss is:  4.003751754760742\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  17\n",
      "training loss is:  1.866660714149475\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  18\n",
      "training loss is:  2.8210294246673584\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  19\n",
      "training loss is:  3.1176867485046387\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  20\n",
      "training loss is:  2.797049045562744\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  21\n",
      "training loss is:  2.460253953933716\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  0\n",
      "training loss is:  2.8432366847991943\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  1\n",
      "training loss is:  4.889868259429932\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  2\n",
      "training loss is:  3.8362598419189453\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  3\n",
      "training loss is:  3.3900673389434814\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  4\n",
      "training loss is:  2.8578555583953857\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  5\n",
      "training loss is:  3.5476043224334717\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  6\n",
      "training loss is:  3.149822950363159\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  7\n",
      "training loss is:  4.108362197875977\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  8\n",
      "training loss is:  2.8509061336517334\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  9\n",
      "training loss is:  2.7461049556732178\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  10\n",
      "training loss is:  2.9895055294036865\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  11\n",
      "training loss is:  3.0274200439453125\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  12\n",
      "training loss is:  3.105905532836914\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  13\n",
      "training loss is:  3.0559091567993164\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  14\n",
      "training loss is:  2.5249550342559814\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  15\n",
      "training loss is:  2.9081079959869385\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  16\n",
      "training loss is:  3.7160000801086426\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  17\n",
      "training loss is:  2.7348783016204834\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  18\n",
      "training loss is:  2.897838830947876\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  19\n",
      "training loss is:  2.5884571075439453\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  20\n",
      "training loss is:  2.8449511528015137\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  21\n",
      "training loss is:  2.3243823051452637\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  0\n",
      "training loss is:  2.904646873474121\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  1\n",
      "training loss is:  2.722015142440796\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  2\n",
      "training loss is:  2.6591949462890625\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  3\n",
      "training loss is:  2.5832159519195557\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  4\n",
      "training loss is:  4.917482376098633\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  5\n",
      "training loss is:  3.094618797302246\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  6\n",
      "training loss is:  3.430398941040039\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  7\n",
      "training loss is:  3.3785901069641113\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  8\n",
      "training loss is:  3.5758795738220215\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  9\n",
      "training loss is:  2.5126349925994873\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  10\n",
      "training loss is:  2.698817729949951\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  11\n",
      "training loss is:  2.9299585819244385\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  12\n",
      "training loss is:  2.7660927772521973\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  13\n",
      "training loss is:  3.105044364929199\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  14\n",
      "training loss is:  4.07003927230835\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  15\n",
      "training loss is:  2.960383415222168\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  16\n",
      "training loss is:  2.3106234073638916\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  17\n",
      "training loss is:  3.8027427196502686\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  18\n",
      "training loss is:  3.0958352088928223\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  19\n",
      "training loss is:  3.285867214202881\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  20\n",
      "training loss is:  3.4237821102142334\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  21\n",
      "training loss is:  2.828974962234497\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  0\n",
      "training loss is:  3.7619810104370117\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  1\n",
      "training loss is:  3.2023916244506836\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  2\n",
      "training loss is:  2.5111238956451416\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  3\n",
      "training loss is:  3.29435658454895\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  4\n",
      "training loss is:  2.505004644393921\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  5\n",
      "training loss is:  3.1989388465881348\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  6\n",
      "training loss is:  2.52624249458313\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  7\n",
      "training loss is:  2.895017623901367\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  8\n",
      "training loss is:  3.1974904537200928\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  9\n",
      "training loss is:  2.613483190536499\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  10\n",
      "training loss is:  5.076818466186523\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  11\n",
      "training loss is:  2.857489585876465\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  12\n",
      "training loss is:  3.0262832641601562\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  13\n",
      "training loss is:  3.080977201461792\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  14\n",
      "training loss is:  3.9534878730773926\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  15\n",
      "training loss is:  2.7264978885650635\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  16\n",
      "training loss is:  2.702158212661743\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  17\n",
      "training loss is:  2.9247963428497314\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  18\n",
      "training loss is:  3.0005338191986084\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  19\n",
      "training loss is:  3.227670907974243\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  20\n",
      "training loss is:  3.773275375366211\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  21\n",
      "training loss is:  3.08154034614563\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  0\n",
      "training loss is:  2.835758924484253\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  1\n",
      "training loss is:  3.399622917175293\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  2\n",
      "training loss is:  2.7677438259124756\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  3\n",
      "training loss is:  2.3110511302948\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  4\n",
      "training loss is:  2.8487937450408936\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  5\n",
      "training loss is:  3.6682913303375244\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  6\n",
      "training loss is:  2.9271435737609863\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  7\n",
      "training loss is:  3.0431246757507324\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  8\n",
      "training loss is:  3.179323196411133\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  9\n",
      "training loss is:  2.878674030303955\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  10\n",
      "training loss is:  6.348002910614014\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  11\n",
      "training loss is:  3.2663402557373047\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  12\n",
      "training loss is:  2.480281352996826\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  13\n",
      "training loss is:  3.905331611633301\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  14\n",
      "training loss is:  2.771829128265381\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  15\n",
      "training loss is:  3.2861130237579346\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  16\n",
      "training loss is:  2.5288705825805664\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  17\n",
      "training loss is:  2.955704689025879\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  18\n",
      "training loss is:  2.7484374046325684\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  19\n",
      "training loss is:  3.9651694297790527\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  20\n",
      "training loss is:  2.1526825428009033\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  21\n",
      "training loss is:  3.063955068588257\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  0\n",
      "training loss is:  3.4860711097717285\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  1\n",
      "training loss is:  3.912533760070801\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  2\n",
      "training loss is:  4.61280632019043\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  3\n",
      "training loss is:  2.350492000579834\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  4\n",
      "training loss is:  2.5598955154418945\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  5\n",
      "training loss is:  2.7010810375213623\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  6\n",
      "training loss is:  3.2598135471343994\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  7\n",
      "training loss is:  3.1207962036132812\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  8\n",
      "training loss is:  3.166086196899414\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  9\n",
      "training loss is:  3.0365984439849854\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  10\n",
      "training loss is:  3.806896448135376\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  11\n",
      "training loss is:  2.978064775466919\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  12\n",
      "training loss is:  3.0145998001098633\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  13\n",
      "training loss is:  3.458544969558716\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  14\n",
      "training loss is:  2.799302101135254\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  15\n",
      "training loss is:  3.4773714542388916\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  16\n",
      "training loss is:  2.552908420562744\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  17\n",
      "training loss is:  2.837808609008789\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  18\n",
      "training loss is:  2.4686989784240723\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  19\n",
      "training loss is:  3.1138882637023926\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  20\n",
      "training loss is:  3.6830127239227295\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  21\n",
      "training loss is:  2.3592398166656494\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  0\n",
      "training loss is:  2.9440765380859375\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  1\n",
      "training loss is:  2.864820957183838\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  2\n",
      "training loss is:  5.505990028381348\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  3\n",
      "training loss is:  3.0577781200408936\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  4\n",
      "training loss is:  2.925579309463501\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  5\n",
      "training loss is:  3.0388903617858887\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  6\n",
      "training loss is:  2.4604640007019043\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  7\n",
      "training loss is:  2.715017795562744\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  8\n",
      "training loss is:  2.941554307937622\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  9\n",
      "training loss is:  3.1693646907806396\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  10\n",
      "training loss is:  3.9653611183166504\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  11\n",
      "training loss is:  2.675311326980591\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  12\n",
      "training loss is:  3.8302953243255615\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  13\n",
      "training loss is:  2.972965955734253\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  14\n",
      "training loss is:  3.0529489517211914\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  15\n",
      "training loss is:  2.6441800594329834\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  16\n",
      "training loss is:  2.3136682510375977\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  17\n",
      "training loss is:  3.55433988571167\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  18\n",
      "training loss is:  3.1837899684906006\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  19\n",
      "training loss is:  2.544642686843872\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  20\n",
      "training loss is:  3.8693366050720215\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  21\n",
      "training loss is:  3.4297549724578857\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  0\n",
      "training loss is:  3.3630530834198\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  1\n",
      "training loss is:  2.940426826477051\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  2\n",
      "training loss is:  2.5071511268615723\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  3\n",
      "training loss is:  2.6793322563171387\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  4\n",
      "training loss is:  3.567650556564331\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  5\n",
      "training loss is:  3.3091444969177246\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  6\n",
      "training loss is:  3.3850908279418945\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  7\n",
      "training loss is:  2.5764734745025635\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  8\n",
      "training loss is:  5.2204790115356445\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  9\n",
      "training loss is:  2.534144163131714\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  10\n",
      "training loss is:  3.217297077178955\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  11\n",
      "training loss is:  3.432669162750244\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  12\n",
      "training loss is:  3.3350002765655518\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  13\n",
      "training loss is:  4.078554630279541\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  14\n",
      "training loss is:  2.9374125003814697\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  15\n",
      "training loss is:  3.296067714691162\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  16\n",
      "training loss is:  2.955098867416382\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  17\n",
      "training loss is:  3.0610122680664062\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  18\n",
      "training loss is:  2.522906541824341\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  19\n",
      "training loss is:  2.5417001247406006\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  20\n",
      "training loss is:  2.854363441467285\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  21\n",
      "training loss is:  2.3241991996765137\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  0\n",
      "training loss is:  3.395641565322876\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  1\n",
      "training loss is:  3.5474729537963867\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  2\n",
      "training loss is:  2.5570828914642334\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  3\n",
      "training loss is:  2.728666305541992\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  4\n",
      "training loss is:  2.9634313583374023\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  5\n",
      "training loss is:  3.0843570232391357\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  6\n",
      "training loss is:  2.5801491737365723\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  7\n",
      "training loss is:  2.9871084690093994\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  8\n",
      "training loss is:  2.961989164352417\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  9\n",
      "training loss is:  3.879375696182251\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  10\n",
      "training loss is:  3.5695276260375977\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  11\n",
      "training loss is:  4.680335521697998\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  12\n",
      "training loss is:  2.6762704849243164\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  13\n",
      "training loss is:  3.180837631225586\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  14\n",
      "training loss is:  3.61197566986084\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  15\n",
      "training loss is:  3.451510429382324\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  16\n",
      "training loss is:  3.084733009338379\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  17\n",
      "training loss is:  2.2724146842956543\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  18\n",
      "training loss is:  2.7709286212921143\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  19\n",
      "training loss is:  3.0023748874664307\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  20\n",
      "training loss is:  3.0414071083068848\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  21\n",
      "training loss is:  3.4471659660339355\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  0\n",
      "training loss is:  3.483712673187256\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  1\n",
      "training loss is:  3.3190524578094482\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  2\n",
      "training loss is:  2.7783544063568115\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  3\n",
      "training loss is:  3.5100536346435547\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  4\n",
      "training loss is:  2.9484636783599854\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  5\n",
      "training loss is:  3.193063974380493\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  6\n",
      "training loss is:  3.2858083248138428\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  7\n",
      "training loss is:  2.7830934524536133\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  8\n",
      "training loss is:  2.5723345279693604\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  9\n",
      "training loss is:  3.072510004043579\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  10\n",
      "training loss is:  2.8981926441192627\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  11\n",
      "training loss is:  3.3934433460235596\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  12\n",
      "training loss is:  2.5629401206970215\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  13\n",
      "training loss is:  2.626549005508423\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  14\n",
      "training loss is:  2.824838638305664\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  15\n",
      "training loss is:  2.656332492828369\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  16\n",
      "training loss is:  5.228426456451416\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  17\n",
      "training loss is:  3.399040937423706\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  18\n",
      "training loss is:  2.830723285675049\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  19\n",
      "training loss is:  3.8695013523101807\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  20\n",
      "training loss is:  2.9135372638702393\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  21\n",
      "training loss is:  2.874228000640869\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  0\n",
      "training loss is:  2.7713816165924072\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  1\n",
      "training loss is:  2.7787222862243652\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  2\n",
      "training loss is:  3.1937286853790283\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  3\n",
      "training loss is:  2.3181354999542236\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  4\n",
      "training loss is:  3.9020755290985107\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  5\n",
      "training loss is:  2.7532215118408203\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  6\n",
      "training loss is:  2.9777886867523193\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  7\n",
      "training loss is:  3.8728673458099365\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  8\n",
      "training loss is:  3.0991430282592773\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  9\n",
      "training loss is:  3.1024014949798584\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  10\n",
      "training loss is:  3.220367193222046\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  11\n",
      "training loss is:  2.809906244277954\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  12\n",
      "training loss is:  4.746009349822998\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  13\n",
      "training loss is:  3.7611947059631348\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  14\n",
      "training loss is:  2.9773290157318115\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  15\n",
      "training loss is:  3.0104336738586426\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  16\n",
      "training loss is:  2.8547310829162598\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  17\n",
      "training loss is:  2.167743444442749\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  18\n",
      "training loss is:  3.2031404972076416\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  19\n",
      "training loss is:  3.1053714752197266\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  20\n",
      "training loss is:  3.8068923950195312\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  21\n",
      "training loss is:  2.1911118030548096\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  0\n",
      "training loss is:  2.281571388244629\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  1\n",
      "training loss is:  3.1561341285705566\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  2\n",
      "training loss is:  5.42228889465332\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  3\n",
      "training loss is:  3.048285484313965\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  4\n",
      "training loss is:  2.576526403427124\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  5\n",
      "training loss is:  4.469752311706543\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  6\n",
      "training loss is:  2.8624486923217773\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  7\n",
      "training loss is:  3.0815067291259766\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  8\n",
      "training loss is:  3.239150047302246\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  9\n",
      "training loss is:  2.724210500717163\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  10\n",
      "training loss is:  3.2205939292907715\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  11\n",
      "training loss is:  3.0834593772888184\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  12\n",
      "training loss is:  3.103496789932251\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  13\n",
      "training loss is:  2.8404910564422607\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  14\n",
      "training loss is:  2.366255760192871\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  15\n",
      "training loss is:  3.0094354152679443\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  16\n",
      "training loss is:  2.713024616241455\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  17\n",
      "training loss is:  2.8687708377838135\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  18\n",
      "training loss is:  3.234696388244629\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  19\n",
      "training loss is:  3.3328309059143066\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  20\n",
      "training loss is:  3.7409961223602295\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  21\n",
      "training loss is:  2.443887948989868\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  0\n",
      "training loss is:  2.396190881729126\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  1\n",
      "training loss is:  2.409435987472534\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  2\n",
      "training loss is:  3.048117160797119\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  3\n",
      "training loss is:  2.4727566242218018\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  4\n",
      "training loss is:  3.1226634979248047\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  5\n",
      "training loss is:  3.390747308731079\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  6\n",
      "training loss is:  3.223965644836426\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  7\n",
      "training loss is:  3.1175355911254883\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  8\n",
      "training loss is:  3.5594482421875\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  9\n",
      "training loss is:  2.870814800262451\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  10\n",
      "training loss is:  2.7734410762786865\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  11\n",
      "training loss is:  6.4552764892578125\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  12\n",
      "training loss is:  2.7006566524505615\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  13\n",
      "training loss is:  3.5819571018218994\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  14\n",
      "training loss is:  3.5398974418640137\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  15\n",
      "training loss is:  3.7679367065429688\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  16\n",
      "training loss is:  2.7663450241088867\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  17\n",
      "training loss is:  2.4617388248443604\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  18\n",
      "training loss is:  3.135469675064087\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  19\n",
      "training loss is:  2.7861850261688232\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  20\n",
      "training loss is:  2.9155542850494385\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  21\n",
      "training loss is:  2.3143465518951416\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  0\n",
      "training loss is:  2.159147024154663\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  1\n",
      "training loss is:  3.2284886837005615\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  2\n",
      "training loss is:  2.3772594928741455\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  3\n",
      "training loss is:  3.792517900466919\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  4\n",
      "training loss is:  2.6917967796325684\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  5\n",
      "training loss is:  4.2674736976623535\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  6\n",
      "training loss is:  3.4687697887420654\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  7\n",
      "training loss is:  2.6233513355255127\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  8\n",
      "training loss is:  3.3361170291900635\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  9\n",
      "training loss is:  2.1554720401763916\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  10\n",
      "training loss is:  3.11409068107605\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  11\n",
      "training loss is:  2.5035407543182373\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  12\n",
      "training loss is:  2.7958920001983643\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  13\n",
      "training loss is:  5.537384033203125\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  14\n",
      "training loss is:  2.370631217956543\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  15\n",
      "training loss is:  3.6603341102600098\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  16\n",
      "training loss is:  3.4888901710510254\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  17\n",
      "training loss is:  2.886303424835205\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  18\n",
      "training loss is:  3.5982019901275635\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  19\n",
      "training loss is:  3.207918405532837\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  20\n",
      "training loss is:  3.2095260620117188\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  21\n",
      "training loss is:  2.0977160930633545\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  0\n",
      "training loss is:  2.784663200378418\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  1\n",
      "training loss is:  2.592175245285034\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  2\n",
      "training loss is:  3.1343894004821777\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  3\n",
      "training loss is:  2.691713571548462\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  4\n",
      "training loss is:  3.850647211074829\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  5\n",
      "training loss is:  3.961158037185669\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  6\n",
      "training loss is:  3.1455271244049072\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  7\n",
      "training loss is:  3.4214284420013428\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  8\n",
      "training loss is:  3.0641379356384277\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  9\n",
      "training loss is:  3.587202787399292\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  10\n",
      "training loss is:  2.4922537803649902\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  11\n",
      "training loss is:  3.182849407196045\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  12\n",
      "training loss is:  2.796307325363159\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  13\n",
      "training loss is:  5.3492584228515625\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  14\n",
      "training loss is:  2.7959365844726562\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  15\n",
      "training loss is:  3.0201451778411865\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  16\n",
      "training loss is:  3.1807971000671387\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  17\n",
      "training loss is:  2.9874279499053955\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  18\n",
      "training loss is:  2.9499573707580566\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  19\n",
      "training loss is:  2.9743688106536865\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  20\n",
      "training loss is:  2.642308235168457\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  21\n",
      "training loss is:  2.118875741958618\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  0\n",
      "training loss is:  3.687788724899292\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  1\n",
      "training loss is:  3.132997751235962\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  2\n",
      "training loss is:  3.7092514038085938\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  3\n",
      "training loss is:  2.4720284938812256\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  4\n",
      "training loss is:  2.6159348487854004\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  5\n",
      "training loss is:  2.998305559158325\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  6\n",
      "training loss is:  3.1550228595733643\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  7\n",
      "training loss is:  4.027304172515869\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  8\n",
      "training loss is:  6.160231113433838\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  9\n",
      "training loss is:  2.641275644302368\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  10\n",
      "training loss is:  3.13226318359375\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  11\n",
      "training loss is:  2.7212469577789307\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  12\n",
      "training loss is:  2.8831491470336914\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  13\n",
      "training loss is:  2.957057476043701\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  14\n",
      "training loss is:  4.000478744506836\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  15\n",
      "training loss is:  1.9513887166976929\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  16\n",
      "training loss is:  2.3768482208251953\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  17\n",
      "training loss is:  2.9210381507873535\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  18\n",
      "training loss is:  2.161881446838379\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  19\n",
      "training loss is:  3.1324551105499268\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  20\n",
      "training loss is:  3.358264684677124\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  21\n",
      "training loss is:  3.0523154735565186\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  0\n",
      "training loss is:  3.617152452468872\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  1\n",
      "training loss is:  3.24202299118042\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  2\n",
      "training loss is:  2.893674612045288\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  3\n",
      "training loss is:  2.3092541694641113\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  4\n",
      "training loss is:  3.9933922290802\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  5\n",
      "training loss is:  3.2912325859069824\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  6\n",
      "training loss is:  3.1583545207977295\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  7\n",
      "training loss is:  2.8557543754577637\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  8\n",
      "training loss is:  3.451556921005249\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  9\n",
      "training loss is:  5.008119583129883\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  10\n",
      "training loss is:  3.045710802078247\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  11\n",
      "training loss is:  2.412048101425171\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  12\n",
      "training loss is:  2.205521583557129\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  13\n",
      "training loss is:  4.160433769226074\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  14\n",
      "training loss is:  2.856471538543701\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  15\n",
      "training loss is:  2.9695212841033936\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  16\n",
      "training loss is:  3.0225942134857178\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  17\n",
      "training loss is:  2.636056900024414\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  18\n",
      "training loss is:  3.2515761852264404\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  19\n",
      "training loss is:  2.7753546237945557\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  20\n",
      "training loss is:  2.96090030670166\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  21\n",
      "training loss is:  2.868809461593628\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  0\n",
      "training loss is:  3.2746198177337646\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  1\n",
      "training loss is:  2.7647833824157715\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  2\n",
      "training loss is:  2.8366658687591553\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  3\n",
      "training loss is:  2.515902042388916\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  4\n",
      "training loss is:  5.3922929763793945\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  5\n",
      "training loss is:  2.8171911239624023\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  6\n",
      "training loss is:  3.2614669799804688\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  7\n",
      "training loss is:  3.2647833824157715\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  8\n",
      "training loss is:  2.8474373817443848\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  9\n",
      "training loss is:  2.784902334213257\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  10\n",
      "training loss is:  2.8075830936431885\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  11\n",
      "training loss is:  3.119887113571167\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  12\n",
      "training loss is:  3.2302849292755127\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  13\n",
      "training loss is:  3.2039380073547363\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  14\n",
      "training loss is:  3.510631799697876\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  15\n",
      "training loss is:  3.344149112701416\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  16\n",
      "training loss is:  3.1039059162139893\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  17\n",
      "training loss is:  3.0266613960266113\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  18\n",
      "training loss is:  2.561480760574341\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  19\n",
      "training loss is:  3.1040618419647217\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  20\n",
      "training loss is:  3.272961378097534\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  21\n",
      "training loss is:  3.416409969329834\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  0\n",
      "training loss is:  4.006495952606201\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  1\n",
      "training loss is:  2.8814010620117188\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  2\n",
      "training loss is:  3.197380781173706\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  3\n",
      "training loss is:  3.0852956771850586\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  4\n",
      "training loss is:  2.835644006729126\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  5\n",
      "training loss is:  3.2790396213531494\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  6\n",
      "training loss is:  3.6553430557250977\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  7\n",
      "training loss is:  3.409090042114258\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  8\n",
      "training loss is:  3.2596304416656494\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  9\n",
      "training loss is:  3.7345943450927734\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  10\n",
      "training loss is:  2.8847198486328125\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  11\n",
      "training loss is:  2.4910361766815186\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  12\n",
      "training loss is:  2.7799179553985596\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  13\n",
      "training loss is:  2.185246467590332\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  14\n",
      "training loss is:  2.7208263874053955\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  15\n",
      "training loss is:  2.7524263858795166\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  16\n",
      "training loss is:  3.1441643238067627\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  17\n",
      "training loss is:  4.974153995513916\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  18\n",
      "training loss is:  2.4560904502868652\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  19\n",
      "training loss is:  3.1449151039123535\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  20\n",
      "training loss is:  3.1144182682037354\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  21\n",
      "training loss is:  3.6702685356140137\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  0\n",
      "training loss is:  5.286567211151123\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  1\n",
      "training loss is:  3.0687994956970215\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  2\n",
      "training loss is:  3.0176002979278564\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  3\n",
      "training loss is:  2.6422548294067383\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  4\n",
      "training loss is:  2.976947546005249\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  5\n",
      "training loss is:  2.852825880050659\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  6\n",
      "training loss is:  3.118422746658325\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  7\n",
      "training loss is:  2.8019402027130127\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  8\n",
      "training loss is:  3.0878243446350098\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  9\n",
      "training loss is:  3.100470781326294\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  10\n",
      "training loss is:  2.632812738418579\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  11\n",
      "training loss is:  3.21038818359375\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  12\n",
      "training loss is:  3.5193843841552734\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  13\n",
      "training loss is:  3.134399175643921\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  14\n",
      "training loss is:  2.151533603668213\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  15\n",
      "training loss is:  2.989081621170044\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  16\n",
      "training loss is:  3.4951398372650146\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  17\n",
      "training loss is:  3.4736499786376953\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  18\n",
      "training loss is:  2.6381914615631104\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  19\n",
      "training loss is:  3.0919923782348633\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  20\n",
      "training loss is:  3.5822277069091797\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  21\n",
      "training loss is:  3.309088945388794\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  0\n",
      "training loss is:  3.110304832458496\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  1\n",
      "training loss is:  2.510115623474121\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  2\n",
      "training loss is:  3.2191927433013916\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  3\n",
      "training loss is:  2.837524175643921\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  4\n",
      "training loss is:  2.7860639095306396\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  5\n",
      "training loss is:  2.3589863777160645\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  6\n",
      "training loss is:  2.5938968658447266\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  7\n",
      "training loss is:  2.7735342979431152\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  8\n",
      "training loss is:  3.01041841506958\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  9\n",
      "training loss is:  3.0300474166870117\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  10\n",
      "training loss is:  3.212641954421997\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  11\n",
      "training loss is:  2.6978890895843506\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  12\n",
      "training loss is:  3.316666603088379\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  13\n",
      "training loss is:  2.977227210998535\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  14\n",
      "training loss is:  5.392758846282959\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  15\n",
      "training loss is:  4.3024516105651855\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  16\n",
      "training loss is:  3.745278835296631\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  17\n",
      "training loss is:  3.265958786010742\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  18\n",
      "training loss is:  3.074894666671753\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  19\n",
      "training loss is:  2.6572420597076416\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  20\n",
      "training loss is:  2.8065950870513916\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  21\n",
      "training loss is:  4.251393795013428\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  0\n",
      "training loss is:  4.824410438537598\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  1\n",
      "training loss is:  2.441477060317993\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  2\n",
      "training loss is:  2.662292718887329\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  3\n",
      "training loss is:  3.2077977657318115\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  4\n",
      "training loss is:  3.289231300354004\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  5\n",
      "training loss is:  4.713674068450928\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  6\n",
      "training loss is:  3.0787792205810547\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  7\n",
      "training loss is:  3.4098329544067383\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  8\n",
      "training loss is:  3.1719112396240234\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  9\n",
      "training loss is:  2.372352123260498\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  10\n",
      "training loss is:  2.7719554901123047\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  11\n",
      "training loss is:  3.0327515602111816\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  12\n",
      "training loss is:  2.786153793334961\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  13\n",
      "training loss is:  3.1162633895874023\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  14\n",
      "training loss is:  3.4445748329162598\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  15\n",
      "training loss is:  3.6780130863189697\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  16\n",
      "training loss is:  2.8472909927368164\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  17\n",
      "training loss is:  2.9762680530548096\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  18\n",
      "training loss is:  2.9472744464874268\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  19\n",
      "training loss is:  2.574991226196289\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  20\n",
      "training loss is:  3.0431747436523438\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  21\n",
      "training loss is:  1.9615739583969116\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  0\n",
      "training loss is:  2.5489118099212646\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  1\n",
      "training loss is:  2.5433123111724854\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  2\n",
      "training loss is:  3.3874895572662354\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  3\n",
      "training loss is:  2.6829466819763184\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  4\n",
      "training loss is:  2.8992419242858887\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  5\n",
      "training loss is:  3.649376630783081\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  6\n",
      "training loss is:  2.5985467433929443\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  7\n",
      "training loss is:  3.035773515701294\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  8\n",
      "training loss is:  2.766822576522827\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  9\n",
      "training loss is:  4.990930080413818\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  10\n",
      "training loss is:  3.934251070022583\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  11\n",
      "training loss is:  2.9238498210906982\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  12\n",
      "training loss is:  2.755631685256958\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  13\n",
      "training loss is:  2.3499581813812256\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  14\n",
      "training loss is:  3.6939878463745117\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  15\n",
      "training loss is:  3.036306381225586\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  16\n",
      "training loss is:  2.9972281455993652\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  17\n",
      "training loss is:  3.249119758605957\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  18\n",
      "training loss is:  3.5112874507904053\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  19\n",
      "training loss is:  2.673226833343506\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  20\n",
      "training loss is:  3.2392776012420654\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  21\n",
      "training loss is:  3.9981155395507812\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  0\n",
      "training loss is:  3.0150818824768066\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  1\n",
      "training loss is:  2.8389949798583984\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  2\n",
      "training loss is:  2.9820241928100586\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  3\n",
      "training loss is:  3.290773391723633\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  4\n",
      "training loss is:  2.8519513607025146\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  5\n",
      "training loss is:  2.5161561965942383\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  6\n",
      "training loss is:  2.5800061225891113\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  7\n",
      "training loss is:  2.508408546447754\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  8\n",
      "training loss is:  5.1997389793396\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  9\n",
      "training loss is:  2.668678045272827\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  10\n",
      "training loss is:  3.1287918090820312\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  11\n",
      "training loss is:  2.786161184310913\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  12\n",
      "training loss is:  2.5993540287017822\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  13\n",
      "training loss is:  3.185269355773926\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  14\n",
      "training loss is:  3.500412702560425\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  15\n",
      "training loss is:  2.8978002071380615\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  16\n",
      "training loss is:  3.7187817096710205\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  17\n",
      "training loss is:  3.394275426864624\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  18\n",
      "training loss is:  2.8605618476867676\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  19\n",
      "training loss is:  3.7162857055664062\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  20\n",
      "training loss is:  4.232370376586914\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  21\n",
      "training loss is:  1.7738755941390991\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  0\n",
      "training loss is:  3.723259687423706\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  1\n",
      "training loss is:  2.492753505706787\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  2\n",
      "training loss is:  3.0256693363189697\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  3\n",
      "training loss is:  2.9556589126586914\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  4\n",
      "training loss is:  3.6040420532226562\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  5\n",
      "training loss is:  2.584026575088501\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  6\n",
      "training loss is:  3.385316848754883\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  7\n",
      "training loss is:  6.011271953582764\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  8\n",
      "training loss is:  3.309168815612793\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  9\n",
      "training loss is:  3.2105712890625\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  10\n",
      "training loss is:  2.1693363189697266\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  11\n",
      "training loss is:  2.5277419090270996\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  12\n",
      "training loss is:  2.4524078369140625\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  13\n",
      "training loss is:  2.987186908721924\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  14\n",
      "training loss is:  3.304337501525879\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  15\n",
      "training loss is:  3.4303369522094727\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  16\n",
      "training loss is:  2.8053274154663086\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  17\n",
      "training loss is:  3.335348129272461\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  18\n",
      "training loss is:  3.58099365234375\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  19\n",
      "training loss is:  2.353428602218628\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  20\n",
      "training loss is:  2.8016793727874756\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  21\n",
      "training loss is:  3.6705782413482666\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  0\n",
      "training loss is:  3.143829107284546\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  1\n",
      "training loss is:  3.942502975463867\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  2\n",
      "training loss is:  2.8126442432403564\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  3\n",
      "training loss is:  2.6694095134735107\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  4\n",
      "training loss is:  2.471611976623535\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  5\n",
      "training loss is:  3.3402698040008545\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  6\n",
      "training loss is:  3.398927927017212\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  7\n",
      "training loss is:  2.921518564224243\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  8\n",
      "training loss is:  2.9869048595428467\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  9\n",
      "training loss is:  2.834406852722168\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  10\n",
      "training loss is:  2.8600101470947266\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  11\n",
      "training loss is:  3.147700548171997\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  12\n",
      "training loss is:  3.4713058471679688\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  13\n",
      "training loss is:  3.0576393604278564\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  14\n",
      "training loss is:  2.7056081295013428\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  15\n",
      "training loss is:  3.7091336250305176\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  16\n",
      "training loss is:  3.1466457843780518\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  17\n",
      "training loss is:  4.987152099609375\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  18\n",
      "training loss is:  2.6499717235565186\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  19\n",
      "training loss is:  3.0855295658111572\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  20\n",
      "training loss is:  3.149508476257324\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  21\n",
      "training loss is:  2.5053012371063232\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  0\n",
      "training loss is:  3.3002500534057617\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  1\n",
      "training loss is:  3.126786470413208\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  2\n",
      "training loss is:  4.824873447418213\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  3\n",
      "training loss is:  3.2225182056427\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  4\n",
      "training loss is:  2.7769243717193604\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  5\n",
      "training loss is:  2.684659719467163\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  6\n",
      "training loss is:  3.2278385162353516\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  7\n",
      "training loss is:  3.2689037322998047\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  8\n",
      "training loss is:  3.806473731994629\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  9\n",
      "training loss is:  2.971029043197632\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  10\n",
      "training loss is:  2.8892977237701416\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  11\n",
      "training loss is:  3.2162532806396484\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  12\n",
      "training loss is:  2.877821683883667\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  13\n",
      "training loss is:  3.311244010925293\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  14\n",
      "training loss is:  2.446594476699829\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  15\n",
      "training loss is:  1.9501953125\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  16\n",
      "training loss is:  3.2845351696014404\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  17\n",
      "training loss is:  3.38789701461792\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  18\n",
      "training loss is:  4.003754138946533\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  19\n",
      "training loss is:  2.724431037902832\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  20\n",
      "training loss is:  3.4831249713897705\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  21\n",
      "training loss is:  2.067319393157959\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  0\n",
      "training loss is:  2.8429670333862305\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  1\n",
      "training loss is:  2.8994762897491455\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  2\n",
      "training loss is:  3.0170187950134277\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  3\n",
      "training loss is:  3.060803174972534\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  4\n",
      "training loss is:  2.8355472087860107\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  5\n",
      "training loss is:  4.144072532653809\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  6\n",
      "training loss is:  2.961578130722046\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  7\n",
      "training loss is:  2.742462158203125\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  8\n",
      "training loss is:  2.4208948612213135\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  9\n",
      "training loss is:  3.52841854095459\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  10\n",
      "training loss is:  2.42144775390625\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  11\n",
      "training loss is:  3.070575475692749\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  12\n",
      "training loss is:  2.8942458629608154\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  13\n",
      "training loss is:  3.1709275245666504\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  14\n",
      "training loss is:  5.475649833679199\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  15\n",
      "training loss is:  3.4697234630584717\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  16\n",
      "training loss is:  3.5731711387634277\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  17\n",
      "training loss is:  2.5653820037841797\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  18\n",
      "training loss is:  2.9941322803497314\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  19\n",
      "training loss is:  2.8880155086517334\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  20\n",
      "training loss is:  3.0981369018554688\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  21\n",
      "training loss is:  2.9302256107330322\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  0\n",
      "training loss is:  2.5211451053619385\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  1\n",
      "training loss is:  2.7527127265930176\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  2\n",
      "training loss is:  2.830155611038208\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  3\n",
      "training loss is:  3.4975435733795166\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  4\n",
      "training loss is:  3.8129873275756836\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  5\n",
      "training loss is:  2.837580919265747\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  6\n",
      "training loss is:  3.641918659210205\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  7\n",
      "training loss is:  3.2699050903320312\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  8\n",
      "training loss is:  2.77107310295105\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  9\n",
      "training loss is:  5.4802374839782715\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  10\n",
      "training loss is:  3.323603630065918\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  11\n",
      "training loss is:  2.5883302688598633\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  12\n",
      "training loss is:  2.442837953567505\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  13\n",
      "training loss is:  3.476858139038086\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  14\n",
      "training loss is:  2.9427382946014404\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  15\n",
      "training loss is:  3.463022470474243\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  16\n",
      "training loss is:  3.911738634109497\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  17\n",
      "training loss is:  2.695829153060913\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  18\n",
      "training loss is:  2.7064871788024902\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  19\n",
      "training loss is:  2.84769344329834\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  20\n",
      "training loss is:  2.158257484436035\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  21\n",
      "training loss is:  2.957705497741699\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  0\n",
      "training loss is:  3.5544748306274414\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  1\n",
      "training loss is:  3.219125747680664\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  2\n",
      "training loss is:  2.5020289421081543\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  3\n",
      "training loss is:  3.0220348834991455\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  4\n",
      "training loss is:  2.9014532566070557\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  5\n",
      "training loss is:  3.733158826828003\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  6\n",
      "training loss is:  2.9895546436309814\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  7\n",
      "training loss is:  3.217142105102539\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  8\n",
      "training loss is:  2.366931200027466\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  9\n",
      "training loss is:  3.624082326889038\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  10\n",
      "training loss is:  3.202939987182617\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  11\n",
      "training loss is:  2.5860249996185303\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  12\n",
      "training loss is:  3.059507131576538\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  13\n",
      "training loss is:  2.559568166732788\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  14\n",
      "training loss is:  2.5855448246002197\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  15\n",
      "training loss is:  3.306438684463501\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  16\n",
      "training loss is:  5.386561870574951\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  17\n",
      "training loss is:  3.030935764312744\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  18\n",
      "training loss is:  3.146935224533081\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  19\n",
      "training loss is:  2.6896791458129883\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  20\n",
      "training loss is:  3.7516777515411377\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  21\n",
      "training loss is:  2.327284574508667\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  0\n",
      "training loss is:  2.6929705142974854\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  1\n",
      "training loss is:  2.433105707168579\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  2\n",
      "training loss is:  3.7707228660583496\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  3\n",
      "training loss is:  2.883131980895996\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  4\n",
      "training loss is:  3.1183669567108154\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  5\n",
      "training loss is:  2.579016923904419\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  6\n",
      "training loss is:  2.7319021224975586\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  7\n",
      "training loss is:  4.107100963592529\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  8\n",
      "training loss is:  2.920212507247925\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  9\n",
      "training loss is:  3.849700689315796\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  10\n",
      "training loss is:  3.0848138332366943\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  11\n",
      "training loss is:  3.303590774536133\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  12\n",
      "training loss is:  5.1707563400268555\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  13\n",
      "training loss is:  3.100405693054199\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  14\n",
      "training loss is:  2.5307037830352783\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  15\n",
      "training loss is:  3.46498441696167\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  16\n",
      "training loss is:  2.9577105045318604\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  17\n",
      "training loss is:  2.7978179454803467\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  18\n",
      "training loss is:  2.851893663406372\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  19\n",
      "training loss is:  2.9794068336486816\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  20\n",
      "training loss is:  2.6014232635498047\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  21\n",
      "training loss is:  2.9321606159210205\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  0\n",
      "training loss is:  3.4913876056671143\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  1\n",
      "training loss is:  2.515220880508423\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  2\n",
      "training loss is:  2.986056327819824\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  3\n",
      "training loss is:  2.5125746726989746\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  4\n",
      "training loss is:  4.017834663391113\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  5\n",
      "training loss is:  2.603555679321289\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  6\n",
      "training loss is:  3.3497698307037354\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  7\n",
      "training loss is:  2.7734153270721436\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  8\n",
      "training loss is:  3.5629260540008545\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  9\n",
      "training loss is:  3.537593364715576\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  10\n",
      "training loss is:  3.2415266036987305\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  11\n",
      "training loss is:  2.6956136226654053\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  12\n",
      "training loss is:  3.2606732845306396\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  13\n",
      "training loss is:  2.5943284034729004\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  14\n",
      "training loss is:  2.8356704711914062\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  15\n",
      "training loss is:  3.2572641372680664\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  16\n",
      "training loss is:  2.8640170097351074\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  17\n",
      "training loss is:  2.966137170791626\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  18\n",
      "training loss is:  4.977010726928711\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  19\n",
      "training loss is:  2.802504777908325\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  20\n",
      "training loss is:  3.299381732940674\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  21\n",
      "training loss is:  3.655189037322998\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  0\n",
      "training loss is:  3.1382827758789062\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  1\n",
      "training loss is:  2.8664655685424805\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  2\n",
      "training loss is:  3.6683547496795654\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  3\n",
      "training loss is:  2.9613165855407715\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  4\n",
      "training loss is:  3.2792553901672363\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  5\n",
      "training loss is:  3.0627081394195557\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  6\n",
      "training loss is:  3.159605026245117\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  7\n",
      "training loss is:  2.3020126819610596\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  8\n",
      "training loss is:  3.1622912883758545\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  9\n",
      "training loss is:  2.987765312194824\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  10\n",
      "training loss is:  3.6730239391326904\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  11\n",
      "training loss is:  3.2172186374664307\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  12\n",
      "training loss is:  3.3959341049194336\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  13\n",
      "training loss is:  2.789612054824829\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  14\n",
      "training loss is:  2.696506977081299\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  15\n",
      "training loss is:  2.9354946613311768\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  16\n",
      "training loss is:  2.3062798976898193\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  17\n",
      "training loss is:  5.838393688201904\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  18\n",
      "training loss is:  2.4202072620391846\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  19\n",
      "training loss is:  2.997384548187256\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  20\n",
      "training loss is:  3.162734270095825\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  21\n",
      "training loss is:  2.607008695602417\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  0\n",
      "training loss is:  5.156415939331055\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  1\n",
      "training loss is:  4.669538974761963\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  2\n",
      "training loss is:  3.261871576309204\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  3\n",
      "training loss is:  2.407160997390747\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  4\n",
      "training loss is:  3.08359432220459\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  5\n",
      "training loss is:  2.5347368717193604\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  6\n",
      "training loss is:  3.110907554626465\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  7\n",
      "training loss is:  3.2800710201263428\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  8\n",
      "training loss is:  2.7164385318756104\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  9\n",
      "training loss is:  2.523237705230713\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  10\n",
      "training loss is:  2.763265371322632\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  11\n",
      "training loss is:  2.979545831680298\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  12\n",
      "training loss is:  3.483266592025757\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  13\n",
      "training loss is:  2.985912561416626\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  14\n",
      "training loss is:  3.5503456592559814\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  15\n",
      "training loss is:  3.463235855102539\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  16\n",
      "training loss is:  2.4706647396087646\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  17\n",
      "training loss is:  3.0506675243377686\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  18\n",
      "training loss is:  2.6555631160736084\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  19\n",
      "training loss is:  2.734901189804077\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  20\n",
      "training loss is:  3.227978467941284\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  21\n",
      "training loss is:  3.0410170555114746\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  0\n",
      "training loss is:  2.5596683025360107\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  1\n",
      "training loss is:  2.628828525543213\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  2\n",
      "training loss is:  2.9373013973236084\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  3\n",
      "training loss is:  3.073002576828003\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  4\n",
      "training loss is:  2.7305667400360107\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  5\n",
      "training loss is:  3.076077938079834\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  6\n",
      "training loss is:  2.8403308391571045\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  7\n",
      "training loss is:  2.5728759765625\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  8\n",
      "training loss is:  2.1928343772888184\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  9\n",
      "training loss is:  2.6651296615600586\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  10\n",
      "training loss is:  3.4378442764282227\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  11\n",
      "training loss is:  2.54799222946167\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  12\n",
      "training loss is:  3.6258511543273926\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  13\n",
      "training loss is:  3.8681135177612305\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  14\n",
      "training loss is:  3.3107073307037354\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  15\n",
      "training loss is:  3.0805437564849854\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  16\n",
      "training loss is:  2.3466320037841797\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  17\n",
      "training loss is:  3.6227638721466064\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  18\n",
      "training loss is:  2.7194576263427734\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  19\n",
      "training loss is:  3.8781392574310303\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  20\n",
      "training loss is:  6.257717132568359\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  21\n",
      "training loss is:  2.811497211456299\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  0\n",
      "training loss is:  3.735048770904541\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  1\n",
      "training loss is:  3.264174222946167\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  2\n",
      "training loss is:  2.661005973815918\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  3\n",
      "training loss is:  2.3585190773010254\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  4\n",
      "training loss is:  3.105346441268921\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  5\n",
      "training loss is:  2.455197811126709\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  6\n",
      "training loss is:  2.748042345046997\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  7\n",
      "training loss is:  2.512791156768799\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  8\n",
      "training loss is:  3.3378615379333496\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  9\n",
      "training loss is:  3.2085351943969727\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  10\n",
      "training loss is:  2.733741044998169\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  11\n",
      "training loss is:  3.6135830879211426\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  12\n",
      "training loss is:  3.064530611038208\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  13\n",
      "training loss is:  2.695624589920044\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  14\n",
      "training loss is:  5.729315280914307\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  15\n",
      "training loss is:  2.2773993015289307\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  16\n",
      "training loss is:  3.4988138675689697\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  17\n",
      "training loss is:  2.7001328468322754\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  18\n",
      "training loss is:  3.1411375999450684\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  19\n",
      "training loss is:  3.8403067588806152\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  20\n",
      "training loss is:  3.6202118396759033\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  21\n",
      "training loss is:  2.5739336013793945\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  0\n",
      "training loss is:  2.985137701034546\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  1\n",
      "training loss is:  3.361818552017212\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  2\n",
      "training loss is:  2.793367624282837\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  3\n",
      "training loss is:  2.50919246673584\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  4\n",
      "training loss is:  2.3764989376068115\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  5\n",
      "training loss is:  2.664541006088257\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  6\n",
      "training loss is:  3.646296977996826\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  7\n",
      "training loss is:  2.81002140045166\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  8\n",
      "training loss is:  3.3345258235931396\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  9\n",
      "training loss is:  4.730225086212158\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  10\n",
      "training loss is:  3.01277494430542\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  11\n",
      "training loss is:  2.886876106262207\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  12\n",
      "training loss is:  4.26719856262207\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  13\n",
      "training loss is:  2.836928129196167\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  14\n",
      "training loss is:  3.3664350509643555\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  15\n",
      "training loss is:  3.236562728881836\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  16\n",
      "training loss is:  3.608383893966675\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  17\n",
      "training loss is:  2.791929244995117\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  18\n",
      "training loss is:  2.8808083534240723\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  19\n",
      "training loss is:  3.0001256465911865\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  20\n",
      "training loss is:  2.8654918670654297\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  21\n",
      "training loss is:  3.3438291549682617\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  0\n",
      "training loss is:  2.901940107345581\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  1\n",
      "training loss is:  3.380199909210205\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  2\n",
      "training loss is:  3.7282590866088867\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  3\n",
      "training loss is:  2.701399564743042\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  4\n",
      "training loss is:  2.418954372406006\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  5\n",
      "training loss is:  3.910682439804077\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  6\n",
      "training loss is:  2.658132553100586\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  7\n",
      "training loss is:  2.773346185684204\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  8\n",
      "training loss is:  3.3417422771453857\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  9\n",
      "training loss is:  3.0699896812438965\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  10\n",
      "training loss is:  2.991452693939209\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  11\n",
      "training loss is:  2.6549103260040283\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  12\n",
      "training loss is:  3.634944200515747\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  13\n",
      "training loss is:  3.0258727073669434\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  14\n",
      "training loss is:  5.342372417449951\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  15\n",
      "training loss is:  3.467498779296875\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  16\n",
      "training loss is:  3.0092766284942627\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  17\n",
      "training loss is:  2.6029725074768066\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  18\n",
      "training loss is:  3.0052366256713867\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  19\n",
      "training loss is:  2.773026704788208\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  20\n",
      "training loss is:  2.665600299835205\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  21\n",
      "training loss is:  2.3641488552093506\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  0\n",
      "training loss is:  2.920552968978882\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  1\n",
      "training loss is:  3.2850730419158936\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  2\n",
      "training loss is:  3.2771270275115967\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  3\n",
      "training loss is:  3.2428700923919678\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  4\n",
      "training loss is:  3.1398231983184814\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  5\n",
      "training loss is:  5.6171875\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  6\n",
      "training loss is:  2.504580020904541\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  7\n",
      "training loss is:  2.838749885559082\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  8\n",
      "training loss is:  2.4241864681243896\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  9\n",
      "training loss is:  3.2904114723205566\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  10\n",
      "training loss is:  2.6622531414031982\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  11\n",
      "training loss is:  3.281872272491455\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  12\n",
      "training loss is:  2.7439992427825928\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  13\n",
      "training loss is:  2.3697423934936523\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  14\n",
      "training loss is:  3.2740705013275146\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  15\n",
      "training loss is:  3.023775100708008\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  16\n",
      "training loss is:  3.8962948322296143\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  17\n",
      "training loss is:  2.951306104660034\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  18\n",
      "training loss is:  2.6209723949432373\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  19\n",
      "training loss is:  3.90669584274292\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  20\n",
      "training loss is:  2.8408684730529785\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  21\n",
      "training loss is:  3.0397725105285645\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  0\n",
      "training loss is:  2.670119524002075\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  1\n",
      "training loss is:  2.385091781616211\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  2\n",
      "training loss is:  4.83933687210083\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  3\n",
      "training loss is:  2.9203484058380127\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  4\n",
      "training loss is:  2.9065141677856445\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  5\n",
      "training loss is:  2.6070291996002197\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  6\n",
      "training loss is:  3.622694730758667\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  7\n",
      "training loss is:  2.967637062072754\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  8\n",
      "training loss is:  2.608436346054077\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  9\n",
      "training loss is:  4.116372108459473\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  10\n",
      "training loss is:  3.8043394088745117\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  11\n",
      "training loss is:  2.849257230758667\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  12\n",
      "training loss is:  3.0885040760040283\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  13\n",
      "training loss is:  2.647228479385376\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  14\n",
      "training loss is:  2.778916597366333\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  15\n",
      "training loss is:  3.793414354324341\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  16\n",
      "training loss is:  2.5559380054473877\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  17\n",
      "training loss is:  3.1247899532318115\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  18\n",
      "training loss is:  3.7074859142303467\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  19\n",
      "training loss is:  2.794464349746704\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  20\n",
      "training loss is:  3.1308302879333496\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  21\n",
      "training loss is:  2.893226146697998\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  0\n",
      "training loss is:  2.7213218212127686\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  1\n",
      "training loss is:  2.923058271408081\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  2\n",
      "training loss is:  3.9880318641662598\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  3\n",
      "training loss is:  4.983384609222412\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  4\n",
      "training loss is:  3.033017635345459\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  5\n",
      "training loss is:  2.4273557662963867\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  6\n",
      "training loss is:  3.2817800045013428\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  7\n",
      "training loss is:  2.8639955520629883\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  8\n",
      "training loss is:  3.6829192638397217\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  9\n",
      "training loss is:  3.195582389831543\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  10\n",
      "training loss is:  2.8706960678100586\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  11\n",
      "training loss is:  2.302607536315918\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  12\n",
      "training loss is:  3.5126943588256836\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  13\n",
      "training loss is:  2.9065101146698\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  14\n",
      "training loss is:  2.5854756832122803\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  15\n",
      "training loss is:  2.9008939266204834\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  16\n",
      "training loss is:  3.426048994064331\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  17\n",
      "training loss is:  2.917102098464966\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  18\n",
      "training loss is:  2.926372766494751\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  19\n",
      "training loss is:  2.9580068588256836\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  20\n",
      "training loss is:  3.431652784347534\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  21\n",
      "training loss is:  3.660966396331787\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  0\n",
      "training loss is:  5.608398914337158\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  1\n",
      "training loss is:  2.908795118331909\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  2\n",
      "training loss is:  3.0874643325805664\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  3\n",
      "training loss is:  2.864380121231079\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  4\n",
      "training loss is:  3.2664425373077393\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  5\n",
      "training loss is:  2.755457878112793\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  6\n",
      "training loss is:  2.733463764190674\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  7\n",
      "training loss is:  3.6250693798065186\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  8\n",
      "training loss is:  2.6742475032806396\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  9\n",
      "training loss is:  2.4982926845550537\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  10\n",
      "training loss is:  2.823784828186035\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  11\n",
      "training loss is:  2.2694649696350098\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  12\n",
      "training loss is:  3.333625078201294\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  13\n",
      "training loss is:  3.1954615116119385\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  14\n",
      "training loss is:  3.11403226852417\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  15\n",
      "training loss is:  3.584064245223999\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  16\n",
      "training loss is:  3.011946678161621\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  17\n",
      "training loss is:  3.715003728866577\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  18\n",
      "training loss is:  2.9285216331481934\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  19\n",
      "training loss is:  3.0260961055755615\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  20\n",
      "training loss is:  2.878551959991455\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  21\n",
      "training loss is:  2.685152769088745\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  0\n",
      "training loss is:  3.13582181930542\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  1\n",
      "training loss is:  2.909980535507202\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  2\n",
      "training loss is:  2.850144624710083\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  3\n",
      "training loss is:  3.5268783569335938\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  4\n",
      "training loss is:  3.080244779586792\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  5\n",
      "training loss is:  3.3460254669189453\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  6\n",
      "training loss is:  2.769022226333618\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  7\n",
      "training loss is:  2.1621248722076416\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  8\n",
      "training loss is:  3.267834186553955\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  9\n",
      "training loss is:  3.525745153427124\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  10\n",
      "training loss is:  2.686342716217041\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  11\n",
      "training loss is:  2.806183099746704\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  12\n",
      "training loss is:  2.9489078521728516\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  13\n",
      "training loss is:  2.892918586730957\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  14\n",
      "training loss is:  2.777536392211914\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  15\n",
      "training loss is:  5.997251987457275\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  16\n",
      "training loss is:  3.015150547027588\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  17\n",
      "training loss is:  3.7421064376831055\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  18\n",
      "training loss is:  2.928921699523926\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  19\n",
      "training loss is:  2.6121466159820557\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  20\n",
      "training loss is:  3.055438756942749\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  21\n",
      "training loss is:  3.1117818355560303\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  0\n",
      "training loss is:  2.6587026119232178\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  1\n",
      "training loss is:  3.026867628097534\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  2\n",
      "training loss is:  3.648634433746338\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  3\n",
      "training loss is:  2.799419641494751\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  4\n",
      "training loss is:  2.587367057800293\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  5\n",
      "training loss is:  3.243502616882324\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  6\n",
      "training loss is:  3.2892563343048096\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  7\n",
      "training loss is:  2.7518203258514404\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  8\n",
      "training loss is:  3.010502576828003\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  9\n",
      "training loss is:  3.048325300216675\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  10\n",
      "training loss is:  3.0834405422210693\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  11\n",
      "training loss is:  3.4472649097442627\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  12\n",
      "training loss is:  2.8396124839782715\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  13\n",
      "training loss is:  2.63127064704895\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  14\n",
      "training loss is:  2.6493988037109375\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  15\n",
      "training loss is:  2.774993896484375\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  16\n",
      "training loss is:  3.7719695568084717\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  17\n",
      "training loss is:  2.3461384773254395\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  18\n",
      "training loss is:  3.612727165222168\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  19\n",
      "training loss is:  3.07401180267334\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  20\n",
      "training loss is:  5.753650665283203\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  21\n",
      "training loss is:  2.945774793624878\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  0\n",
      "training loss is:  2.6071078777313232\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  1\n",
      "training loss is:  3.3513872623443604\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  2\n",
      "training loss is:  3.5515334606170654\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  3\n",
      "training loss is:  2.7193117141723633\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  4\n",
      "training loss is:  2.7648301124572754\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  5\n",
      "training loss is:  4.977022647857666\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  6\n",
      "training loss is:  3.5056333541870117\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  7\n",
      "training loss is:  3.6688127517700195\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  8\n",
      "training loss is:  2.7967679500579834\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  9\n",
      "training loss is:  3.0902514457702637\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  10\n",
      "training loss is:  3.1053640842437744\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  11\n",
      "training loss is:  2.6605842113494873\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  12\n",
      "training loss is:  3.0040998458862305\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  13\n",
      "training loss is:  2.9561526775360107\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  14\n",
      "training loss is:  2.6349685192108154\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  15\n",
      "training loss is:  2.572713613510132\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  16\n",
      "training loss is:  2.7147741317749023\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  17\n",
      "training loss is:  3.6915512084960938\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  18\n",
      "training loss is:  2.8018033504486084\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  19\n",
      "training loss is:  3.543712854385376\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  20\n",
      "training loss is:  2.8247082233428955\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  21\n",
      "training loss is:  4.50747013092041\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  0\n",
      "training loss is:  2.4764564037323\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  1\n",
      "training loss is:  5.485739231109619\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  2\n",
      "training loss is:  3.0656065940856934\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  3\n",
      "training loss is:  3.869509696960449\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  4\n",
      "training loss is:  2.837864875793457\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  5\n",
      "training loss is:  3.0632681846618652\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  6\n",
      "training loss is:  3.1397287845611572\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  7\n",
      "training loss is:  3.0924181938171387\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  8\n",
      "training loss is:  3.383626699447632\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  9\n",
      "training loss is:  2.162881851196289\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  10\n",
      "training loss is:  2.9842355251312256\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  11\n",
      "training loss is:  3.1132116317749023\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  12\n",
      "training loss is:  3.1813454627990723\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  13\n",
      "training loss is:  2.52362060546875\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  14\n",
      "training loss is:  2.95352840423584\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  15\n",
      "training loss is:  2.5024120807647705\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  16\n",
      "training loss is:  3.7039308547973633\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  17\n",
      "training loss is:  2.913837194442749\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  18\n",
      "training loss is:  2.622697114944458\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  19\n",
      "training loss is:  3.6241695880889893\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  20\n",
      "training loss is:  3.1597671508789062\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  21\n",
      "training loss is:  3.5322325229644775\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  0\n",
      "training loss is:  2.779818534851074\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  1\n",
      "training loss is:  3.0613906383514404\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  2\n",
      "training loss is:  2.3943674564361572\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  3\n",
      "training loss is:  3.019223928451538\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  4\n",
      "training loss is:  3.592655658721924\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  5\n",
      "training loss is:  2.4537060260772705\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  6\n",
      "training loss is:  2.8511340618133545\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  7\n",
      "training loss is:  3.14011287689209\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  8\n",
      "training loss is:  2.6060891151428223\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  9\n",
      "training loss is:  2.1579971313476562\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  10\n",
      "training loss is:  2.435173749923706\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  11\n",
      "training loss is:  3.5426530838012695\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  12\n",
      "training loss is:  2.604334831237793\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  13\n",
      "training loss is:  4.80124568939209\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  14\n",
      "training loss is:  5.4090776443481445\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  15\n",
      "training loss is:  3.2826149463653564\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  16\n",
      "training loss is:  3.5559182167053223\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  17\n",
      "training loss is:  2.858455181121826\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  18\n",
      "training loss is:  3.289644479751587\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  19\n",
      "training loss is:  3.314589738845825\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  20\n",
      "training loss is:  2.6213605403900146\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  21\n",
      "training loss is:  2.864875555038452\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  0\n",
      "training loss is:  2.825324058532715\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  1\n",
      "training loss is:  3.550065040588379\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  2\n",
      "training loss is:  2.8843228816986084\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  3\n",
      "training loss is:  2.7004806995391846\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  4\n",
      "training loss is:  5.553635120391846\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  5\n",
      "training loss is:  2.518282651901245\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  6\n",
      "training loss is:  3.5713961124420166\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  7\n",
      "training loss is:  4.010519027709961\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  8\n",
      "training loss is:  2.2910263538360596\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  9\n",
      "training loss is:  2.9786412715911865\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  10\n",
      "training loss is:  3.1499407291412354\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  11\n",
      "training loss is:  2.6033313274383545\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  12\n",
      "training loss is:  3.010892391204834\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  13\n",
      "training loss is:  2.5037057399749756\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  14\n",
      "training loss is:  2.662243127822876\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  15\n",
      "training loss is:  3.824000358581543\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  16\n",
      "training loss is:  3.502370834350586\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  17\n",
      "training loss is:  2.89704966545105\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  18\n",
      "training loss is:  2.81435489654541\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  19\n",
      "training loss is:  2.889155864715576\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  20\n",
      "training loss is:  3.2574689388275146\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  21\n",
      "training loss is:  2.6862666606903076\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  0\n",
      "training loss is:  3.1597201824188232\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  1\n",
      "training loss is:  3.2100319862365723\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  2\n",
      "training loss is:  2.7286903858184814\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  3\n",
      "training loss is:  3.213244915008545\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  4\n",
      "training loss is:  2.9937233924865723\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  5\n",
      "training loss is:  2.6257543563842773\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  6\n",
      "training loss is:  6.116818904876709\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  7\n",
      "training loss is:  3.097574472427368\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  8\n",
      "training loss is:  3.149876594543457\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  9\n",
      "training loss is:  3.058502435684204\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  10\n",
      "training loss is:  3.401010513305664\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  11\n",
      "training loss is:  2.6100428104400635\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  12\n",
      "training loss is:  2.8343629837036133\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  13\n",
      "training loss is:  2.8033971786499023\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  14\n",
      "training loss is:  3.320643901824951\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  15\n",
      "training loss is:  2.727307081222534\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  16\n",
      "training loss is:  2.996720314025879\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  17\n",
      "training loss is:  2.5042200088500977\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  18\n",
      "training loss is:  2.7268357276916504\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  19\n",
      "training loss is:  3.676379919052124\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  20\n",
      "training loss is:  2.7623584270477295\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  21\n",
      "training loss is:  4.332850456237793\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  0\n",
      "training loss is:  3.0645008087158203\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  1\n",
      "training loss is:  3.3615002632141113\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  2\n",
      "training loss is:  2.358248233795166\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  3\n",
      "training loss is:  3.3580117225646973\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  4\n",
      "training loss is:  2.6691155433654785\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  5\n",
      "training loss is:  2.868342399597168\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  6\n",
      "training loss is:  5.35257625579834\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  7\n",
      "training loss is:  2.2874295711517334\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  8\n",
      "training loss is:  3.599977493286133\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  9\n",
      "training loss is:  2.736053705215454\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  10\n",
      "training loss is:  2.6372885704040527\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  11\n",
      "training loss is:  2.502014398574829\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  12\n",
      "training loss is:  2.3694138526916504\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  13\n",
      "training loss is:  3.1476993560791016\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  14\n",
      "training loss is:  3.5499813556671143\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  15\n",
      "training loss is:  3.7172226905822754\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  16\n",
      "training loss is:  2.8147027492523193\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  17\n",
      "training loss is:  3.1747570037841797\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  18\n",
      "training loss is:  3.7031805515289307\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  19\n",
      "training loss is:  3.5380403995513916\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  20\n",
      "training loss is:  3.223733901977539\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  21\n",
      "training loss is:  2.8403677940368652\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  0\n",
      "training loss is:  3.2580254077911377\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  1\n",
      "training loss is:  4.080802917480469\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  2\n",
      "training loss is:  2.5907955169677734\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  3\n",
      "training loss is:  5.2081828117370605\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  4\n",
      "training loss is:  3.3678250312805176\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  5\n",
      "training loss is:  3.4098939895629883\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  6\n",
      "training loss is:  2.747127056121826\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  7\n",
      "training loss is:  3.6977529525756836\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  8\n",
      "training loss is:  2.628976821899414\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  9\n",
      "training loss is:  2.3058433532714844\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  10\n",
      "training loss is:  3.9051785469055176\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  11\n",
      "training loss is:  2.1748340129852295\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  12\n",
      "training loss is:  3.4467544555664062\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  13\n",
      "training loss is:  2.823662757873535\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  14\n",
      "training loss is:  3.115401268005371\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  15\n",
      "training loss is:  3.0540878772735596\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  16\n",
      "training loss is:  2.659238338470459\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  17\n",
      "training loss is:  3.157811164855957\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  18\n",
      "training loss is:  2.967895269393921\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  19\n",
      "training loss is:  2.4894330501556396\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  20\n",
      "training loss is:  2.884206771850586\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  21\n",
      "training loss is:  2.8814096450805664\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  0\n",
      "training loss is:  4.033118724822998\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  1\n",
      "training loss is:  3.0855441093444824\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  2\n",
      "training loss is:  2.9459218978881836\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  3\n",
      "training loss is:  3.556124210357666\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  4\n",
      "training loss is:  3.849440574645996\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  5\n",
      "training loss is:  3.1477174758911133\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  6\n",
      "training loss is:  2.727534294128418\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  7\n",
      "training loss is:  3.283083200454712\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  8\n",
      "training loss is:  2.5034422874450684\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  9\n",
      "training loss is:  3.227524995803833\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  10\n",
      "training loss is:  3.5502710342407227\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  11\n",
      "training loss is:  2.7549548149108887\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  12\n",
      "training loss is:  5.228304862976074\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  13\n",
      "training loss is:  2.860562801361084\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  14\n",
      "training loss is:  3.0082051753997803\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  15\n",
      "training loss is:  2.37626576423645\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  16\n",
      "training loss is:  2.617957353591919\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  17\n",
      "training loss is:  2.5856757164001465\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  18\n",
      "training loss is:  2.6776506900787354\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  19\n",
      "training loss is:  2.9645509719848633\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  20\n",
      "training loss is:  3.2311296463012695\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  21\n",
      "training loss is:  2.9467599391937256\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  0\n",
      "training loss is:  3.050306797027588\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  1\n",
      "training loss is:  3.0535335540771484\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  2\n",
      "training loss is:  2.260561466217041\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  3\n",
      "training loss is:  2.8542182445526123\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  4\n",
      "training loss is:  2.6559853553771973\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  5\n",
      "training loss is:  3.2475826740264893\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  6\n",
      "training loss is:  2.9057345390319824\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  7\n",
      "training loss is:  2.9959423542022705\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  8\n",
      "training loss is:  4.056872367858887\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  9\n",
      "training loss is:  2.8895862102508545\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  10\n",
      "training loss is:  3.9836432933807373\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  11\n",
      "training loss is:  2.8795478343963623\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  12\n",
      "training loss is:  3.931840181350708\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  13\n",
      "training loss is:  2.444223403930664\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  14\n",
      "training loss is:  3.529409170150757\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  15\n",
      "training loss is:  2.481889486312866\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  16\n",
      "training loss is:  3.041029930114746\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  17\n",
      "training loss is:  2.8958616256713867\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  18\n",
      "training loss is:  4.740362167358398\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  19\n",
      "training loss is:  3.220278024673462\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  20\n",
      "training loss is:  2.6274304389953613\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  21\n",
      "training loss is:  3.435915946960449\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  0\n",
      "training loss is:  3.000816583633423\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  1\n",
      "training loss is:  5.651974678039551\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  2\n",
      "training loss is:  2.953554630279541\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  3\n",
      "training loss is:  3.191948652267456\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  4\n",
      "training loss is:  2.3103525638580322\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  5\n",
      "training loss is:  2.776153326034546\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  6\n",
      "training loss is:  2.9700934886932373\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  7\n",
      "training loss is:  2.566612958908081\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  8\n",
      "training loss is:  3.237133741378784\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  9\n",
      "training loss is:  3.050889492034912\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  10\n",
      "training loss is:  2.9807839393615723\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  11\n",
      "training loss is:  3.2592597007751465\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  12\n",
      "training loss is:  3.242480754852295\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  13\n",
      "training loss is:  2.7094779014587402\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  14\n",
      "training loss is:  2.9231040477752686\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  15\n",
      "training loss is:  3.068917751312256\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  16\n",
      "training loss is:  3.3100664615631104\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  17\n",
      "training loss is:  2.953212261199951\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  18\n",
      "training loss is:  2.868259906768799\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  19\n",
      "training loss is:  3.4428153038024902\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  0\n",
      "training loss is:  3.2441506385803223\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  0\n",
      "training loss is:  2.8132407665252686\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  0\n",
      "training loss is:  2.6532905101776123\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  0\n",
      "training loss is:  2.5979974269866943\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  0\n",
      "training loss is:  3.033543586730957\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  0\n",
      "training loss is:  2.671872138977051\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  0\n",
      "training loss is:  3.0337700843811035\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  0\n",
      "training loss is:  2.463167428970337\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  0\n",
      "training loss is:  2.4909894466400146\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  0\n",
      "training loss is:  3.224619150161743\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  0\n",
      "training loss is:  3.4694344997406006\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  0\n",
      "training loss is:  2.8967785835266113\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  0\n",
      "training loss is:  3.40934681892395\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  0\n",
      "training loss is:  2.5197510719299316\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  0\n",
      "training loss is:  3.0429086685180664\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  0\n",
      "training loss is:  3.022397756576538\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  0\n",
      "training loss is:  3.9949231147766113\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  0\n",
      "training loss is:  3.5689187049865723\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  0\n",
      "training loss is:  3.1913652420043945\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  0\n",
      "training loss is:  2.3030526638031006\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  0\n",
      "training loss is:  2.828080177307129\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  0\n",
      "training loss is:  2.8992605209350586\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  0\n",
      "training loss is:  5.3263630867004395\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  0\n",
      "training loss is:  2.877304792404175\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  0\n",
      "training loss is:  3.440049648284912\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  0\n",
      "training loss is:  3.5294437408447266\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  0\n",
      "training loss is:  3.395443916320801\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  0\n",
      "training loss is:  2.1474647521972656\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  0\n",
      "training loss is:  2.321274995803833\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  0\n",
      "training loss is:  2.530527353286743\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  0\n",
      "training loss is:  3.1285812854766846\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  0\n",
      "training loss is:  2.4593639373779297\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  0\n",
      "training loss is:  2.916926622390747\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  0\n",
      "training loss is:  3.3230795860290527\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  0\n",
      "training loss is:  3.233452796936035\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  0\n",
      "training loss is:  2.913851022720337\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  0\n",
      "training loss is:  3.0617640018463135\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  0\n",
      "training loss is:  3.3876965045928955\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  0\n",
      "training loss is:  2.830035924911499\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  0\n",
      "training loss is:  2.334036111831665\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  0\n",
      "training loss is:  2.9673912525177\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  0\n",
      "training loss is:  2.4189224243164062\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final model for nn:\n",
    "count = 0\n",
    "for epoch in range(100):\n",
    "    for step, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        print('Epoch:', epoch, '| Step: ', step)\n",
    "        \n",
    "        b_x = Variable(batch_x.float())\n",
    "        b_y = Variable(batch_y.float())\n",
    "        \n",
    "        prediction = net(b_x)\n",
    "        train_loss = loss_func(prediction, b_y)\n",
    "        print('training loss is: ',train_loss.data[0])\n",
    "        print('\\n')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if count+1 == 98 * 13:\n",
    "            break\n",
    "        \n",
    "        count +=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2b1ab2272f60>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final prediction\n",
    "final_prediction_y = net(x_test).data.numpy()\n",
    "true_y = y_test.data.numpy()\n",
    "plt.scatter(true_y, final_prediction_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc86V5f8Dd8gMgp9X//kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b1ab225ad68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('predicted difference in sentencing length')\n",
    "plt.ylabel('actual difference in sentencing length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is: 4.170378684997559\n",
      "mean absolute error is 1.4059722423553467\n"
     ]
    }
   ],
   "source": [
    "#final result avoid overfitting\n",
    "print('mean squared error is: {}'.format(mean_squared_error(true_y, final_prediction_y)))\n",
    "print('mean absolute error is {}'.format(mean_absolute_error(true_y, final_prediction_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#compare Neural Network result with GBR with its best parameters performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>Res_binary</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.100533</td>\n",
       "      <td>0.021049</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>-0.019895</td>\n",
       "      <td>-0.008439</td>\n",
       "      <td>0.018857</td>\n",
       "      <td>-0.024885</td>\n",
       "      <td>-0.014900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>-0.002709</td>\n",
       "      <td>0.018256</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>-0.016751</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.003150</td>\n",
       "      <td>-0.004419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>0.157620</td>\n",
       "      <td>-0.026298</td>\n",
       "      <td>-0.043520</td>\n",
       "      <td>-0.057402</td>\n",
       "      <td>-0.039799</td>\n",
       "      <td>0.026700</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>-0.012378</td>\n",
       "      <td>-0.011192</td>\n",
       "      <td>-0.021375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.016863</td>\n",
       "      <td>-0.008680</td>\n",
       "      <td>0.017289</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>-0.027626</td>\n",
       "      <td>-0.004125</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.141970</td>\n",
       "      <td>-0.026448</td>\n",
       "      <td>-0.080584</td>\n",
       "      <td>-0.054681</td>\n",
       "      <td>-0.016293</td>\n",
       "      <td>0.081749</td>\n",
       "      <td>-0.037343</td>\n",
       "      <td>0.017813</td>\n",
       "      <td>-0.010692</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>0.033197</td>\n",
       "      <td>-0.019105</td>\n",
       "      <td>0.019378</td>\n",
       "      <td>0.109131</td>\n",
       "      <td>0.060335</td>\n",
       "      <td>-0.017806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.150298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>0.190061</td>\n",
       "      <td>0.104727</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>0.053561</td>\n",
       "      <td>0.034232</td>\n",
       "      <td>0.072467</td>\n",
       "      <td>0.050863</td>\n",
       "      <td>-0.012928</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>0.014233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030851</td>\n",
       "      <td>-0.009701</td>\n",
       "      <td>0.009954</td>\n",
       "      <td>-0.044379</td>\n",
       "      <td>-0.006101</td>\n",
       "      <td>-0.011122</td>\n",
       "      <td>-0.019106</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.195514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6602</th>\n",
       "      <td>0.228796</td>\n",
       "      <td>-0.007014</td>\n",
       "      <td>0.070923</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>-0.030451</td>\n",
       "      <td>-0.000764</td>\n",
       "      <td>0.019736</td>\n",
       "      <td>-0.031986</td>\n",
       "      <td>-0.012740</td>\n",
       "      <td>0.044786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029470</td>\n",
       "      <td>-0.000433</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>-0.002859</td>\n",
       "      <td>0.019277</td>\n",
       "      <td>-0.010504</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>-0.016495</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.082692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>0.144139</td>\n",
       "      <td>0.128313</td>\n",
       "      <td>-0.015371</td>\n",
       "      <td>0.029075</td>\n",
       "      <td>-0.019189</td>\n",
       "      <td>-0.068175</td>\n",
       "      <td>-0.039146</td>\n",
       "      <td>0.038496</td>\n",
       "      <td>-0.071963</td>\n",
       "      <td>-0.031979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>-0.010152</td>\n",
       "      <td>0.026670</td>\n",
       "      <td>0.071666</td>\n",
       "      <td>0.026278</td>\n",
       "      <td>-0.038351</td>\n",
       "      <td>-0.005259</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.952032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3083</th>\n",
       "      <td>0.095873</td>\n",
       "      <td>-0.019261</td>\n",
       "      <td>0.031074</td>\n",
       "      <td>-0.017900</td>\n",
       "      <td>0.007718</td>\n",
       "      <td>-0.023413</td>\n",
       "      <td>0.016189</td>\n",
       "      <td>0.009415</td>\n",
       "      <td>-0.020744</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007099</td>\n",
       "      <td>-0.013895</td>\n",
       "      <td>-0.031551</td>\n",
       "      <td>-0.001493</td>\n",
       "      <td>-0.022240</td>\n",
       "      <td>0.003993</td>\n",
       "      <td>0.008809</td>\n",
       "      <td>0.011543</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.855352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388</th>\n",
       "      <td>0.195317</td>\n",
       "      <td>0.067205</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.048019</td>\n",
       "      <td>0.072217</td>\n",
       "      <td>0.043359</td>\n",
       "      <td>0.006027</td>\n",
       "      <td>-0.033956</td>\n",
       "      <td>0.021628</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014765</td>\n",
       "      <td>0.024269</td>\n",
       "      <td>-0.036191</td>\n",
       "      <td>-0.026400</td>\n",
       "      <td>-0.025293</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.067023</td>\n",
       "      <td>0.021221</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.731262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6609</th>\n",
       "      <td>0.150863</td>\n",
       "      <td>-0.070331</td>\n",
       "      <td>-0.008770</td>\n",
       "      <td>-0.027240</td>\n",
       "      <td>-0.048602</td>\n",
       "      <td>-0.031284</td>\n",
       "      <td>0.029797</td>\n",
       "      <td>0.171062</td>\n",
       "      <td>0.164740</td>\n",
       "      <td>-0.016277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.008336</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>-0.008048</td>\n",
       "      <td>0.021765</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.729401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7681</th>\n",
       "      <td>0.121016</td>\n",
       "      <td>-0.015463</td>\n",
       "      <td>-0.021101</td>\n",
       "      <td>-0.043039</td>\n",
       "      <td>0.038102</td>\n",
       "      <td>0.030395</td>\n",
       "      <td>0.007071</td>\n",
       "      <td>-0.025070</td>\n",
       "      <td>-0.005048</td>\n",
       "      <td>0.030557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>0.025401</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>-0.004134</td>\n",
       "      <td>0.008201</td>\n",
       "      <td>-0.019747</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.760434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>0.192685</td>\n",
       "      <td>-0.079692</td>\n",
       "      <td>-0.010976</td>\n",
       "      <td>-0.043918</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>-0.011606</td>\n",
       "      <td>0.040228</td>\n",
       "      <td>-0.014700</td>\n",
       "      <td>-0.017641</td>\n",
       "      <td>-0.035290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.060702</td>\n",
       "      <td>-0.067159</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>0.003783</td>\n",
       "      <td>-0.010586</td>\n",
       "      <td>-0.006428</td>\n",
       "      <td>-0.015603</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.973780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2204</th>\n",
       "      <td>0.078289</td>\n",
       "      <td>-0.006438</td>\n",
       "      <td>0.073342</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.032931</td>\n",
       "      <td>0.058917</td>\n",
       "      <td>0.017836</td>\n",
       "      <td>0.006734</td>\n",
       "      <td>-0.021162</td>\n",
       "      <td>0.020126</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001483</td>\n",
       "      <td>0.007011</td>\n",
       "      <td>-0.006419</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.034334</td>\n",
       "      <td>0.006056</td>\n",
       "      <td>-0.004555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.930957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8048</th>\n",
       "      <td>0.219425</td>\n",
       "      <td>-0.012205</td>\n",
       "      <td>0.059353</td>\n",
       "      <td>-0.049863</td>\n",
       "      <td>-0.019195</td>\n",
       "      <td>-0.069487</td>\n",
       "      <td>0.049369</td>\n",
       "      <td>0.040263</td>\n",
       "      <td>-0.002217</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002778</td>\n",
       "      <td>-0.010547</td>\n",
       "      <td>0.019876</td>\n",
       "      <td>-0.041036</td>\n",
       "      <td>0.026249</td>\n",
       "      <td>-0.002565</td>\n",
       "      <td>-0.024128</td>\n",
       "      <td>-0.006258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.636408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>0.127055</td>\n",
       "      <td>-0.019013</td>\n",
       "      <td>0.118781</td>\n",
       "      <td>0.054853</td>\n",
       "      <td>0.019987</td>\n",
       "      <td>0.042690</td>\n",
       "      <td>-0.073304</td>\n",
       "      <td>0.030028</td>\n",
       "      <td>-0.037629</td>\n",
       "      <td>0.060950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011336</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>-0.018203</td>\n",
       "      <td>-0.002083</td>\n",
       "      <td>0.004936</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>-0.016168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.308966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6458</th>\n",
       "      <td>0.099552</td>\n",
       "      <td>-0.028892</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>-0.051345</td>\n",
       "      <td>-0.017393</td>\n",
       "      <td>0.032032</td>\n",
       "      <td>-0.015352</td>\n",
       "      <td>0.030312</td>\n",
       "      <td>-0.018613</td>\n",
       "      <td>-0.025277</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008217</td>\n",
       "      <td>0.016194</td>\n",
       "      <td>0.012646</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>-0.009544</td>\n",
       "      <td>-0.008272</td>\n",
       "      <td>0.027868</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.531688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7425</th>\n",
       "      <td>0.133894</td>\n",
       "      <td>0.063301</td>\n",
       "      <td>-0.070460</td>\n",
       "      <td>-0.013220</td>\n",
       "      <td>-0.029290</td>\n",
       "      <td>0.049852</td>\n",
       "      <td>-0.025099</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>-0.023100</td>\n",
       "      <td>0.043773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>0.006733</td>\n",
       "      <td>0.031536</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>-0.021649</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.672693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>0.155718</td>\n",
       "      <td>-0.111228</td>\n",
       "      <td>-0.120305</td>\n",
       "      <td>0.155237</td>\n",
       "      <td>-0.054127</td>\n",
       "      <td>-0.000899</td>\n",
       "      <td>-0.021407</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>0.038929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.019592</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0.022248</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>-0.005659</td>\n",
       "      <td>0.016490</td>\n",
       "      <td>0.018592</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.309356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>0.166352</td>\n",
       "      <td>-0.039814</td>\n",
       "      <td>-0.113195</td>\n",
       "      <td>0.065394</td>\n",
       "      <td>-0.062233</td>\n",
       "      <td>0.074337</td>\n",
       "      <td>-0.016653</td>\n",
       "      <td>0.031258</td>\n",
       "      <td>-0.012603</td>\n",
       "      <td>0.029895</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004737</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>-0.025158</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>-0.021842</td>\n",
       "      <td>0.013028</td>\n",
       "      <td>-0.014359</td>\n",
       "      <td>0.002419</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.008592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>0.048166</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>-0.007364</td>\n",
       "      <td>0.022536</td>\n",
       "      <td>-0.003720</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>-0.037234</td>\n",
       "      <td>0.037657</td>\n",
       "      <td>0.008604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006839</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>-0.011284</td>\n",
       "      <td>-0.010500</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>-0.002148</td>\n",
       "      <td>0.008908</td>\n",
       "      <td>-0.002170</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.779555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3336</th>\n",
       "      <td>0.156574</td>\n",
       "      <td>0.079739</td>\n",
       "      <td>0.038049</td>\n",
       "      <td>0.008960</td>\n",
       "      <td>-0.038234</td>\n",
       "      <td>0.006534</td>\n",
       "      <td>-0.033062</td>\n",
       "      <td>-0.010756</td>\n",
       "      <td>-0.012152</td>\n",
       "      <td>0.025814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.008616</td>\n",
       "      <td>0.013974</td>\n",
       "      <td>0.030070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.660848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8042</th>\n",
       "      <td>0.133434</td>\n",
       "      <td>-0.052769</td>\n",
       "      <td>-0.025043</td>\n",
       "      <td>-0.065400</td>\n",
       "      <td>-0.022372</td>\n",
       "      <td>0.026441</td>\n",
       "      <td>0.023845</td>\n",
       "      <td>0.003709</td>\n",
       "      <td>-0.007114</td>\n",
       "      <td>-0.021151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017191</td>\n",
       "      <td>-0.012561</td>\n",
       "      <td>0.008849</td>\n",
       "      <td>-0.012236</td>\n",
       "      <td>0.032461</td>\n",
       "      <td>-0.007867</td>\n",
       "      <td>0.027316</td>\n",
       "      <td>-0.017433</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.285322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>0.095227</td>\n",
       "      <td>-0.062701</td>\n",
       "      <td>-0.040025</td>\n",
       "      <td>0.088440</td>\n",
       "      <td>-0.033644</td>\n",
       "      <td>-0.010209</td>\n",
       "      <td>-0.005660</td>\n",
       "      <td>-0.006937</td>\n",
       "      <td>-0.005667</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026142</td>\n",
       "      <td>-0.006730</td>\n",
       "      <td>0.033593</td>\n",
       "      <td>-0.040031</td>\n",
       "      <td>0.010658</td>\n",
       "      <td>0.021819</td>\n",
       "      <td>-0.051881</td>\n",
       "      <td>-0.015041</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.535885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6729</th>\n",
       "      <td>0.131985</td>\n",
       "      <td>-0.026506</td>\n",
       "      <td>0.025754</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.038123</td>\n",
       "      <td>-0.013229</td>\n",
       "      <td>0.044252</td>\n",
       "      <td>-0.052202</td>\n",
       "      <td>-0.025072</td>\n",
       "      <td>0.162762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006310</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>-0.011159</td>\n",
       "      <td>-0.009929</td>\n",
       "      <td>-0.023733</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>-0.022472</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.259840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>0.206417</td>\n",
       "      <td>0.045811</td>\n",
       "      <td>0.030511</td>\n",
       "      <td>0.019107</td>\n",
       "      <td>0.074435</td>\n",
       "      <td>0.067970</td>\n",
       "      <td>-0.032415</td>\n",
       "      <td>-0.040292</td>\n",
       "      <td>0.029273</td>\n",
       "      <td>0.034987</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016453</td>\n",
       "      <td>-0.022294</td>\n",
       "      <td>-0.000502</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.013769</td>\n",
       "      <td>-0.019778</td>\n",
       "      <td>0.040352</td>\n",
       "      <td>0.008314</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.490622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4243</th>\n",
       "      <td>0.075323</td>\n",
       "      <td>-0.023357</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>-0.010337</td>\n",
       "      <td>0.014655</td>\n",
       "      <td>-0.013482</td>\n",
       "      <td>0.016483</td>\n",
       "      <td>-0.007582</td>\n",
       "      <td>-0.009490</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008359</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.006746</td>\n",
       "      <td>0.010589</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>-0.000499</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.179164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6580</th>\n",
       "      <td>0.139106</td>\n",
       "      <td>0.043822</td>\n",
       "      <td>-0.021138</td>\n",
       "      <td>-0.001695</td>\n",
       "      <td>-0.010085</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.013402</td>\n",
       "      <td>-0.035239</td>\n",
       "      <td>0.023651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030917</td>\n",
       "      <td>-0.007445</td>\n",
       "      <td>-0.012201</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>-0.007424</td>\n",
       "      <td>0.030796</td>\n",
       "      <td>-0.014016</td>\n",
       "      <td>0.015896</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.621066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8419</th>\n",
       "      <td>0.143460</td>\n",
       "      <td>-0.017018</td>\n",
       "      <td>-0.030724</td>\n",
       "      <td>0.050332</td>\n",
       "      <td>0.006545</td>\n",
       "      <td>-0.042576</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>0.020122</td>\n",
       "      <td>-0.008640</td>\n",
       "      <td>-0.036044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>-0.006375</td>\n",
       "      <td>-0.006452</td>\n",
       "      <td>-0.005168</td>\n",
       "      <td>-0.006065</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.019496</td>\n",
       "      <td>0.014180</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.010986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>0.134029</td>\n",
       "      <td>-0.010522</td>\n",
       "      <td>0.203619</td>\n",
       "      <td>0.078687</td>\n",
       "      <td>0.039703</td>\n",
       "      <td>0.071488</td>\n",
       "      <td>-0.106097</td>\n",
       "      <td>0.050342</td>\n",
       "      <td>-0.040677</td>\n",
       "      <td>0.107675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063421</td>\n",
       "      <td>0.005740</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>-0.051274</td>\n",
       "      <td>0.074691</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.009366</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.421632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492</th>\n",
       "      <td>0.225110</td>\n",
       "      <td>-0.073467</td>\n",
       "      <td>0.185777</td>\n",
       "      <td>-0.015088</td>\n",
       "      <td>-0.072543</td>\n",
       "      <td>0.011396</td>\n",
       "      <td>-0.068093</td>\n",
       "      <td>0.025852</td>\n",
       "      <td>-0.012643</td>\n",
       "      <td>-0.091027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028269</td>\n",
       "      <td>0.025741</td>\n",
       "      <td>0.050546</td>\n",
       "      <td>-0.004424</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.017495</td>\n",
       "      <td>0.072654</td>\n",
       "      <td>-0.019630</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.592588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6632</th>\n",
       "      <td>0.200990</td>\n",
       "      <td>0.043904</td>\n",
       "      <td>0.074057</td>\n",
       "      <td>-0.019388</td>\n",
       "      <td>-0.130346</td>\n",
       "      <td>-0.010877</td>\n",
       "      <td>-0.033402</td>\n",
       "      <td>-0.147529</td>\n",
       "      <td>0.120269</td>\n",
       "      <td>-0.023467</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006956</td>\n",
       "      <td>0.006164</td>\n",
       "      <td>-0.010452</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>0.026309</td>\n",
       "      <td>-0.006202</td>\n",
       "      <td>-0.018552</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.191471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>0.150534</td>\n",
       "      <td>-0.015949</td>\n",
       "      <td>-0.022612</td>\n",
       "      <td>-0.033062</td>\n",
       "      <td>-0.028675</td>\n",
       "      <td>0.003069</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>0.009369</td>\n",
       "      <td>-0.037090</td>\n",
       "      <td>-0.010817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024745</td>\n",
       "      <td>-0.030275</td>\n",
       "      <td>-0.013718</td>\n",
       "      <td>-0.057483</td>\n",
       "      <td>0.013258</td>\n",
       "      <td>0.034237</td>\n",
       "      <td>0.014637</td>\n",
       "      <td>-0.011492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.497116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>0.145553</td>\n",
       "      <td>-0.040859</td>\n",
       "      <td>-0.008375</td>\n",
       "      <td>-0.022534</td>\n",
       "      <td>0.036042</td>\n",
       "      <td>-0.048114</td>\n",
       "      <td>0.034963</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>-0.016482</td>\n",
       "      <td>-0.001047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067431</td>\n",
       "      <td>-0.041877</td>\n",
       "      <td>-0.040731</td>\n",
       "      <td>0.027509</td>\n",
       "      <td>-0.036932</td>\n",
       "      <td>-0.019291</td>\n",
       "      <td>-0.026647</td>\n",
       "      <td>0.016609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.280298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6651</th>\n",
       "      <td>0.229772</td>\n",
       "      <td>-0.026551</td>\n",
       "      <td>0.012175</td>\n",
       "      <td>-0.037859</td>\n",
       "      <td>0.043613</td>\n",
       "      <td>-0.039462</td>\n",
       "      <td>0.083772</td>\n",
       "      <td>-0.033045</td>\n",
       "      <td>-0.005726</td>\n",
       "      <td>0.022710</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022489</td>\n",
       "      <td>0.029329</td>\n",
       "      <td>0.028068</td>\n",
       "      <td>-0.004969</td>\n",
       "      <td>-0.018100</td>\n",
       "      <td>-0.009231</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>-0.003796</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.077597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>0.263038</td>\n",
       "      <td>0.057229</td>\n",
       "      <td>-0.069019</td>\n",
       "      <td>-0.047101</td>\n",
       "      <td>-0.090173</td>\n",
       "      <td>-0.037458</td>\n",
       "      <td>-0.020393</td>\n",
       "      <td>0.023741</td>\n",
       "      <td>-0.068383</td>\n",
       "      <td>-0.055859</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.004621</td>\n",
       "      <td>0.027975</td>\n",
       "      <td>0.078964</td>\n",
       "      <td>0.004311</td>\n",
       "      <td>-0.048937</td>\n",
       "      <td>-0.009980</td>\n",
       "      <td>-0.022252</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.241403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6015</th>\n",
       "      <td>0.119530</td>\n",
       "      <td>-0.067560</td>\n",
       "      <td>-0.067543</td>\n",
       "      <td>0.055900</td>\n",
       "      <td>-0.041115</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>-0.012338</td>\n",
       "      <td>-0.003353</td>\n",
       "      <td>-0.010930</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007693</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>0.014256</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>-0.017873</td>\n",
       "      <td>0.005043</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.388459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1770</th>\n",
       "      <td>0.171899</td>\n",
       "      <td>0.061994</td>\n",
       "      <td>0.059988</td>\n",
       "      <td>0.010991</td>\n",
       "      <td>-0.085747</td>\n",
       "      <td>-0.043814</td>\n",
       "      <td>-0.019584</td>\n",
       "      <td>-0.029624</td>\n",
       "      <td>-0.017837</td>\n",
       "      <td>-0.002738</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042630</td>\n",
       "      <td>-0.039309</td>\n",
       "      <td>0.018034</td>\n",
       "      <td>-0.006456</td>\n",
       "      <td>-0.051591</td>\n",
       "      <td>0.004748</td>\n",
       "      <td>-0.004811</td>\n",
       "      <td>-0.010484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.135797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>0.100591</td>\n",
       "      <td>-0.016462</td>\n",
       "      <td>0.031444</td>\n",
       "      <td>-0.009137</td>\n",
       "      <td>0.026286</td>\n",
       "      <td>-0.061675</td>\n",
       "      <td>0.061389</td>\n",
       "      <td>-0.010583</td>\n",
       "      <td>-0.035377</td>\n",
       "      <td>0.077385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034511</td>\n",
       "      <td>0.097556</td>\n",
       "      <td>-0.008341</td>\n",
       "      <td>-0.020553</td>\n",
       "      <td>0.007398</td>\n",
       "      <td>0.003272</td>\n",
       "      <td>-0.023717</td>\n",
       "      <td>0.088225</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.258852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>0.207419</td>\n",
       "      <td>0.050917</td>\n",
       "      <td>-0.029363</td>\n",
       "      <td>-0.007535</td>\n",
       "      <td>-0.040500</td>\n",
       "      <td>-0.007163</td>\n",
       "      <td>-0.024553</td>\n",
       "      <td>-0.057764</td>\n",
       "      <td>0.018950</td>\n",
       "      <td>-0.041811</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003232</td>\n",
       "      <td>0.019732</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>-0.021312</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>-0.003630</td>\n",
       "      <td>0.007919</td>\n",
       "      <td>-0.024178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.343981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5893</th>\n",
       "      <td>0.170570</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>-0.011597</td>\n",
       "      <td>0.056934</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>-0.007719</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>-0.026571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.022090</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>-0.006344</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>-0.019562</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.166765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5165</th>\n",
       "      <td>0.121275</td>\n",
       "      <td>-0.058882</td>\n",
       "      <td>0.038314</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>0.026572</td>\n",
       "      <td>0.012267</td>\n",
       "      <td>-0.034249</td>\n",
       "      <td>0.023525</td>\n",
       "      <td>-0.011961</td>\n",
       "      <td>-0.026437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.067252</td>\n",
       "      <td>0.051149</td>\n",
       "      <td>0.024296</td>\n",
       "      <td>-0.040153</td>\n",
       "      <td>-0.031555</td>\n",
       "      <td>-0.100400</td>\n",
       "      <td>-0.002002</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.959885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>0.089379</td>\n",
       "      <td>-0.061137</td>\n",
       "      <td>-0.053345</td>\n",
       "      <td>0.090525</td>\n",
       "      <td>-0.017230</td>\n",
       "      <td>-0.010795</td>\n",
       "      <td>-0.014450</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.003506</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013032</td>\n",
       "      <td>-0.004006</td>\n",
       "      <td>0.021961</td>\n",
       "      <td>-0.003272</td>\n",
       "      <td>-0.001876</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>-0.009662</td>\n",
       "      <td>-0.005003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.117241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>0.126746</td>\n",
       "      <td>-0.017160</td>\n",
       "      <td>0.075593</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>-0.000182</td>\n",
       "      <td>0.022183</td>\n",
       "      <td>-0.040325</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>-0.007817</td>\n",
       "      <td>0.020139</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013606</td>\n",
       "      <td>-0.013328</td>\n",
       "      <td>-0.030904</td>\n",
       "      <td>-0.007405</td>\n",
       "      <td>0.029483</td>\n",
       "      <td>0.030742</td>\n",
       "      <td>0.020167</td>\n",
       "      <td>-0.009032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.259774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8019</th>\n",
       "      <td>0.226011</td>\n",
       "      <td>-0.100552</td>\n",
       "      <td>-0.088662</td>\n",
       "      <td>0.085863</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>-0.022861</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>-0.086581</td>\n",
       "      <td>0.055967</td>\n",
       "      <td>0.058681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>-0.014687</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>-0.018985</td>\n",
       "      <td>-0.001826</td>\n",
       "      <td>0.009792</td>\n",
       "      <td>-0.016440</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.089598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2580</th>\n",
       "      <td>0.180950</td>\n",
       "      <td>-0.019347</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>-0.005292</td>\n",
       "      <td>0.088634</td>\n",
       "      <td>0.019674</td>\n",
       "      <td>0.016461</td>\n",
       "      <td>-0.028095</td>\n",
       "      <td>0.006273</td>\n",
       "      <td>0.018213</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015798</td>\n",
       "      <td>-0.046812</td>\n",
       "      <td>-0.038905</td>\n",
       "      <td>-0.012914</td>\n",
       "      <td>0.038330</td>\n",
       "      <td>0.009677</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>0.014102</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.094150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>0.142842</td>\n",
       "      <td>0.042324</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>-0.030485</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>-0.027680</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>0.031616</td>\n",
       "      <td>-0.037503</td>\n",
       "      <td>-0.040318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007355</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>-0.001818</td>\n",
       "      <td>-0.006830</td>\n",
       "      <td>-0.016023</td>\n",
       "      <td>0.024806</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.027052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.960389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1285</th>\n",
       "      <td>0.110946</td>\n",
       "      <td>-0.083216</td>\n",
       "      <td>-0.058415</td>\n",
       "      <td>0.113529</td>\n",
       "      <td>-0.040173</td>\n",
       "      <td>-0.037633</td>\n",
       "      <td>-0.002428</td>\n",
       "      <td>0.085966</td>\n",
       "      <td>0.107677</td>\n",
       "      <td>0.052169</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006815</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>-0.002800</td>\n",
       "      <td>0.003477</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>0.003302</td>\n",
       "      <td>0.021909</td>\n",
       "      <td>-0.003753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.157542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>0.170574</td>\n",
       "      <td>0.057503</td>\n",
       "      <td>0.028613</td>\n",
       "      <td>-0.019403</td>\n",
       "      <td>-0.070091</td>\n",
       "      <td>-0.036897</td>\n",
       "      <td>-0.025272</td>\n",
       "      <td>-0.075958</td>\n",
       "      <td>0.037816</td>\n",
       "      <td>-0.020132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031579</td>\n",
       "      <td>-0.036450</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>-0.004782</td>\n",
       "      <td>-0.004364</td>\n",
       "      <td>0.004689</td>\n",
       "      <td>-0.003069</td>\n",
       "      <td>-0.011550</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.634144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>0.075246</td>\n",
       "      <td>-0.041916</td>\n",
       "      <td>0.029104</td>\n",
       "      <td>-0.023082</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>-0.008412</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>-0.007032</td>\n",
       "      <td>-0.040920</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>-0.000551</td>\n",
       "      <td>-0.012864</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.008431</td>\n",
       "      <td>-0.032602</td>\n",
       "      <td>-0.024878</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.059720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7840</th>\n",
       "      <td>0.093683</td>\n",
       "      <td>-0.009891</td>\n",
       "      <td>-0.017496</td>\n",
       "      <td>-0.012434</td>\n",
       "      <td>-0.021385</td>\n",
       "      <td>-0.010213</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>-0.020052</td>\n",
       "      <td>0.010993</td>\n",
       "      <td>-0.018660</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007595</td>\n",
       "      <td>-0.003466</td>\n",
       "      <td>-0.007109</td>\n",
       "      <td>-0.003492</td>\n",
       "      <td>-0.011104</td>\n",
       "      <td>0.002459</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.598694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5423</th>\n",
       "      <td>0.106609</td>\n",
       "      <td>-0.032434</td>\n",
       "      <td>-0.029281</td>\n",
       "      <td>0.022449</td>\n",
       "      <td>0.034985</td>\n",
       "      <td>-0.027256</td>\n",
       "      <td>-0.001282</td>\n",
       "      <td>0.014846</td>\n",
       "      <td>0.010605</td>\n",
       "      <td>-0.034964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002386</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>-0.003245</td>\n",
       "      <td>-0.000925</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>-0.004247</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.499407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>0.068778</td>\n",
       "      <td>-0.027130</td>\n",
       "      <td>-0.023937</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>0.029374</td>\n",
       "      <td>-0.008482</td>\n",
       "      <td>0.004273</td>\n",
       "      <td>-0.018407</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.023776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>-0.006046</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>-0.002953</td>\n",
       "      <td>-0.019386</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>0.006616</td>\n",
       "      <td>-0.009521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.938785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>0.118567</td>\n",
       "      <td>-0.085583</td>\n",
       "      <td>-0.083728</td>\n",
       "      <td>0.131316</td>\n",
       "      <td>-0.042081</td>\n",
       "      <td>-0.017253</td>\n",
       "      <td>-0.015197</td>\n",
       "      <td>-0.004842</td>\n",
       "      <td>0.005362</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011873</td>\n",
       "      <td>0.002484</td>\n",
       "      <td>0.007547</td>\n",
       "      <td>-0.021241</td>\n",
       "      <td>0.018853</td>\n",
       "      <td>0.026568</td>\n",
       "      <td>-0.008351</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.049534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7871</th>\n",
       "      <td>0.230431</td>\n",
       "      <td>0.109053</td>\n",
       "      <td>-0.031787</td>\n",
       "      <td>-0.046402</td>\n",
       "      <td>-0.122054</td>\n",
       "      <td>-0.021126</td>\n",
       "      <td>-0.023374</td>\n",
       "      <td>-0.082508</td>\n",
       "      <td>0.091017</td>\n",
       "      <td>0.016224</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039014</td>\n",
       "      <td>-0.013844</td>\n",
       "      <td>-0.014660</td>\n",
       "      <td>0.040636</td>\n",
       "      <td>0.027237</td>\n",
       "      <td>-0.024434</td>\n",
       "      <td>-0.011717</td>\n",
       "      <td>0.010606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.176311</td>\n",
       "      <td>0.021928</td>\n",
       "      <td>0.087048</td>\n",
       "      <td>0.090134</td>\n",
       "      <td>0.033507</td>\n",
       "      <td>0.066389</td>\n",
       "      <td>0.002460</td>\n",
       "      <td>0.057933</td>\n",
       "      <td>-0.058886</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023853</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.003201</td>\n",
       "      <td>0.041810</td>\n",
       "      <td>-0.010399</td>\n",
       "      <td>0.033445</td>\n",
       "      <td>-0.021412</td>\n",
       "      <td>-0.016606</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2.012394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>0.081245</td>\n",
       "      <td>0.033144</td>\n",
       "      <td>-0.040207</td>\n",
       "      <td>-0.006717</td>\n",
       "      <td>-0.043247</td>\n",
       "      <td>-0.008837</td>\n",
       "      <td>-0.008665</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>-0.001934</td>\n",
       "      <td>-0.001078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009445</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>-0.003719</td>\n",
       "      <td>0.010614</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.006495</td>\n",
       "      <td>-0.016290</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.219812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7592</th>\n",
       "      <td>0.226623</td>\n",
       "      <td>0.045655</td>\n",
       "      <td>-0.071834</td>\n",
       "      <td>-0.124387</td>\n",
       "      <td>-0.045221</td>\n",
       "      <td>0.198179</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>-0.036253</td>\n",
       "      <td>0.026222</td>\n",
       "      <td>0.091691</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004555</td>\n",
       "      <td>0.068639</td>\n",
       "      <td>0.032621</td>\n",
       "      <td>0.010584</td>\n",
       "      <td>0.011506</td>\n",
       "      <td>-0.002246</td>\n",
       "      <td>-0.018053</td>\n",
       "      <td>-0.050417</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.990451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>0.179825</td>\n",
       "      <td>0.042677</td>\n",
       "      <td>-0.066829</td>\n",
       "      <td>-0.029672</td>\n",
       "      <td>-0.053329</td>\n",
       "      <td>0.044669</td>\n",
       "      <td>-0.002332</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>-0.015284</td>\n",
       "      <td>-0.001378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011263</td>\n",
       "      <td>-0.013241</td>\n",
       "      <td>-0.005535</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>-0.020507</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.965008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>0.203415</td>\n",
       "      <td>-0.040337</td>\n",
       "      <td>-0.103334</td>\n",
       "      <td>-0.008877</td>\n",
       "      <td>-0.019432</td>\n",
       "      <td>0.007675</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>-0.003018</td>\n",
       "      <td>-0.015368</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034323</td>\n",
       "      <td>-0.022540</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>-0.004306</td>\n",
       "      <td>-0.020601</td>\n",
       "      <td>-0.032909</td>\n",
       "      <td>-0.049230</td>\n",
       "      <td>0.026367</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.332651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4112</th>\n",
       "      <td>0.193762</td>\n",
       "      <td>0.106815</td>\n",
       "      <td>0.014803</td>\n",
       "      <td>0.105639</td>\n",
       "      <td>-0.021875</td>\n",
       "      <td>0.068985</td>\n",
       "      <td>0.113936</td>\n",
       "      <td>0.041517</td>\n",
       "      <td>-0.031917</td>\n",
       "      <td>-0.054189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073855</td>\n",
       "      <td>-0.014096</td>\n",
       "      <td>0.058849</td>\n",
       "      <td>0.097044</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.050074</td>\n",
       "      <td>-0.006443</td>\n",
       "      <td>-0.045955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-3.666103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6626</th>\n",
       "      <td>0.088230</td>\n",
       "      <td>-0.012538</td>\n",
       "      <td>0.070084</td>\n",
       "      <td>0.013664</td>\n",
       "      <td>-0.003932</td>\n",
       "      <td>0.017838</td>\n",
       "      <td>-0.040979</td>\n",
       "      <td>0.015762</td>\n",
       "      <td>-0.008428</td>\n",
       "      <td>-0.010965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003455</td>\n",
       "      <td>0.007696</td>\n",
       "      <td>0.013849</td>\n",
       "      <td>-0.009448</td>\n",
       "      <td>-0.007368</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>-0.009651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.499225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5533 rows Ã— 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "2072  0.100533  0.021049  0.007305  0.001948  0.001179 -0.019895 -0.008439   \n",
       "7052  0.157620 -0.026298 -0.043520 -0.057402 -0.039799  0.026700  0.008013   \n",
       "1582  0.141970 -0.026448 -0.080584 -0.054681 -0.016293  0.081749 -0.037343   \n",
       "2054  0.190061  0.104727  0.008652  0.053561  0.034232  0.072467  0.050863   \n",
       "6602  0.228796 -0.007014  0.070923 -0.003444 -0.030451 -0.000764  0.019736   \n",
       "7532  0.144139  0.128313 -0.015371  0.029075 -0.019189 -0.068175 -0.039146   \n",
       "3083  0.095873 -0.019261  0.031074 -0.017900  0.007718 -0.023413  0.016189   \n",
       "2388  0.195317  0.067205  0.000734  0.048019  0.072217  0.043359  0.006027   \n",
       "6609  0.150863 -0.070331 -0.008770 -0.027240 -0.048602 -0.031284  0.029797   \n",
       "7681  0.121016 -0.015463 -0.021101 -0.043039  0.038102  0.030395  0.007071   \n",
       "4673  0.192685 -0.079692 -0.010976 -0.043918  0.003020 -0.011606  0.040228   \n",
       "2204  0.078289 -0.006438  0.073342  0.056635  0.032931  0.058917  0.017836   \n",
       "8048  0.219425 -0.012205  0.059353 -0.049863 -0.019195 -0.069487  0.049369   \n",
       "5220  0.127055 -0.019013  0.118781  0.054853  0.019987  0.042690 -0.073304   \n",
       "6458  0.099552 -0.028892  0.015528 -0.051345 -0.017393  0.032032 -0.015352   \n",
       "7425  0.133894  0.063301 -0.070460 -0.013220 -0.029290  0.049852 -0.025099   \n",
       "2836  0.155718 -0.111228 -0.120305  0.155237 -0.054127 -0.000899 -0.021407   \n",
       "914   0.166352 -0.039814 -0.113195  0.065394 -0.062233  0.074337 -0.016653   \n",
       "8324  0.048166 -0.007195  0.012581 -0.007364  0.022536 -0.003720  0.002895   \n",
       "3336  0.156574  0.079739  0.038049  0.008960 -0.038234  0.006534 -0.033062   \n",
       "8042  0.133434 -0.052769 -0.025043 -0.065400 -0.022372  0.026441  0.023845   \n",
       "3784  0.095227 -0.062701 -0.040025  0.088440 -0.033644 -0.010209 -0.005660   \n",
       "6729  0.131985 -0.026506  0.025754  0.001737  0.038123 -0.013229  0.044252   \n",
       "338   0.206417  0.045811  0.030511  0.019107  0.074435  0.067970 -0.032415   \n",
       "4243  0.075323 -0.023357  0.000669 -0.010337  0.014655 -0.013482  0.016483   \n",
       "6580  0.139106  0.043822 -0.021138 -0.001695 -0.010085  0.014545  0.001627   \n",
       "8419  0.143460 -0.017018 -0.030724  0.050332  0.006545 -0.042576 -0.002451   \n",
       "2787  0.134029 -0.010522  0.203619  0.078687  0.039703  0.071488 -0.106097   \n",
       "3492  0.225110 -0.073467  0.185777 -0.015088 -0.072543  0.011396 -0.068093   \n",
       "6632  0.200990  0.043904  0.074057 -0.019388 -0.130346 -0.010877 -0.033402   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1615  0.150534 -0.015949 -0.022612 -0.033062 -0.028675  0.003069  0.009518   \n",
       "4843  0.145553 -0.040859 -0.008375 -0.022534  0.036042 -0.048114  0.034963   \n",
       "6651  0.229772 -0.026551  0.012175 -0.037859  0.043613 -0.039462  0.083772   \n",
       "1486  0.263038  0.057229 -0.069019 -0.047101 -0.090173 -0.037458 -0.020393   \n",
       "6015  0.119530 -0.067560 -0.067543  0.055900 -0.041115  0.008681 -0.012338   \n",
       "1770  0.171899  0.061994  0.059988  0.010991 -0.085747 -0.043814 -0.019584   \n",
       "3076  0.100591 -0.016462  0.031444 -0.009137  0.026286 -0.061675  0.061389   \n",
       "2154  0.207419  0.050917 -0.029363 -0.007535 -0.040500 -0.007163 -0.024553   \n",
       "5893  0.170570 -0.015747  0.001329 -0.011597  0.056934  0.014344  0.006773   \n",
       "5165  0.121275 -0.058882  0.038314 -0.009521  0.026572  0.012267 -0.034249   \n",
       "2260  0.089379 -0.061137 -0.053345  0.090525 -0.017230 -0.010795 -0.014450   \n",
       "5171  0.126746 -0.017160  0.075593  0.001232 -0.000182  0.022183 -0.040325   \n",
       "8019  0.226011 -0.100552 -0.088662  0.085863  0.125240 -0.022861  0.003415   \n",
       "2580  0.180950 -0.019347  0.001499 -0.005292  0.088634  0.019674  0.016461   \n",
       "767   0.142842  0.042324 -0.004093 -0.030485  0.004498 -0.027680 -0.013267   \n",
       "1285  0.110946 -0.083216 -0.058415  0.113529 -0.040173 -0.037633 -0.002428   \n",
       "4141  0.170574  0.057503  0.028613 -0.019403 -0.070091 -0.036897 -0.025272   \n",
       "512   0.075246 -0.041916  0.029104 -0.023082  0.007251 -0.008412 -0.003998   \n",
       "7840  0.093683 -0.009891 -0.017496 -0.012434 -0.021385 -0.010213  0.002468   \n",
       "5423  0.106609 -0.032434 -0.029281  0.022449  0.034985 -0.027256 -0.001282   \n",
       "7568  0.068778 -0.027130 -0.023937  0.024811  0.029374 -0.008482  0.004273   \n",
       "1189  0.118567 -0.085583 -0.083728  0.131316 -0.042081 -0.017253 -0.015197   \n",
       "7871  0.230431  0.109053 -0.031787 -0.046402 -0.122054 -0.021126 -0.023374   \n",
       "407   0.176311  0.021928  0.087048  0.090134  0.033507  0.066389  0.002460   \n",
       "1263  0.081245  0.033144 -0.040207 -0.006717 -0.043247 -0.008837 -0.008665   \n",
       "7592  0.226623  0.045655 -0.071834 -0.124387 -0.045221  0.198179  0.003131   \n",
       "3151  0.179825  0.042677 -0.066829 -0.029672 -0.053329  0.044669 -0.002332   \n",
       "6672  0.203415 -0.040337 -0.103334 -0.008877 -0.019432  0.007675  0.004049   \n",
       "4112  0.193762  0.106815  0.014803  0.105639 -0.021875  0.068985  0.113936   \n",
       "6626  0.088230 -0.012538  0.070084  0.013664 -0.003932  0.017838 -0.040979   \n",
       "\n",
       "             7         8         9    ...           72        73        74  \\\n",
       "2072  0.018857 -0.024885 -0.014900    ...     0.007469 -0.002709  0.018256   \n",
       "7052 -0.012378 -0.011192 -0.021375    ...     0.003264  0.016863 -0.008680   \n",
       "1582  0.017813 -0.010692 -0.000912    ...     0.018319  0.009916  0.033197   \n",
       "2054 -0.012928 -0.008381  0.014233    ...     0.030851 -0.009701  0.009954   \n",
       "6602 -0.031986 -0.012740  0.044786    ...     0.029470 -0.000433  0.028144   \n",
       "7532  0.038496 -0.071963 -0.031979    ...     0.010524 -0.010152  0.026670   \n",
       "3083  0.009415 -0.020744  0.003223    ...     0.007099 -0.013895 -0.031551   \n",
       "2388 -0.033956  0.021628 -0.000014    ...     0.014765  0.024269 -0.036191   \n",
       "6609  0.171062  0.164740 -0.016277    ...     0.000865  0.008336  0.016944   \n",
       "7681 -0.025070 -0.005048  0.030557    ...     0.005886  0.025401  0.005997   \n",
       "4673 -0.014700 -0.017641 -0.035290    ...     0.060702 -0.067159  0.020219   \n",
       "2204  0.006734 -0.021162  0.020126    ...    -0.001483  0.007011 -0.006419   \n",
       "8048  0.040263 -0.002217  0.012888    ...    -0.002778 -0.010547  0.019876   \n",
       "5220  0.030028 -0.037629  0.060950    ...    -0.011336  0.016546 -0.018203   \n",
       "6458  0.030312 -0.018613 -0.025277    ...    -0.008217  0.016194  0.012646   \n",
       "7425  0.010667 -0.023100  0.043773    ...     0.004280  0.005711  0.007642   \n",
       "2836  0.001138 -0.001456  0.038929    ...     0.007752  0.019592  0.006083   \n",
       "914   0.031258 -0.012603  0.029895    ...    -0.004737  0.024457 -0.025158   \n",
       "8324 -0.037234  0.037657  0.008604    ...    -0.006839  0.007527 -0.011284   \n",
       "3336 -0.010756 -0.012152  0.025814    ...     0.038556  0.010221  0.011157   \n",
       "8042  0.003709 -0.007114 -0.021151    ...    -0.017191 -0.012561  0.008849   \n",
       "3784 -0.006937 -0.005667  0.005161    ...    -0.026142 -0.006730  0.033593   \n",
       "6729 -0.052202 -0.025072  0.162762    ...    -0.006310  0.003822 -0.011159   \n",
       "338  -0.040292  0.029273  0.034987    ...    -0.016453 -0.022294 -0.000502   \n",
       "4243 -0.007582 -0.009490  0.001605    ...    -0.008359  0.000030  0.002106   \n",
       "6580  0.013402 -0.035239  0.023651    ...     0.030917 -0.007445 -0.012201   \n",
       "8419  0.020122 -0.008640 -0.036044    ...    -0.029793 -0.006375 -0.006452   \n",
       "2787  0.050342 -0.040677  0.107675    ...     0.063421  0.005740  0.003279   \n",
       "3492  0.025852 -0.012643 -0.091027    ...    -0.028269  0.025741  0.050546   \n",
       "6632 -0.147529  0.120269 -0.023467    ...    -0.006956  0.006164 -0.010452   \n",
       "...        ...       ...       ...    ...          ...       ...       ...   \n",
       "1615  0.009369 -0.037090 -0.010817    ...    -0.024745 -0.030275 -0.013718   \n",
       "4843  0.010966 -0.016482 -0.001047    ...     0.067431 -0.041877 -0.040731   \n",
       "6651 -0.033045 -0.005726  0.022710    ...     0.022489  0.029329  0.028068   \n",
       "1486  0.023741 -0.068383 -0.055859    ...    -0.000113  0.004621  0.027975   \n",
       "6015 -0.003353 -0.010930  0.002313    ...     0.007693 -0.006512  0.014256   \n",
       "1770 -0.029624 -0.017837 -0.002738    ...     0.042630 -0.039309  0.018034   \n",
       "3076 -0.010583 -0.035377  0.077385    ...     0.034511  0.097556 -0.008341   \n",
       "2154 -0.057764  0.018950 -0.041811    ...    -0.003232  0.019732  0.000398   \n",
       "5893 -0.007719  0.033445 -0.026571    ...    -0.002001  0.022090  0.003384   \n",
       "5165  0.023525 -0.011961 -0.026437    ...     0.079167  0.067252  0.051149   \n",
       "2260  0.000339  0.000803  0.003506    ...    -0.013032 -0.004006  0.021961   \n",
       "5171  0.005085 -0.007817  0.020139    ...    -0.013606 -0.013328 -0.030904   \n",
       "8019 -0.086581  0.055967  0.058681    ...     0.003585 -0.014687  0.001020   \n",
       "2580 -0.028095  0.006273  0.018213    ...     0.015798 -0.046812 -0.038905   \n",
       "767   0.031616 -0.037503 -0.040318    ...    -0.007355  0.006323 -0.001818   \n",
       "1285  0.085966  0.107677  0.052169    ...     0.006815 -0.000912 -0.002800   \n",
       "4141 -0.075958  0.037816 -0.020132    ...     0.031579 -0.036450  0.011861   \n",
       "512   0.030595 -0.007032 -0.040920    ...    -0.010279  0.002903 -0.000551   \n",
       "7840 -0.020052  0.010993 -0.018660    ...     0.007595 -0.003466 -0.007109   \n",
       "5423  0.014846  0.010605 -0.034964    ...    -0.002386  0.003964 -0.003245   \n",
       "7568 -0.018407 -0.000021  0.023776    ...    -0.004842 -0.006046  0.003075   \n",
       "1189 -0.004842  0.005362  0.013193    ...    -0.011873  0.002484  0.007547   \n",
       "7871 -0.082508  0.091017  0.016224    ...    -0.039014 -0.013844 -0.014660   \n",
       "407   0.057933 -0.058886  0.014467    ...     0.023853  0.000154  0.003201   \n",
       "1263  0.004895 -0.001934 -0.001078    ...    -0.009445 -0.000214 -0.003719   \n",
       "7592 -0.036253  0.026222  0.091691    ...    -0.004555  0.068639  0.032621   \n",
       "3151 -0.002207 -0.015284 -0.001378    ...     0.011263 -0.013241 -0.005535   \n",
       "6672 -0.003018 -0.015368  0.035937    ...    -0.034323 -0.022540  0.031383   \n",
       "4112  0.041517 -0.031917 -0.054189    ...     0.073855 -0.014096  0.058849   \n",
       "6626  0.015762 -0.008428 -0.010965    ...    -0.003455  0.007696  0.013849   \n",
       "\n",
       "            75        76        77        78        79  Res_binary         y  \n",
       "2072  0.000921 -0.016751 -0.000313  0.003150 -0.004419         1.0  0.608711  \n",
       "7052  0.017289 -0.012610 -0.027626 -0.004125  0.002743         1.0  0.228592  \n",
       "1582 -0.019105  0.019378  0.109131  0.060335 -0.017806         1.0 -3.150298  \n",
       "2054 -0.044379 -0.006101 -0.011122 -0.019106 -0.000104         1.0 -1.195514  \n",
       "6602 -0.002859  0.019277 -0.010504  0.002817 -0.016495         2.0 -0.082692  \n",
       "7532  0.071666  0.026278 -0.038351 -0.005259  0.004339         1.0 -1.952032  \n",
       "3083 -0.001493 -0.022240  0.003993  0.008809  0.011543         1.0  0.855352  \n",
       "2388 -0.026400 -0.025293  0.002643  0.067023  0.021221         1.0  3.731262  \n",
       "6609  0.014142  0.009529  0.005552 -0.008048  0.021765         1.0  0.729401  \n",
       "7681 -0.018488  0.010354 -0.004134  0.008201 -0.019747         2.0  0.760434  \n",
       "4673  0.018639  0.003783 -0.010586 -0.006428 -0.015603         1.0  0.973780  \n",
       "2204  0.000327  0.010034  0.034334  0.006056 -0.004555         1.0 -1.930957  \n",
       "8048 -0.041036  0.026249 -0.002565 -0.024128 -0.006258         1.0 -1.636408  \n",
       "5220 -0.002083  0.004936  0.008453  0.005740 -0.016168         1.0  0.308966  \n",
       "6458  0.008665  0.018303 -0.009544 -0.008272  0.027868         1.0  1.531688  \n",
       "7425  0.006733  0.031536  0.004710 -0.021649  0.004476         1.0 -1.672693  \n",
       "2836  0.022248  0.003634 -0.005659  0.016490  0.018592         2.0  0.309356  \n",
       "914   0.017915 -0.021842  0.013028 -0.014359  0.002419         1.0  2.008592  \n",
       "8324 -0.010500  0.004731 -0.002148  0.008908 -0.002170         2.0  0.779555  \n",
       "3336  0.004586  0.003631  0.008616  0.013974  0.030070         1.0 -0.660848  \n",
       "8042 -0.012236  0.032461 -0.007867  0.027316 -0.017433         2.0 -0.285322  \n",
       "3784 -0.040031  0.010658  0.021819 -0.051881 -0.015041         1.0 -2.535885  \n",
       "6729 -0.009929 -0.023733  0.000809 -0.002230 -0.022472         1.0  1.259840  \n",
       "338   0.013771  0.013769 -0.019778  0.040352  0.008314         1.0  0.490622  \n",
       "4243  0.006746  0.010589  0.006518 -0.000499  0.000647         1.0  1.179164  \n",
       "6580  0.003874 -0.007424  0.030796 -0.014016  0.015896         1.0 -0.621066  \n",
       "8419 -0.005168 -0.006065 -0.000058 -0.019496  0.014180         1.0 -0.010986  \n",
       "2787 -0.051274  0.074691  0.000408  0.000053  0.009366         2.0  0.421632  \n",
       "3492 -0.004424  0.051613  0.017495  0.072654 -0.019630         2.0  0.592588  \n",
       "6632 -0.002407  0.026309 -0.006202 -0.018552  0.007515         1.0 -0.191471  \n",
       "...        ...       ...       ...       ...       ...         ...       ...  \n",
       "1615 -0.057483  0.013258  0.034237  0.014637 -0.011492         1.0 -0.497116  \n",
       "4843  0.027509 -0.036932 -0.019291 -0.026647  0.016609         1.0 -0.280298  \n",
       "6651 -0.004969 -0.018100 -0.009231  0.003325 -0.003796         1.0 -2.077597  \n",
       "1486  0.078964  0.004311 -0.048937 -0.009980 -0.022252         1.0  5.241403  \n",
       "6015  0.002982  0.000504 -0.017873  0.005043  0.002032         1.0 -2.388459  \n",
       "1770 -0.006456 -0.051591  0.004748 -0.004811 -0.010484         1.0 -2.135797  \n",
       "3076 -0.020553  0.007398  0.003272 -0.023717  0.088225         1.0 -0.258852  \n",
       "2154 -0.021312  0.000116 -0.003630  0.007919 -0.024178         1.0 -1.343981  \n",
       "5893 -0.008295 -0.006344  0.002825 -0.019562 -0.002559         1.0 -0.166765  \n",
       "5165  0.024296 -0.040153 -0.031555 -0.100400 -0.002002         1.0  4.959885  \n",
       "2260 -0.003272 -0.001876 -0.015837 -0.009662 -0.005003         1.0 -2.117241  \n",
       "5171 -0.007405  0.029483  0.030742  0.020167 -0.009032         1.0 -1.259774  \n",
       "8019 -0.002716 -0.018985 -0.001826  0.009792 -0.016440         1.0  1.089598  \n",
       "2580 -0.012914  0.038330  0.009677  0.035281  0.014102         1.0  1.094150  \n",
       "767  -0.006830 -0.016023  0.024806  0.020469  0.027052         1.0 -1.960389  \n",
       "1285  0.003477 -0.000957  0.003302  0.021909 -0.003753         1.0  1.157542  \n",
       "4141 -0.004782 -0.004364  0.004689 -0.003069 -0.011550         1.0  0.634144  \n",
       "512  -0.012864  0.001860  0.008431 -0.032602 -0.024878         2.0  1.059720  \n",
       "7840 -0.003492 -0.011104  0.002459  0.014803 -0.012023         1.0  2.598694  \n",
       "5423 -0.000925  0.001514 -0.004247  0.003387  0.009916         1.0 -1.499407  \n",
       "7568 -0.002953 -0.019386  0.014842  0.006616 -0.009521         1.0 -1.938785  \n",
       "1189 -0.021241  0.018853  0.026568 -0.008351 -0.001996         1.0 -2.049534  \n",
       "7871  0.040636  0.027237 -0.024434 -0.011717  0.010606         1.0  0.560467  \n",
       "407   0.041810 -0.010399  0.033445 -0.021412 -0.016606         1.0 -2.012394  \n",
       "1263  0.010614 -0.002345 -0.006495 -0.016290  0.000712         1.0  2.219812  \n",
       "7592  0.010584  0.011506 -0.002246 -0.018053 -0.050417         2.0 -0.990451  \n",
       "3151  0.002931  0.001313  0.003746  0.010966 -0.020507         1.0  1.965008  \n",
       "6672 -0.004306 -0.020601 -0.032909 -0.049230  0.026367         1.0 -0.332651  \n",
       "4112  0.097044  0.001760  0.050074 -0.006443 -0.045955         1.0 -3.666103  \n",
       "6626 -0.009448 -0.007368 -0.003926  0.001142 -0.009651         1.0 -1.499225  \n",
       "\n",
       "[5533 rows x 82 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 77.1 ms, total: 3min 56s\n",
      "Wall time: 3min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbr = GradientBoostingRegressor()\n",
    "parameters = {'n_estimators': [10, 50, 100, 200, 300], 'max_depth':[2,3,4,5], 'max_features':['auto','sqrt','log2']}\n",
    "gbr_cv = GridSearchCV(gbr, parameters)\n",
    "gbr_cv.fit(svd_train_df.iloc[:,:-1], svd_train_df.iloc[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 10}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2,\n",
       "             max_features='log2', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_opt = GradientBoostingRegressor(max_depth= 2, max_features='log2', n_estimators= 10)\n",
    "gbr_opt.fit(svd_train_df.iloc[:,:-1], svd_train_df.iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gbr_opt_tfidf_2gram_pred = gbr_opt.predict(svd_test_df.iloc[:,:-1])\n",
    "gbr_opt_tfidf_2gram_mse = mean_squared_error(svd_test_df.iloc[:,-1], gbr_opt_tfidf_2gram_pred)\n",
    "gbr_opt_tfidf_2gram_mae = mean_absolute_error(svd_test_df.iloc[:,-1], gbr_opt_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 4.112428151515572\n",
      "mean absolute error is 1.3932484102878269\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(gbr_opt_tfidf_2gram_mse))\n",
    "print('mean absolute error is {}'.format(gbr_opt_tfidf_2gram_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Look at R^2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inference_data = pd.read_csv('bio_txt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "true_df = inference_data[['index','0', '1',\n",
    "       '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n",
    "       '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24','Res_binary', 'length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_df = inference_data[['index','0_hat', '1_hat', '2_hat', '3_hat',\n",
    "       '4_hat', '5_hat', '6_hat', '7_hat', '8_hat', '9_hat', '10_hat',\n",
    "       '11_hat', '12_hat', '13_hat', '14_hat', '15_hat', '16_hat',\n",
    "       '17_hat', '18_hat', '19_hat', '20_hat', '21_hat', '22_hat',\n",
    "       '23_hat', '24_hat','Res_binary','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "predicted_df = predicted_df.drop(['index'],axis=1)\n",
    "true_df = true_df.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#prepare data:\n",
    "msk = np.random.rand(7388) < 0.8\n",
    "\n",
    "predicted_df_x = predicted_df.iloc[:,:-1]\n",
    "predicted_df_y = predicted_df.iloc[:,-1]\n",
    "\n",
    "predicted_df_train_x = predicted_df_x[msk]\n",
    "predicted_df_test_x = predicted_df_x[~msk]\n",
    "\n",
    "predicted_df_train_y = predicted_df_y[msk]\n",
    "predicted_df_test_y = predicted_df_y[~msk]\n",
    "\n",
    "\n",
    "true_df_x = true_df.iloc[:,:-1]\n",
    "true_df_y = true_df.iloc[:,-1]\n",
    "\n",
    "true_df_train_x = true_df_x[msk]\n",
    "true_df_test_x = true_df_x[~msk]\n",
    "\n",
    "true_df_train_y = true_df_y[msk]\n",
    "true_df_test_y = true_df_y[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#random forest regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#predicted values R2 score\n",
    "rfr_predicted_r2 = RandomForestRegressor(n_estimators=1000)\n",
    "rfr_predicted_r2.fit(predicted_df_train_x, predicted_df_train_y)\n",
    "rfr_predicted_r2_pred = rfr_predicted_r2.predict(predicted_df_test_x)\n",
    "\n",
    "rfr_predicted_r2score = r2_score(predicted_df_test_y,rfr_predicted_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0117588985546\n"
     ]
    }
   ],
   "source": [
    "print(rfr_predicted_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#True values R2 score\n",
    "rfr_true_r2 = RandomForestRegressor(n_estimators=1000)\n",
    "rfr_true_r2.fit(predicted_df_train_x, predicted_df_train_y)\n",
    "rfr_true_r2_pred = rfr_true_r2.predict(predicted_df_test_x)\n",
    "\n",
    "rfr_true_r2score = r2_score(predicted_df_test_y,rfr_true_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0102580076061\n"
     ]
    }
   ],
   "source": [
    "print(rfr_true_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#predicted value R2 score\n",
    "lr_predicted_r2= linear_model.LinearRegression()\n",
    "lr_predicted_r2.fit(predicted_df_train_x, predicted_df_train_y)\n",
    "lr_predicted_r2_pred = lr_predicted_r2.predict(predicted_df_test_x)\n",
    "\n",
    "lr_predicted_r2score = r2_score(predicted_df_test_y,lr_predicted_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00498929305651\n"
     ]
    }
   ],
   "source": [
    "print(lr_predicted_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#True values R2 score\n",
    "lr_true_r2= linear_model.LinearRegression()\n",
    "lr_true_r2.fit(true_df_train_x, true_df_train_y)\n",
    "lr_true_r2_pred = lr_true_r2.predict(true_df_test_x)\n",
    "\n",
    "lr_true_r2score = r2_score(true_df_test_y,lr_true_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00981999902508\n"
     ]
    }
   ],
   "source": [
    "print(lr_true_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Gradient boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#True value R2 score\n",
    "gbr_opt_true_r2 = GradientBoostingRegressor(max_depth= 2, max_features='log2', n_estimators= 10)\n",
    "gbr_opt_true_r2.fit(true_df_train_x, true_df_train_y)\n",
    "gbr_opt_true_r2_pred = gbr_opt_true_r2.predict(true_df_test_x)\n",
    "\n",
    "gbr_opt_true_r2score = r2_score(true_df_test_y,gbr_opt_true_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.000800481644807\n"
     ]
    }
   ],
   "source": [
    "print(gbr_opt_true_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predicted value R2 score\n",
    "gbr_opt_pred_r2 = GradientBoostingRegressor(max_depth= 2, max_features='log2', n_estimators= 10)\n",
    "gbr_opt_pred_r2.fit(predicted_df_train_x, predicted_df_train_y)\n",
    "gbr_opt_pred_r2_pred = gbr_opt_pred_r2.predict(predicted_df_test_x)\n",
    "\n",
    "gbr_opt_pred_r2score = r2_score(predicted_df_test_y,gbr_opt_pred_r2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00439219373346\n"
     ]
    }
   ],
   "source": [
    "print(gbr_opt_pred_r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11456391346\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(predicted_df_test_y,gbr_opt_pred_r2_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
