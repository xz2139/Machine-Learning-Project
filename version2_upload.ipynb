{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cc_merged_0412.csv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>year</th>\n",
       "      <th>caseid</th>\n",
       "      <th>opinion_type</th>\n",
       "      <th>judge_name</th>\n",
       "      <th>decision</th>\n",
       "      <th>date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>...</th>\n",
       "      <th>suffix</th>\n",
       "      <th>month_3</th>\n",
       "      <th>length_3m</th>\n",
       "      <th>month_6</th>\n",
       "      <th>length_6m</th>\n",
       "      <th>month_3m_b</th>\n",
       "      <th>month_3_b</th>\n",
       "      <th>length_3m_dif</th>\n",
       "      <th>txt</th>\n",
       "      <th>x_dem_p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2304</td>\n",
       "      <td>1993</td>\n",
       "      <td>X35807</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>PER CURIAM</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1993-08-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1993-11-04</td>\n",
       "      <td>49.758998</td>\n",
       "      <td>1994-02-04</td>\n",
       "      <td>49.603455</td>\n",
       "      <td>1993-05-04</td>\n",
       "      <td>1993-05-04</td>\n",
       "      <td>-1.099144</td>\n",
       "      <td>per curiam: charo appeals his sentence for co...</td>\n",
       "      <td>0.292902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2796</td>\n",
       "      <td>1992</td>\n",
       "      <td>X3AD9D</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1992-11-13</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1993-02-13</td>\n",
       "      <td>52.334119</td>\n",
       "      <td>1993-05-13</td>\n",
       "      <td>51.257239</td>\n",
       "      <td>1992-08-13</td>\n",
       "      <td>1992-08-13</td>\n",
       "      <td>1.663181</td>\n",
       "      <td>logan , circuit judge. the only issue in this...</td>\n",
       "      <td>0.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2805</td>\n",
       "      <td>1992</td>\n",
       "      <td>X3ADTB</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>reversed</td>\n",
       "      <td>1992-12-29</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1993-03-29</td>\n",
       "      <td>50.282495</td>\n",
       "      <td>1993-06-29</td>\n",
       "      <td>51.134191</td>\n",
       "      <td>1992-09-29</td>\n",
       "      <td>1992-09-29</td>\n",
       "      <td>-3.563471</td>\n",
       "      <td>logan , circuit judge. defendant bernard d. r...</td>\n",
       "      <td>0.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2814</td>\n",
       "      <td>1997</td>\n",
       "      <td>X37GPR</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>JOHN C</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1997-02-11</td>\n",
       "      <td>PORFILIO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1997-05-11</td>\n",
       "      <td>51.570342</td>\n",
       "      <td>1997-08-11</td>\n",
       "      <td>51.983779</td>\n",
       "      <td>1996-11-11</td>\n",
       "      <td>1996-11-11</td>\n",
       "      <td>0.760434</td>\n",
       "      <td>john c. porfilio , circuit judge. defendant b...</td>\n",
       "      <td>0.360773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2816</td>\n",
       "      <td>1999</td>\n",
       "      <td>X4QAM7</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1999-01-04</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1999-04-04</td>\n",
       "      <td>59.568882</td>\n",
       "      <td>1999-07-04</td>\n",
       "      <td>59.854130</td>\n",
       "      <td>1998-10-04</td>\n",
       "      <td>1998-10-04</td>\n",
       "      <td>-0.630426</td>\n",
       "      <td>holloway , circuit judge. mr. unser brings th...</td>\n",
       "      <td>0.411135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2817</td>\n",
       "      <td>1998</td>\n",
       "      <td>XN5ACRQNB5G0</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>JOHN C</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1998-02-23</td>\n",
       "      <td>PORFILIO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1998-05-23</td>\n",
       "      <td>63.292860</td>\n",
       "      <td>1998-08-23</td>\n",
       "      <td>61.762051</td>\n",
       "      <td>1997-11-23</td>\n",
       "      <td>1997-11-23</td>\n",
       "      <td>3.591201</td>\n",
       "      <td>john c. porfilio , circuit judge. these three...</td>\n",
       "      <td>0.399229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>2819</td>\n",
       "      <td>1991</td>\n",
       "      <td>X57EQB</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>JOHN P</td>\n",
       "      <td>reversed</td>\n",
       "      <td>1991-12-17</td>\n",
       "      <td>MOORE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1992-03-17</td>\n",
       "      <td>50.717301</td>\n",
       "      <td>1992-06-17</td>\n",
       "      <td>50.744294</td>\n",
       "      <td>1991-09-17</td>\n",
       "      <td>1991-09-17</td>\n",
       "      <td>1.751541</td>\n",
       "      <td>john p. moore , circuit judge. james l. norma...</td>\n",
       "      <td>0.350113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>2820</td>\n",
       "      <td>1991</td>\n",
       "      <td>XAC38F</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>BRORBY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1991-12-31</td>\n",
       "      <td>BRORBY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1992-03-31</td>\n",
       "      <td>49.808413</td>\n",
       "      <td>1992-06-30</td>\n",
       "      <td>50.642475</td>\n",
       "      <td>1991-09-30</td>\n",
       "      <td>1991-09-30</td>\n",
       "      <td>0.303059</td>\n",
       "      <td>brorby , circuit judge. defendant juan dozal-...</td>\n",
       "      <td>0.350113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>2821</td>\n",
       "      <td>1995</td>\n",
       "      <td>X4NT3Q</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1995-10-18</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1996-01-18</td>\n",
       "      <td>50.519827</td>\n",
       "      <td>1996-04-18</td>\n",
       "      <td>51.128395</td>\n",
       "      <td>1995-07-18</td>\n",
       "      <td>1995-07-18</td>\n",
       "      <td>-0.039012</td>\n",
       "      <td>holloway , circuit judge. petitioner-appellan...</td>\n",
       "      <td>0.341123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>2822</td>\n",
       "      <td>2000</td>\n",
       "      <td>X3FBF1</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>TACHA</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>2000-04-04</td>\n",
       "      <td>TACHA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>2000-07-04</td>\n",
       "      <td>59.489909</td>\n",
       "      <td>2000-10-04</td>\n",
       "      <td>59.785273</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>-1.213819</td>\n",
       "      <td>tacha , circuit judge. defendant tony smith a...</td>\n",
       "      <td>0.440144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>2823</td>\n",
       "      <td>1998</td>\n",
       "      <td>X369TF</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>EBEL</td>\n",
       "      <td>reversed</td>\n",
       "      <td>1998-09-18</td>\n",
       "      <td>EBEL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1998-12-18</td>\n",
       "      <td>61.202553</td>\n",
       "      <td>1999-03-18</td>\n",
       "      <td>60.579140</td>\n",
       "      <td>1998-06-18</td>\n",
       "      <td>1998-06-18</td>\n",
       "      <td>0.911760</td>\n",
       "      <td>ebel , circuit judge. manuel coronado-cervant...</td>\n",
       "      <td>0.399229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>2824</td>\n",
       "      <td>2001</td>\n",
       "      <td>X50L71</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>PAUL KELLY</td>\n",
       "      <td>reversed</td>\n",
       "      <td>2001-03-22</td>\n",
       "      <td>KELLY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>2001-06-22</td>\n",
       "      <td>60.074257</td>\n",
       "      <td>2001-09-22</td>\n",
       "      <td>60.182141</td>\n",
       "      <td>2000-12-22</td>\n",
       "      <td>2000-12-22</td>\n",
       "      <td>0.675637</td>\n",
       "      <td>paul kelly, jr. , circuit judge. defendant-ap...</td>\n",
       "      <td>0.450312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>2829</td>\n",
       "      <td>1992</td>\n",
       "      <td>X321KN</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1992-02-18</td>\n",
       "      <td>LOGAN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1992-05-18</td>\n",
       "      <td>49.651231</td>\n",
       "      <td>1992-08-18</td>\n",
       "      <td>50.409258</td>\n",
       "      <td>1991-11-18</td>\n",
       "      <td>1991-11-18</td>\n",
       "      <td>-0.972865</td>\n",
       "      <td>logan , circuit judge. in no. 91-3187 the uni...</td>\n",
       "      <td>0.328425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>2830</td>\n",
       "      <td>1996</td>\n",
       "      <td>X36F13</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>JOHN C</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1996-12-31</td>\n",
       "      <td>PORFILIO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>1997-03-31</td>\n",
       "      <td>51.385926</td>\n",
       "      <td>1997-06-30</td>\n",
       "      <td>51.853494</td>\n",
       "      <td>1996-09-30</td>\n",
       "      <td>1996-09-30</td>\n",
       "      <td>0.112642</td>\n",
       "      <td>john c. porfilio , circuit judge. the central...</td>\n",
       "      <td>0.359739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>2835</td>\n",
       "      <td>2001</td>\n",
       "      <td>X519BI</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>2001-10-09</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>2002-01-09</td>\n",
       "      <td>59.244898</td>\n",
       "      <td>2002-04-09</td>\n",
       "      <td>61.060251</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>-0.877279</td>\n",
       "      <td>holloway , circuit judge. defendant-appellant...</td>\n",
       "      <td>0.450312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>2836</td>\n",
       "      <td>2000</td>\n",
       "      <td>X3J03E</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>2000-12-15</td>\n",
       "      <td>HOLLOWAY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>2001-03-15</td>\n",
       "      <td>59.462579</td>\n",
       "      <td>2001-06-15</td>\n",
       "      <td>59.715754</td>\n",
       "      <td>2000-09-15</td>\n",
       "      <td>2000-09-15</td>\n",
       "      <td>-1.581288</td>\n",
       "      <td>holloway , circuit judge. defendant/appellant...</td>\n",
       "      <td>0.440144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>2842</td>\n",
       "      <td>1999</td>\n",
       "      <td>X3F6GM</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>MURPHY</td>\n",
       "      <td>reversed</td>\n",
       "      <td>1999-12-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>JR.</td>\n",
       "      <td>2000-03-29</td>\n",
       "      <td>60.588383</td>\n",
       "      <td>2000-06-29</td>\n",
       "      <td>60.149401</td>\n",
       "      <td>1999-09-29</td>\n",
       "      <td>1999-09-29</td>\n",
       "      <td>1.185504</td>\n",
       "      <td>murphy , circuit judge. i. introduction defen...</td>\n",
       "      <td>0.411135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>3065</td>\n",
       "      <td>1993</td>\n",
       "      <td>X30IAK</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>MORGAN</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1993-05-26</td>\n",
       "      <td>MORGAN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1993-08-26</td>\n",
       "      <td>51.180044</td>\n",
       "      <td>1993-11-26</td>\n",
       "      <td>50.462687</td>\n",
       "      <td>1993-02-26</td>\n",
       "      <td>1993-02-26</td>\n",
       "      <td>0.959357</td>\n",
       "      <td>morgan , senior circuit judge: appellants wer...</td>\n",
       "      <td>0.292902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>3392</td>\n",
       "      <td>1998</td>\n",
       "      <td>X40T80</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>HATCHETT</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1998-12-31</td>\n",
       "      <td>HATCHETT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1999-03-31</td>\n",
       "      <td>59.528372</td>\n",
       "      <td>1999-06-30</td>\n",
       "      <td>59.856889</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>1998-09-30</td>\n",
       "      <td>-1.595323</td>\n",
       "      <td>hatchett , chief judge: we address issues of ...</td>\n",
       "      <td>0.399229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>3400</td>\n",
       "      <td>1994</td>\n",
       "      <td>X38NGO</td>\n",
       "      <td>contentMajOp</td>\n",
       "      <td>DYER</td>\n",
       "      <td>affirmed</td>\n",
       "      <td>1994-06-22</td>\n",
       "      <td>DYER</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>1994-09-22</td>\n",
       "      <td>52.221083</td>\n",
       "      <td>1994-12-22</td>\n",
       "      <td>52.701810</td>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>1994-03-22</td>\n",
       "      <td>-2.049534</td>\n",
       "      <td>dyer , senior circuit judge: john peeples, pa...</td>\n",
       "      <td>0.297749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 298 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  index  year        caseid  opinion_type  judge_name  decision  \\\n",
       "0            0   2304  1993        X35807  contentMajOp  PER CURIAM  affirmed   \n",
       "1            1   2796  1992        X3AD9D  contentMajOp       LOGAN  affirmed   \n",
       "2            2   2805  1992        X3ADTB  contentMajOp       LOGAN  reversed   \n",
       "3            3   2814  1997        X37GPR  contentMajOp      JOHN C  affirmed   \n",
       "4            4   2816  1999        X4QAM7  contentMajOp    HOLLOWAY  affirmed   \n",
       "5            5   2817  1998  XN5ACRQNB5G0  contentMajOp      JOHN C  affirmed   \n",
       "6            6   2819  1991        X57EQB  contentMajOp      JOHN P  reversed   \n",
       "7            7   2820  1991        XAC38F  contentMajOp      BRORBY  affirmed   \n",
       "8            8   2821  1995        X4NT3Q  contentMajOp    HOLLOWAY  affirmed   \n",
       "9            9   2822  2000        X3FBF1  contentMajOp       TACHA  affirmed   \n",
       "10          10   2823  1998        X369TF  contentMajOp        EBEL  reversed   \n",
       "11          11   2824  2001        X50L71  contentMajOp  PAUL KELLY  reversed   \n",
       "12          12   2829  1992        X321KN  contentMajOp       LOGAN  affirmed   \n",
       "13          13   2830  1996        X36F13  contentMajOp      JOHN C  affirmed   \n",
       "14          14   2835  2001        X519BI  contentMajOp    HOLLOWAY  affirmed   \n",
       "15          15   2836  2000        X3J03E  contentMajOp    HOLLOWAY  affirmed   \n",
       "16          16   2842  1999        X3F6GM  contentMajOp      MURPHY  reversed   \n",
       "17          17   3065  1993        X30IAK  contentMajOp      MORGAN  affirmed   \n",
       "18          18   3392  1998        X40T80  contentMajOp    HATCHETT  affirmed   \n",
       "19          19   3400  1994        X38NGO  contentMajOp        DYER  affirmed   \n",
       "\n",
       "          date    Author  Affirmed    ...     suffix     month_3  length_3m  \\\n",
       "0   1993-08-04       NaN       1.0    ...             1993-11-04  49.758998   \n",
       "1   1992-11-13     LOGAN       1.0    ...        JR.  1993-02-13  52.334119   \n",
       "2   1992-12-29     LOGAN       0.0    ...        JR.  1993-03-29  50.282495   \n",
       "3   1997-02-11  PORFILIO       1.0    ...        JR.  1997-05-11  51.570342   \n",
       "4   1999-01-04  HOLLOWAY       1.0    ...        JR.  1999-04-04  59.568882   \n",
       "5   1998-02-23  PORFILIO       1.0    ...        JR.  1998-05-23  63.292860   \n",
       "6   1991-12-17     MOORE       0.0    ...        JR.  1992-03-17  50.717301   \n",
       "7   1991-12-31    BRORBY       1.0    ...        JR.  1992-03-31  49.808413   \n",
       "8   1995-10-18  HOLLOWAY       1.0    ...        JR.  1996-01-18  50.519827   \n",
       "9   2000-04-04     TACHA       1.0    ...        JR.  2000-07-04  59.489909   \n",
       "10  1998-09-18      EBEL       0.0    ...        JR.  1998-12-18  61.202553   \n",
       "11  2001-03-22     KELLY       0.0    ...        JR.  2001-06-22  60.074257   \n",
       "12  1992-02-18     LOGAN       1.0    ...        JR.  1992-05-18  49.651231   \n",
       "13  1996-12-31  PORFILIO       1.0    ...        JR.  1997-03-31  51.385926   \n",
       "14  2001-10-09  HOLLOWAY       1.0    ...        JR.  2002-01-09  59.244898   \n",
       "15  2000-12-15  HOLLOWAY       1.0    ...        JR.  2001-03-15  59.462579   \n",
       "16  1999-12-29       NaN       1.0    ...        JR.  2000-03-29  60.588383   \n",
       "17  1993-05-26    MORGAN       1.0    ...             1993-08-26  51.180044   \n",
       "18  1998-12-31  HATCHETT       1.0    ...             1999-03-31  59.528372   \n",
       "19  1994-06-22      DYER       0.0    ...             1994-09-22  52.221083   \n",
       "\n",
       "       month_6  length_6m  month_3m_b   month_3_b  length_3m_dif  \\\n",
       "0   1994-02-04  49.603455  1993-05-04  1993-05-04      -1.099144   \n",
       "1   1993-05-13  51.257239  1992-08-13  1992-08-13       1.663181   \n",
       "2   1993-06-29  51.134191  1992-09-29  1992-09-29      -3.563471   \n",
       "3   1997-08-11  51.983779  1996-11-11  1996-11-11       0.760434   \n",
       "4   1999-07-04  59.854130  1998-10-04  1998-10-04      -0.630426   \n",
       "5   1998-08-23  61.762051  1997-11-23  1997-11-23       3.591201   \n",
       "6   1992-06-17  50.744294  1991-09-17  1991-09-17       1.751541   \n",
       "7   1992-06-30  50.642475  1991-09-30  1991-09-30       0.303059   \n",
       "8   1996-04-18  51.128395  1995-07-18  1995-07-18      -0.039012   \n",
       "9   2000-10-04  59.785273  2000-01-04  2000-01-04      -1.213819   \n",
       "10  1999-03-18  60.579140  1998-06-18  1998-06-18       0.911760   \n",
       "11  2001-09-22  60.182141  2000-12-22  2000-12-22       0.675637   \n",
       "12  1992-08-18  50.409258  1991-11-18  1991-11-18      -0.972865   \n",
       "13  1997-06-30  51.853494  1996-09-30  1996-09-30       0.112642   \n",
       "14  2002-04-09  61.060251  2001-07-09  2001-07-09      -0.877279   \n",
       "15  2001-06-15  59.715754  2000-09-15  2000-09-15      -1.581288   \n",
       "16  2000-06-29  60.149401  1999-09-29  1999-09-29       1.185504   \n",
       "17  1993-11-26  50.462687  1993-02-26  1993-02-26       0.959357   \n",
       "18  1999-06-30  59.856889  1998-09-30  1998-09-30      -1.595323   \n",
       "19  1994-12-22  52.701810  1994-03-22  1994-03-22      -2.049534   \n",
       "\n",
       "                                                  txt   x_dem_p  \n",
       "0    per curiam: charo appeals his sentence for co...  0.292902  \n",
       "1    logan , circuit judge. the only issue in this...  0.328425  \n",
       "2    logan , circuit judge. defendant bernard d. r...  0.328425  \n",
       "3    john c. porfilio , circuit judge. defendant b...  0.360773  \n",
       "4    holloway , circuit judge. mr. unser brings th...  0.411135  \n",
       "5    john c. porfilio , circuit judge. these three...  0.399229  \n",
       "6    john p. moore , circuit judge. james l. norma...  0.350113  \n",
       "7    brorby , circuit judge. defendant juan dozal-...  0.350113  \n",
       "8    holloway , circuit judge. petitioner-appellan...  0.341123  \n",
       "9    tacha , circuit judge. defendant tony smith a...  0.440144  \n",
       "10   ebel , circuit judge. manuel coronado-cervant...  0.399229  \n",
       "11   paul kelly, jr. , circuit judge. defendant-ap...  0.450312  \n",
       "12   logan , circuit judge. in no. 91-3187 the uni...  0.328425  \n",
       "13   john c. porfilio , circuit judge. the central...  0.359739  \n",
       "14   holloway , circuit judge. defendant-appellant...  0.450312  \n",
       "15   holloway , circuit judge. defendant/appellant...  0.440144  \n",
       "16   murphy , circuit judge. i. introduction defen...  0.411135  \n",
       "17   morgan , senior circuit judge: appellants wer...  0.292902  \n",
       "18   hatchett , chief judge: we address issues of ...  0.399229  \n",
       "19   dyer , senior circuit judge: john peeples, pa...  0.297749  \n",
       "\n",
       "[20 rows x 298 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8434 entries, 0 to 8433\n",
      "Columns: 298 entries, Unnamed: 0 to x_dem_p\n",
      "dtypes: float64(211), int64(61), object(26)\n",
      "memory usage: 19.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Unnamed: 0', 'index', 'year', 'caseid', 'opinion_type',\n",
       "       'judge_name', 'decision', 'date', 'Author', 'Affirmed',\n",
       "       'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated',\n",
       "       'VacatedInPart', 'Circuit', 'judgeid1', 'judgeid2', 'judgeid3',\n",
       "       'x_dem_x', 'x_republican_x', 'x_instate_ba_x', 'x_elev_x',\n",
       "       'x_unity_x', 'x_aba_x', 'x_crossa_x', 'x_pfedjdge_x',\n",
       "       'x_pindreg1_x', 'x_plawprof_x', 'x_pscab_x', 'x_pcab_x', 'x_pusa_x',\n",
       "       'x_pssenate_x', 'x_paag_x', 'x_psp_x', 'x_pslc_x', 'x_pssc_x',\n",
       "       'x_pshouse_x', 'x_psg_x', 'x_psgo_x', 'x_psenate_x', 'x_psatty_x',\n",
       "       'x_pprivate_x', 'x_pmayor_x', 'x_plocct_x', 'x_phouse_x',\n",
       "       'x_pgov_x', 'x_pda_x', 'x_pcc_x', 'x_pccoun_x', 'x_pausa_x',\n",
       "       'x_pasatty_x', 'x_pag_x', 'x_pada_x', 'x_pgovt_x', 'x_llm_sjd_x',\n",
       "       'x_protestant_x', 'x_evangelical_x', 'x_mainline_x',\n",
       "       'x_noreligion_x', 'x_catholic_x', 'x_jewish_x', 'x_black_x',\n",
       "       'x_nonwhite_x', 'x_female_x', 'x_jd_public_x', 'x_ba_public_x',\n",
       "       'x_b10s_x', 'x_b20s_x', 'x_b30s_x', 'x_b40s_x', 'x_b50s_x',\n",
       "       'x_pbank_x', 'x_pmag_x', 'x_ageon40s_x', 'x_ageon50s_x',\n",
       "       'x_ageon60s_x', 'x_ageon40orless_x', 'x_ageon70ormore_x',\n",
       "       'x_pago_x', 'x_apptoter_x', 'x_dccircuit_x', 'x_8circuit_x',\n",
       "       'x_11circuit_x', 'x_fcircuit_x', 'x_5circuit_x', 'x_1circuit_x',\n",
       "       'x_4circuit_x', 'x_9circuit_x', 'x_2circuit_x', 'x_7circuit_x',\n",
       "       'x_6circuit_x', 'x_10circuit_x', 'x_3circuit_x', 'id_x', 'x_term_x',\n",
       "       'x_circuit_x', 'x_hdem_x', 'x_hrep_x', 'x_sdem_x', 'x_srep_x',\n",
       "       'x_hother_x', 'x_sother_x', 'x_agecommi_x', 'fullname_x',\n",
       "       'lastname_x', 'firstname_x', 'middlename_x', 'suffix_x', 'x_dem_y',\n",
       "       'x_republican_y', 'x_instate_ba_y', 'x_elev_y', 'x_unity_y',\n",
       "       'x_aba_y', 'x_crossa_y', 'x_pfedjdge_y', 'x_pindreg1_y',\n",
       "       'x_plawprof_y', 'x_pscab_y', 'x_pcab_y', 'x_pusa_y', 'x_pssenate_y',\n",
       "       'x_paag_y', 'x_psp_y', 'x_pslc_y', 'x_pssc_y', 'x_pshouse_y',\n",
       "       'x_psg_y', 'x_psgo_y', 'x_psenate_y', 'x_psatty_y', 'x_pprivate_y',\n",
       "       'x_pmayor_y', 'x_plocct_y', 'x_phouse_y', 'x_pgov_y', 'x_pda_y',\n",
       "       'x_pcc_y', 'x_pccoun_y', 'x_pausa_y', 'x_pasatty_y', 'x_pag_y',\n",
       "       'x_pada_y', 'x_pgovt_y', 'x_llm_sjd_y', 'x_protestant_y',\n",
       "       'x_evangelical_y', 'x_mainline_y', 'x_noreligion_y', 'x_catholic_y',\n",
       "       'x_jewish_y', 'x_black_y', 'x_nonwhite_y', 'x_female_y',\n",
       "       'x_jd_public_y', 'x_ba_public_y', 'x_b10s_y', 'x_b20s_y',\n",
       "       'x_b30s_y', 'x_b40s_y', 'x_b50s_y', 'x_pbank_y', 'x_pmag_y',\n",
       "       'x_ageon40s_y', 'x_ageon50s_y', 'x_ageon60s_y', 'x_ageon40orless_y',\n",
       "       'x_ageon70ormore_y', 'x_pago_y', 'x_apptoter_y', 'x_dccircuit_y',\n",
       "       'x_8circuit_y', 'x_11circuit_y', 'x_fcircuit_y', 'x_5circuit_y',\n",
       "       'x_1circuit_y', 'x_4circuit_y', 'x_9circuit_y', 'x_2circuit_y',\n",
       "       'x_7circuit_y', 'x_6circuit_y', 'x_10circuit_y', 'x_3circuit_y',\n",
       "       'id_y', 'x_term_y', 'x_circuit_y', 'x_hdem_y', 'x_hrep_y',\n",
       "       'x_sdem_y', 'x_srep_y', 'x_hother_y', 'x_sother_y', 'x_agecommi_y',\n",
       "       'fullname_y', 'lastname_y', 'firstname_y', 'middlename_y',\n",
       "       'suffix_y', 'x_dem', 'x_republican', 'x_instate_ba', 'x_elev',\n",
       "       'x_unity', 'x_aba', 'x_crossa', 'x_pfedjdge', 'x_pindreg1',\n",
       "       'x_plawprof', 'x_pscab', 'x_pcab', 'x_pusa', 'x_pssenate', 'x_paag',\n",
       "       'x_psp', 'x_pslc', 'x_pssc', 'x_pshouse', 'x_psg', 'x_psgo',\n",
       "       'x_psenate', 'x_psatty', 'x_pprivate', 'x_pmayor', 'x_plocct',\n",
       "       'x_phouse', 'x_pgov', 'x_pda', 'x_pcc', 'x_pccoun', 'x_pausa',\n",
       "       'x_pasatty', 'x_pag', 'x_pada', 'x_pgovt', 'x_llm_sjd',\n",
       "       'x_protestant', 'x_evangelical', 'x_mainline', 'x_noreligion',\n",
       "       'x_catholic', 'x_jewish', 'x_black', 'x_nonwhite', 'x_female',\n",
       "       'x_jd_public', 'x_ba_public', 'x_b10s', 'x_b20s', 'x_b30s',\n",
       "       'x_b40s', 'x_b50s', 'x_pbank', 'x_pmag', 'x_ageon40s', 'x_ageon50s',\n",
       "       'x_ageon60s', 'x_ageon40orless', 'x_ageon70ormore', 'x_pago',\n",
       "       'x_apptoter', 'x_dccircuit', 'x_8circuit', 'x_11circuit',\n",
       "       'x_fcircuit', 'x_5circuit', 'x_1circuit', 'x_4circuit',\n",
       "       'x_9circuit', 'x_2circuit', 'x_7circuit', 'x_6circuit',\n",
       "       'x_10circuit', 'x_3circuit', 'id', 'x_term', 'x_circuit', 'x_hdem',\n",
       "       'x_hrep', 'x_sdem', 'x_srep', 'x_hother', 'x_sother', 'x_agecommi',\n",
       "       'fullname', 'lastname', 'firstname', 'middlename', 'suffix',\n",
       "       'month_3', 'length_3m', 'month_6', 'length_6m', 'month_3m_b',\n",
       "       'month_3_b', 'length_3m_dif', 'txt', 'x_dem_p'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# affrimed,affimed, ..., other, + txt to predict length_3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_use = data[['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','txt','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>txt</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>per curiam: charo appeals his sentence for co...</td>\n",
       "      <td>-1.099144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. the only issue in this...</td>\n",
       "      <td>1.663181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. defendant bernard d. r...</td>\n",
       "      <td>-3.563471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>john c. porfilio , circuit judge. defendant b...</td>\n",
       "      <td>0.760434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>holloway , circuit judge. mr. unser brings th...</td>\n",
       "      <td>-0.630426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>john c. porfilio , circuit judge. these three...</td>\n",
       "      <td>3.591201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>john p. moore , circuit judge. james l. norma...</td>\n",
       "      <td>1.751541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>brorby , circuit judge. defendant juan dozal-...</td>\n",
       "      <td>0.303059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>holloway , circuit judge. petitioner-appellan...</td>\n",
       "      <td>-0.039012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>tacha , circuit judge. defendant tony smith a...</td>\n",
       "      <td>-1.213819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  VacatedInPart  \\\n",
       "0       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "1       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "2       0.0             0.0       1.0             0.0      0.0            0.0   \n",
       "3       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "4       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "5       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "6       0.0             0.0       0.0             0.0      1.0            0.0   \n",
       "7       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "8       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "9       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "\n",
       "                                                 txt  length_3m_dif  \n",
       "0   per curiam: charo appeals his sentence for co...      -1.099144  \n",
       "1   logan , circuit judge. the only issue in this...       1.663181  \n",
       "2   logan , circuit judge. defendant bernard d. r...      -3.563471  \n",
       "3   john c. porfilio , circuit judge. defendant b...       0.760434  \n",
       "4   holloway , circuit judge. mr. unser brings th...      -0.630426  \n",
       "5   john c. porfilio , circuit judge. these three...       3.591201  \n",
       "6   john p. moore , circuit judge. james l. norma...       1.751541  \n",
       "7   brorby , circuit judge. defendant juan dozal-...       0.303059  \n",
       "8   holloway , circuit judge. petitioner-appellan...      -0.039012  \n",
       "9   tacha , circuit judge. defendant tony smith a...      -1.213819  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_use.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' per curiam: charo appeals his sentence for conspiracy to possess with intent to distribute cocaine and money laundering. he maintains that the district court erred in refusing to grant him a two-level reduction for acceptance of responsibility under u.s.s.g. 3e1.1(a) . he contends that he manifested acceptance of responsibility by admitting his involvement in the offense, showing remorse, and cooperating with law enforcement officials. the government responds that the district court\\'s finding that charo had not accepted responsibility for the offense conduct was not clearly erroneous. the sentencing court\\'s determination that a defendant is not entitled to a reduction for acceptance of responsibility is entitled to great deference and will not be overturned unless it is clearly erroneous. united states v. spraggins , 868 f.2d 1541, 1543 (11th cir.1989). section 3e1.1(a) requires a sentencing court to reduce the offense level by two levels \"if the defendant clearly demonstrates a recognition and affirmative acceptance of personal responsibility for his criminal conduct.\" in determining whether defendant qualifies for an acceptance of responsibility adjustment, the sentencing court may consider whether the defendant made a \"voluntary and truthful admission to authorities of involvement in the offense and related conduct.\" u.s.s.g. 3e1.1 , comment. (n. 1(c)). charo pleaded guilty to one count of conspiracy to possess with intent to distribute cocaine base and one count of money laundering. at the change of plea hearing, he admitted that from june through october of 1981, he transported cocaine base from houston, texas to mobile, alabama on several occasions. he further admitted that he transported approximately one-half a kilogram at a time. charo told the probation officer that he transported approximately one and one-half kilograms of cocaine base on four occasions. thus, the probation officer estimated that he transported four kilograms of cocaine base. at sentencing, he admitted to transporting a total of one and one-half kilograms of cocaine base. charo\\'s statement was not a truthful admission and did not show a clear acceptance of responsibility. the court, therefore, did not err in declining to grant a two-level reduction. the sentence and judgment are affirmed. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_use['txt'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create bin:\n",
    "#0: decrease\n",
    "#1: same\n",
    "#2: increase\n",
    "def create_bin(row):\n",
    "    if row['length_3m_dif']<-0.2:\n",
    "        return 0\n",
    "    elif row['length_3m_dif']>0.2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/apps/python3/3.5.3/intel/lib/python3.5/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "data_use['bin']=data_use.apply(create_bin,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>txt</th>\n",
       "      <th>length_3m_dif</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>per curiam: charo appeals his sentence for co...</td>\n",
       "      <td>-1.099144</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. the only issue in this...</td>\n",
       "      <td>1.663181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>logan , circuit judge. defendant bernard d. r...</td>\n",
       "      <td>-3.563471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>john c. porfilio , circuit judge. defendant b...</td>\n",
       "      <td>0.760434</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>holloway , circuit judge. mr. unser brings th...</td>\n",
       "      <td>-0.630426</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  VacatedInPart  \\\n",
       "0       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "1       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "2       0.0             0.0       1.0             0.0      0.0            0.0   \n",
       "3       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "4       1.0             0.0       0.0             0.0      0.0            0.0   \n",
       "\n",
       "                                                 txt  length_3m_dif  bin  \n",
       "0   per curiam: charo appeals his sentence for co...      -1.099144    0  \n",
       "1   logan , circuit judge. the only issue in this...       1.663181    2  \n",
       "2   logan , circuit judge. defendant bernard d. r...      -3.563471    0  \n",
       "3   john c. porfilio , circuit judge. defendant b...       0.760434    2  \n",
       "4   holloway , circuit judge. mr. unser brings th...      -0.630426    0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data1_attemp = data_use[['txt','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6326, 9)\n",
      "(2108, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from numpy.random import RandomState\n",
    "\n",
    "RS1 = RandomState(1)\n",
    "train_data = data_use.sample(frac = 0.75, random_state = 200)\n",
    "test_data = data_use.drop(train_data.index)\n",
    "\n",
    "x_train = train_data['txt']\n",
    "x_test = test_data['txt']\n",
    "Y_train = train_data['length_3m_dif']\n",
    "Y_test = test_data['length_3m_dif']\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 20s, sys: 2.73 s, total: 3min 23s\n",
      "Wall time: 3min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vectorizer_1gram = CountVectorizer(stop_words = 'english', binary = False)\n",
    "count_vectorizer_2gram = CountVectorizer(stop_words = 'english', ngram_range = (1,2) , binary = False)\n",
    "tfid_vectorizer_1gram = TfidfVectorizer(stop_words = 'english', binary = False)\n",
    "tfid_vectorizer_2gram = TfidfVectorizer(stop_words = 'english', ngram_range= (1,2), binary = False)\n",
    "\n",
    "count_vectorizer_1gram.fit(x_train)\n",
    "count_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "tfid_vectorizer_1gram.fit(x_train)\n",
    "tfid_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "\n",
    "#count_vectorizer\n",
    "count_train_1gram = count_vectorizer_1gram.transform(x_train)\n",
    "count_test_1gram = count_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "count_train_2gram = count_vectorizer_2gram.transform(x_train)\n",
    "count_test_2gram = count_vectorizer_2gram.transform(x_test)\n",
    "\n",
    "#tfid_vectorizer\n",
    "tf_train_1gram = tfid_vectorizer_1gram.transform(x_train)\n",
    "tf_test_1gram = tfid_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "tf_train_2gram = tfid_vectorizer_2gram.transform(x_train)\n",
    "tf_test_2gram = tfid_vectorizer_2gram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_base_count_1gram = linear_model.LinearRegression()\n",
    "lr_base_count_1gram.fit(count_train_1gram, Y_train)\n",
    "lr_base_count_1gram_pred = lr_base_count_1gram.predict(count_test_1gram)\n",
    "count_1gram_base_mae = mean_absolute_error(Y_test, lr_base_count_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.69964025579824"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_1gram_base_mae_r2 = r2_score(Y_test, lr_base_count_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8035214992717261"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_1gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/DmtcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2abaafabe1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_count_1gram_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#count 2 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_base_count_2gram = linear_model.LinearRegression()\n",
    "lr_base_count_2gram.fit(count_train_2gram, Y_train)\n",
    "lr_base_count_2gram_pred = lr_base_count_2gram.predict(count_test_2gram)\n",
    "count_2gram_base_mae = mean_absolute_error(Y_test, lr_base_count_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7585600238429082"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_2gram_base_mae_r2 = r2_score(Y_test, lr_base_count_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.53133179890467863"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_2gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2abaafc12978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_count_2gram_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_base_tfidf_1gram = linear_model.LinearRegression()\n",
    "lr_base_tfidf_1gram.fit(tf_train_1gram, Y_train)\n",
    "lr_base_tfidf_1gram_pred = lr_base_tfidf_1gram.predict(tf_test_1gram)\n",
    "tfidf_1gram_base_mae = mean_absolute_error(Y_test, lr_base_tfidf_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7085882973015794"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_1gram_base_mae_r2 = r2_score(Y_test, lr_base_tfidf_1gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.37162225423885853"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_1gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2abaafc1dc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_tfidf_1gram_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tfidf 2 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_base_tfidf_2gram = linear_model.LinearRegression()\n",
    "lr_base_tfidf_2gram.fit(tf_train_2gram, Y_train)\n",
    "lr_base_tfidf_2gram_pred = lr_base_tfidf_2gram.predict(tf_test_2gram)\n",
    "tfidf_2gram_base_mae = mean_absolute_error(Y_test, lr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4516721052128747"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2gram_base_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_2gram_base_mae_r2 = r2_score(Y_test, lr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.072263646659945113"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_2gram_base_mae_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aba8e3d63c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,lr_base_tfidf_2gram_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#using tfidf vectorizer with 2 gram. \n",
    "#try randomforest regressor, decision tree regressor, svm regressor, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble.gradient_boosting import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.64 s, sys: 8 ms, total: 7.65 s\n",
      "Wall time: 7.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(tf_train_2gram, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr_base_tfidf_2gram_pred = rfr.predict(tf_test_2gram)\n",
    "rfr_tfidf_2gram_base_mse = mean_squared_error(Y_test, rfr_base_tfidf_2gram_pred)\n",
    "rfr_tfidf_2gram_base_mae = mean_absolute_error(Y_test, rfr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 4.37850130179453\n",
      "mean absolute error is 1.4632700051212577\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(rfr_tfidf_2gram_base_mse))\n",
    "print('mean absolute error is {}'.format(rfr_tfidf_2gram_base_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.13 s, sys: 1 ms, total: 1.13 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtr = DecisionTreeRegressor(random_state=0)\n",
    "dtr.fit(tf_train_2gram, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtr_base_tfidf_2gram_pred = dtr.predict(tf_test_2gram)\n",
    "dtr_tfidf_2gram_base_mse = mean_squared_error(Y_test, dtr_base_tfidf_2gram_pred)\n",
    "dtr_tfidf_2gram_base_mae = mean_absolute_error(Y_test, dtr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 7.489150408785389\n",
      "mean absolute error is 1.9510289650535433\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(dtr_tfidf_2gram_base_mse))\n",
    "print('mean absolute error is {}'.format(dtr_tfidf_2gram_base_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.51 s, sys: 39 ms, total: 6.55 s\n",
      "Wall time: 6.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "svrr = SVR(C=1.0, epsilon=0.2)\n",
    "svrr.fit(tf_train_2gram, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svrr_base_tfidf_2gram_pred = svrr.predict(tf_test_2gram)\n",
    "svrr_tfidf_2gram_base_mse = mean_squared_error(Y_test, svrr_base_tfidf_2gram_pred)\n",
    "svrr_tfidf_2gram_base_mae = mean_absolute_error(Y_test, svrr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 4.017991628968971\n",
      "mean absolute error is 1.3796404846821375\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(svrr_tfidf_2gram_base_mse))\n",
    "print('mean absolute error is {}'.format(svrr_tfidf_2gram_base_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.06 s, sys: 1 ms, total: 6.07 s\n",
      "Wall time: 6.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "gbr = GradientBoostingRegressor()\n",
    "gbr.fit(tf_train_2gram, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gradient boosting accept sparse matrix as input in \"fit\" method, but currently does not accept sparse matrix for \"predict\" method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_base_tfidf_2gram_pred = gbr.predict(tf_test_2gram.todense())\n",
    "gbr_tfidf_2gram_base_mse = mean_squared_error(Y_test, gbr_base_tfidf_2gram_pred)\n",
    "gbr_tfidf_2gram_base_mae = mean_absolute_error(Y_test, gbr_base_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 4.090358081427639\n",
      "mean absolute error is 1.390988839978189\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(gbr_tfidf_2gram_base_mse))\n",
    "print('mean absolute error is {}'.format(gbr_tfidf_2gram_base_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aba8e452ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(Y_test,gbr_base_tfidf_2gram_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If we use all the text feature, the number of features are substaintially larger than the number of data we have\n",
    "# this causes two main problem: 1. making the model prone to overfitting\n",
    "#                               2. extremely costly to tune a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 5s, sys: 2.26 s, total: 3min 7s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "count_vectorizer_1gram = CountVectorizer(stop_words = 'english', analyzer='word',binary = False, max_features= 50)\n",
    "count_vectorizer_2gram = CountVectorizer(stop_words = 'english',analyzer='word', ngram_range = (1,2) , binary = False, max_features= 50)\n",
    "tfid_vectorizer_1gram = TfidfVectorizer(stop_words = 'english', analyzer='word',binary = False, max_features= 50)\n",
    "tfid_vectorizer_2gram = TfidfVectorizer(stop_words = 'english',analyzer='word', ngram_range= (1,2), binary = False,max_features= 50)\n",
    "\n",
    "count_vectorizer_1gram.fit(x_train)\n",
    "count_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "tfid_vectorizer_1gram.fit(x_train)\n",
    "tfid_vectorizer_2gram.fit(x_train)\n",
    "\n",
    "\n",
    "#count_vectorizer\n",
    "count_train_1gram = count_vectorizer_1gram.transform(x_train)\n",
    "count_test_1gram = count_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "count_train_2gram = count_vectorizer_2gram.transform(x_train)\n",
    "count_test_2gram = count_vectorizer_2gram.transform(x_test)\n",
    "\n",
    "#tfid_vectorizer\n",
    "tf_train_1gram = tfid_vectorizer_1gram.transform(x_train)\n",
    "tf_test_1gram = tfid_vectorizer_1gram.transform(x_test)\n",
    "\n",
    "tf_train_2gram = tfid_vectorizer_2gram.transform(x_train)\n",
    "tf_test_2gram = tfid_vectorizer_2gram.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing cross validation for gradient boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 31s, sys: 31 ms, total: 2min 31s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gbr = GradientBoostingRegressor()\n",
    "parameters = {'n_estimators': [10, 50, 100, 200, 300], 'max_depth':[2,3,4,5], 'max_features':['auto','sqrt','log2']}\n",
    "gbr_cv = GridSearchCV(gbr, parameters)\n",
    "gbr_cv.fit(tf_train_2gram.todense(), Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 2, 'max_features': 'log2', 'n_estimators': 10}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=2,\n",
       "             max_features='sqrt', max_leaf_nodes=None,\n",
       "             min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=10, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr_opt = GradientBoostingRegressor(max_depth= 2, max_features='sqrt', n_estimators= 10)\n",
    "gbr_opt.fit(tf_train_2gram, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr_opt_tfidf_2gram_pred = gbr_opt.predict(tf_test_2gram.todense())\n",
    "gbr_opt_tfidf_2gram_mse = mean_squared_error(Y_test, gbr_opt_tfidf_2gram_pred)\n",
    "gbr_opt_tfidf_2gram_mae = mean_absolute_error(Y_test, gbr_opt_tfidf_2gram_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error is 4.016127097816454\n",
      "mean absolute error is 1.379458460705143\n"
     ]
    }
   ],
   "source": [
    "print('mean squared error is {}'.format(gbr_opt_tfidf_2gram_mse))\n",
    "print('mean absolute error is {}'.format(gbr_opt_tfidf_2gram_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pd.DataFrame(tf_train_2gram.todense(), index = x_train.index , columns = tfid_vectorizer_2gram.get_feature_names())\n",
    "df_test_x = pd.DataFrame(tf_test_2gram.todense(), index = x_test.index , columns = tfid_vectorizer_2gram.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['18',\n",
       " '2d',\n",
       " '3d',\n",
       " 'appeal',\n",
       " 'case',\n",
       " 'cir',\n",
       " 'cocaine',\n",
       " 'conduct',\n",
       " 'conspiracy',\n",
       " 'conviction',\n",
       " 'counsel',\n",
       " 'court',\n",
       " 'criminal',\n",
       " 'ct',\n",
       " 'defendant',\n",
       " 'defendants',\n",
       " 'denied',\n",
       " 'did',\n",
       " 'district',\n",
       " 'district court',\n",
       " 'does',\n",
       " 'drug',\n",
       " 'ed',\n",
       " 'ed 2d',\n",
       " 'error',\n",
       " 'evidence',\n",
       " 'federal',\n",
       " 'government',\n",
       " 'guidelines',\n",
       " 'guilty',\n",
       " 'id',\n",
       " 'issue',\n",
       " 'judge',\n",
       " 'jury',\n",
       " 'law',\n",
       " 'motion',\n",
       " 'mr',\n",
       " 'offense',\n",
       " 'reasonable',\n",
       " 'rule',\n",
       " 'sentence',\n",
       " 'sentencing',\n",
       " 'state',\n",
       " 'states',\n",
       " 'testimony',\n",
       " 'time',\n",
       " 'trial',\n",
       " 'united',\n",
       " 'united states',\n",
       " 'violation']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfid_vectorizer_2gram.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>appeal</th>\n",
       "      <th>case</th>\n",
       "      <th>cir</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>conduct</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>conviction</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentencing</th>\n",
       "      <th>state</th>\n",
       "      <th>states</th>\n",
       "      <th>testimony</th>\n",
       "      <th>time</th>\n",
       "      <th>trial</th>\n",
       "      <th>united</th>\n",
       "      <th>united states</th>\n",
       "      <th>violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>0.169819</td>\n",
       "      <td>0.054873</td>\n",
       "      <td>0.174669</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.410566</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056193</td>\n",
       "      <td>0.080041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148274</td>\n",
       "      <td>0.045451</td>\n",
       "      <td>0.170210</td>\n",
       "      <td>0.179406</td>\n",
       "      <td>0.135736</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.118620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.144221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045582</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265125</td>\n",
       "      <td>0.169939</td>\n",
       "      <td>0.122077</td>\n",
       "      <td>0.228952</td>\n",
       "      <td>0.064334</td>\n",
       "      <td>0.048184</td>\n",
       "      <td>0.050788</td>\n",
       "      <td>0.230550</td>\n",
       "      <td>0.230587</td>\n",
       "      <td>0.095943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.304782</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253637</td>\n",
       "      <td>0.535524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.039881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043578</td>\n",
       "      <td>0.046555</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250884</td>\n",
       "      <td>0.237926</td>\n",
       "      <td>0.039600</td>\n",
       "      <td>0.187828</td>\n",
       "      <td>0.252636</td>\n",
       "      <td>0.252676</td>\n",
       "      <td>0.019713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.167126</td>\n",
       "      <td>0.038516</td>\n",
       "      <td>0.052612</td>\n",
       "      <td>0.195583</td>\n",
       "      <td>0.653837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044805</td>\n",
       "      <td>0.191461</td>\n",
       "      <td>0.025788</td>\n",
       "      <td>0.274068</td>\n",
       "      <td>0.190263</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.257488</td>\n",
       "      <td>0.227279</td>\n",
       "      <td>0.227315</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.076738</td>\n",
       "      <td>0.254158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050932</td>\n",
       "      <td>0.066260</td>\n",
       "      <td>0.086211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.681995</td>\n",
       "      <td>0.046476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008464</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231462</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0.030766</td>\n",
       "      <td>0.129712</td>\n",
       "      <td>0.220810</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>0.045945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            18        2d        3d    appeal      case       cir   cocaine  \\\n",
       "5577  0.169819  0.054873  0.174669  0.080508  0.410566  0.068137  0.000000   \n",
       "2072  0.144221  0.000000  0.000000  0.045582  0.041509  0.000000  0.000000   \n",
       "7052  0.039509  0.031916  0.304782  0.018731  0.000000  0.253637  0.535524   \n",
       "1582  0.000000  0.131258  0.167126  0.038516  0.052612  0.195583  0.653837   \n",
       "587   0.076738  0.254158  0.000000  0.050932  0.066260  0.086211  0.000000   \n",
       "\n",
       "       conduct  conspiracy  conviction    ...      sentence  sentencing  \\\n",
       "5577  0.000000    0.000000    0.171416    ...      0.056193    0.080041   \n",
       "2072  0.173358    0.000000    0.048526    ...      0.265125    0.169939   \n",
       "7052  0.000000    0.030270    0.039881    ...      0.043578    0.046555   \n",
       "1582  0.000000    0.000000    0.041003    ...      0.044805    0.191461   \n",
       "587   0.009224    0.681995    0.046476    ...      0.008464    0.000000   \n",
       "\n",
       "         state    states  testimony      time     trial    united  \\\n",
       "5577  0.000000  0.148274   0.045451  0.170210  0.179406  0.135736   \n",
       "2072  0.122077  0.228952   0.064334  0.048184  0.050788  0.230550   \n",
       "7052  0.000000  0.250884   0.237926  0.039600  0.187828  0.252636   \n",
       "1582  0.025788  0.274068   0.190263  0.000000  0.257488  0.227279   \n",
       "587   0.000000  0.231462   0.010269  0.030766  0.129712  0.220810   \n",
       "\n",
       "      united states  violation  \n",
       "5577       0.135757   0.118620  \n",
       "2072       0.230587   0.095943  \n",
       "7052       0.252676   0.019713  \n",
       "1582       0.227315   0.000000  \n",
       "587        0.220845   0.045945  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>appeal</th>\n",
       "      <th>case</th>\n",
       "      <th>cir</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>conduct</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>conviction</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentencing</th>\n",
       "      <th>state</th>\n",
       "      <th>states</th>\n",
       "      <th>testimony</th>\n",
       "      <th>time</th>\n",
       "      <th>trial</th>\n",
       "      <th>united</th>\n",
       "      <th>united states</th>\n",
       "      <th>violation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370186</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056526</td>\n",
       "      <td>0.210134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150469</td>\n",
       "      <td>0.066081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366283</td>\n",
       "      <td>0.366341</td>\n",
       "      <td>0.163315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022408</td>\n",
       "      <td>0.054303</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125695</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.049006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024715</td>\n",
       "      <td>0.105613</td>\n",
       "      <td>0.037934</td>\n",
       "      <td>0.056322</td>\n",
       "      <td>0.064971</td>\n",
       "      <td>0.018716</td>\n",
       "      <td>0.059181</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.053739</td>\n",
       "      <td>0.007453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.036440</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.157903</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.061322</td>\n",
       "      <td>0.312683</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064308</td>\n",
       "      <td>0.034350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156189</td>\n",
       "      <td>0.165799</td>\n",
       "      <td>0.051133</td>\n",
       "      <td>0.177085</td>\n",
       "      <td>0.151455</td>\n",
       "      <td>0.151479</td>\n",
       "      <td>0.014545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.123592</td>\n",
       "      <td>0.076799</td>\n",
       "      <td>0.097786</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>0.061032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073404</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>0.675973</td>\n",
       "      <td>0.090555</td>\n",
       "      <td>0.025445</td>\n",
       "      <td>0.076231</td>\n",
       "      <td>0.100438</td>\n",
       "      <td>0.091187</td>\n",
       "      <td>0.091202</td>\n",
       "      <td>0.018974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.168303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014129</td>\n",
       "      <td>0.151002</td>\n",
       "      <td>0.457441</td>\n",
       "      <td>0.029503</td>\n",
       "      <td>0.188046</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027072</td>\n",
       "      <td>0.077123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162351</td>\n",
       "      <td>0.164230</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.060503</td>\n",
       "      <td>0.163485</td>\n",
       "      <td>0.163511</td>\n",
       "      <td>0.024492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          18        2d        3d    appeal      case       cir   cocaine  \\\n",
       "2   0.000000  0.370186  0.033668  0.000000  0.056526  0.210134  0.000000   \n",
       "5   0.022408  0.054303  0.042254  0.003541  0.032246  0.047950  0.000000   \n",
       "17  0.036440  0.194279  0.000000  0.027640  0.056635  0.157903  0.432177   \n",
       "18  0.123592  0.076799  0.097786  0.054085  0.090298  0.061032  0.000000   \n",
       "23  0.008181  0.138790  0.168303  0.000000  0.014129  0.151002  0.457441   \n",
       "\n",
       "     conduct  conspiracy  conviction    ...      sentence  sentencing  \\\n",
       "2   0.000000    0.150469    0.066081    ...      0.000000    0.000000   \n",
       "5   0.125695    0.034335    0.049006    ...      0.024715    0.105613   \n",
       "17  0.061322    0.312683    0.036782    ...      0.064308    0.034350   \n",
       "18  0.034283    0.000000    0.143947    ...      0.073404    0.056012   \n",
       "23  0.029503    0.188046    0.008258    ...      0.027072    0.077123   \n",
       "\n",
       "       state    states  testimony      time     trial    united  \\\n",
       "2   0.000000  0.363743   0.000000  0.065616  0.000000  0.366283   \n",
       "5   0.037934  0.056322   0.064971  0.018716  0.059181  0.053731   \n",
       "17  0.000000  0.156189   0.165799  0.051133  0.177085  0.151455   \n",
       "18  0.675973  0.090555   0.025445  0.076231  0.100438  0.091187   \n",
       "23  0.000000  0.162351   0.164230  0.016401  0.060503  0.163485   \n",
       "\n",
       "    united states  violation  \n",
       "2        0.366341   0.163315  \n",
       "5        0.053739   0.007453  \n",
       "17       0.151479   0.014545  \n",
       "18       0.091202   0.018974  \n",
       "23       0.163511   0.024492  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = df_train_x \n",
    "df_test = df_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# append columns:\n",
    "df_train[['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']] = train_data[['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']]\n",
    "df_test[['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']] = test_data[['Affirmed', 'AffirmedInPart', 'Reversed', 'ReversedInPart', 'Vacated', 'VacatedInPart','length_3m_dif']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>appeal</th>\n",
       "      <th>case</th>\n",
       "      <th>cir</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>conduct</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>conviction</th>\n",
       "      <th>...</th>\n",
       "      <th>united</th>\n",
       "      <th>united states</th>\n",
       "      <th>violation</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>0.169819</td>\n",
       "      <td>0.054873</td>\n",
       "      <td>0.174669</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.410566</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.171416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135736</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.118620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.766357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.144221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045582</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.048526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230550</td>\n",
       "      <td>0.230587</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.304782</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253637</td>\n",
       "      <td>0.535524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>0.039881</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252636</td>\n",
       "      <td>0.252676</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.167126</td>\n",
       "      <td>0.038516</td>\n",
       "      <td>0.052612</td>\n",
       "      <td>0.195583</td>\n",
       "      <td>0.653837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227279</td>\n",
       "      <td>0.227315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.150298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.076738</td>\n",
       "      <td>0.254158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050932</td>\n",
       "      <td>0.066260</td>\n",
       "      <td>0.086211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.681995</td>\n",
       "      <td>0.046476</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220810</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>0.045945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.467751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            18        2d        3d    appeal      case       cir   cocaine  \\\n",
       "5577  0.169819  0.054873  0.174669  0.080508  0.410566  0.068137  0.000000   \n",
       "2072  0.144221  0.000000  0.000000  0.045582  0.041509  0.000000  0.000000   \n",
       "7052  0.039509  0.031916  0.304782  0.018731  0.000000  0.253637  0.535524   \n",
       "1582  0.000000  0.131258  0.167126  0.038516  0.052612  0.195583  0.653837   \n",
       "587   0.076738  0.254158  0.000000  0.050932  0.066260  0.086211  0.000000   \n",
       "\n",
       "       conduct  conspiracy  conviction      ...          united  \\\n",
       "5577  0.000000    0.000000    0.171416      ...        0.135736   \n",
       "2072  0.173358    0.000000    0.048526      ...        0.230550   \n",
       "7052  0.000000    0.030270    0.039881      ...        0.252636   \n",
       "1582  0.000000    0.000000    0.041003      ...        0.227279   \n",
       "587   0.009224    0.681995    0.046476      ...        0.220810   \n",
       "\n",
       "      united states  violation  Affirmed  AffirmedInPart  Reversed  \\\n",
       "5577       0.135757   0.118620       0.0             0.0       0.0   \n",
       "2072       0.230587   0.095943       1.0             0.0       0.0   \n",
       "7052       0.252676   0.019713       1.0             0.0       0.0   \n",
       "1582       0.227315   0.000000       1.0             0.0       0.0   \n",
       "587        0.220845   0.045945       0.0             0.0       0.0   \n",
       "\n",
       "      ReversedInPart  Vacated  VacatedInPart  length_3m_dif  \n",
       "5577             1.0      0.0            0.0      -0.766357  \n",
       "2072             0.0      0.0            0.0       0.608711  \n",
       "7052             0.0      0.0            0.0       0.228592  \n",
       "1582             0.0      0.0            0.0      -3.150298  \n",
       "587              0.0      0.0            0.0      -2.467751  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>18</th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>appeal</th>\n",
       "      <th>case</th>\n",
       "      <th>cir</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>conduct</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>conviction</th>\n",
       "      <th>...</th>\n",
       "      <th>united</th>\n",
       "      <th>united states</th>\n",
       "      <th>violation</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.370186</td>\n",
       "      <td>0.033668</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056526</td>\n",
       "      <td>0.210134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150469</td>\n",
       "      <td>0.066081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.366283</td>\n",
       "      <td>0.366341</td>\n",
       "      <td>0.163315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.563471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.022408</td>\n",
       "      <td>0.054303</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.032246</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125695</td>\n",
       "      <td>0.034335</td>\n",
       "      <td>0.049006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053731</td>\n",
       "      <td>0.053739</td>\n",
       "      <td>0.007453</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.591201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.036440</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027640</td>\n",
       "      <td>0.056635</td>\n",
       "      <td>0.157903</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.061322</td>\n",
       "      <td>0.312683</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151455</td>\n",
       "      <td>0.151479</td>\n",
       "      <td>0.014545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.959357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.123592</td>\n",
       "      <td>0.076799</td>\n",
       "      <td>0.097786</td>\n",
       "      <td>0.054085</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>0.061032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091187</td>\n",
       "      <td>0.091202</td>\n",
       "      <td>0.018974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.595323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.138790</td>\n",
       "      <td>0.168303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014129</td>\n",
       "      <td>0.151002</td>\n",
       "      <td>0.457441</td>\n",
       "      <td>0.029503</td>\n",
       "      <td>0.188046</td>\n",
       "      <td>0.008258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163485</td>\n",
       "      <td>0.163511</td>\n",
       "      <td>0.024492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.045180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          18        2d        3d    appeal      case       cir   cocaine  \\\n",
       "2   0.000000  0.370186  0.033668  0.000000  0.056526  0.210134  0.000000   \n",
       "5   0.022408  0.054303  0.042254  0.003541  0.032246  0.047950  0.000000   \n",
       "17  0.036440  0.194279  0.000000  0.027640  0.056635  0.157903  0.432177   \n",
       "18  0.123592  0.076799  0.097786  0.054085  0.090298  0.061032  0.000000   \n",
       "23  0.008181  0.138790  0.168303  0.000000  0.014129  0.151002  0.457441   \n",
       "\n",
       "     conduct  conspiracy  conviction      ...          united  united states  \\\n",
       "2   0.000000    0.150469    0.066081      ...        0.366283       0.366341   \n",
       "5   0.125695    0.034335    0.049006      ...        0.053731       0.053739   \n",
       "17  0.061322    0.312683    0.036782      ...        0.151455       0.151479   \n",
       "18  0.034283    0.000000    0.143947      ...        0.091187       0.091202   \n",
       "23  0.029503    0.188046    0.008258      ...        0.163485       0.163511   \n",
       "\n",
       "    violation  Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "2    0.163315       0.0             0.0       1.0             0.0      0.0   \n",
       "5    0.007453       1.0             0.0       0.0             0.0      0.0   \n",
       "17   0.014545       1.0             0.0       0.0             0.0      0.0   \n",
       "18   0.018974       1.0             0.0       0.0             0.0      0.0   \n",
       "23   0.024492       1.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "    VacatedInPart  length_3m_dif  \n",
       "2             0.0      -3.563471  \n",
       "5             0.0       3.591201  \n",
       "17            0.0       0.959357  \n",
       "18            0.0      -1.595323  \n",
       "23            0.0      -2.045180  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>txt</th>\n",
       "      <th>length_3m_dif</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5577</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>opinion of the court alito, circuit judge. th...</td>\n",
       "      <td>-0.766357</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>politz , chief judge: charles arthur daughenb...</td>\n",
       "      <td>0.608711</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>cyr, senior circuit judge. percio reynoso app...</td>\n",
       "      <td>0.228592</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1582</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>per curiam: appellant david earl kates, sente...</td>\n",
       "      <td>-3.150298</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>george c. pratt , circuit judge: in theory th...</td>\n",
       "      <td>-2.467751</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "5577       0.0             0.0       0.0             1.0      0.0   \n",
       "2072       1.0             0.0       0.0             0.0      0.0   \n",
       "7052       1.0             0.0       0.0             0.0      0.0   \n",
       "1582       1.0             0.0       0.0             0.0      0.0   \n",
       "587        0.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "      VacatedInPart                                                txt  \\\n",
       "5577            0.0   opinion of the court alito, circuit judge. th...   \n",
       "2072            0.0   politz , chief judge: charles arthur daughenb...   \n",
       "7052            0.0   cyr, senior circuit judge. percio reynoso app...   \n",
       "1582            0.0   per curiam: appellant david earl kates, sente...   \n",
       "587             0.0   george c. pratt , circuit judge: in theory th...   \n",
       "\n",
       "      length_3m_dif  bin  \n",
       "5577      -0.766357    0  \n",
       "2072       0.608711    2  \n",
       "7052       0.228592    2  \n",
       "1582      -3.150298    0  \n",
       "587       -2.467751    0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train.to_csv(\"nn_prepared_train.csv\", sep=',')\n",
    "#df_test.to_csv(\"nn_prepared_test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"nn_prepared_train.csv\", sep=',')\n",
    "df_test = pd.read_csv(\"nn_prepared_test.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>18</th>\n",
       "      <th>2d</th>\n",
       "      <th>3d</th>\n",
       "      <th>appeal</th>\n",
       "      <th>case</th>\n",
       "      <th>cir</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>conduct</th>\n",
       "      <th>conspiracy</th>\n",
       "      <th>...</th>\n",
       "      <th>united</th>\n",
       "      <th>united states</th>\n",
       "      <th>violation</th>\n",
       "      <th>Affirmed</th>\n",
       "      <th>AffirmedInPart</th>\n",
       "      <th>Reversed</th>\n",
       "      <th>ReversedInPart</th>\n",
       "      <th>Vacated</th>\n",
       "      <th>VacatedInPart</th>\n",
       "      <th>length_3m_dif</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5577</td>\n",
       "      <td>0.169819</td>\n",
       "      <td>0.054873</td>\n",
       "      <td>0.174669</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.410566</td>\n",
       "      <td>0.068137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135736</td>\n",
       "      <td>0.135757</td>\n",
       "      <td>0.118620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.766357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2072</td>\n",
       "      <td>0.144221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045582</td>\n",
       "      <td>0.041509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230550</td>\n",
       "      <td>0.230587</td>\n",
       "      <td>0.095943</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.608711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7052</td>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.304782</td>\n",
       "      <td>0.018731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.253637</td>\n",
       "      <td>0.535524</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030270</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252636</td>\n",
       "      <td>0.252676</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.228592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1582</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131258</td>\n",
       "      <td>0.167126</td>\n",
       "      <td>0.038516</td>\n",
       "      <td>0.052612</td>\n",
       "      <td>0.195583</td>\n",
       "      <td>0.653837</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227279</td>\n",
       "      <td>0.227315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.150298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>587</td>\n",
       "      <td>0.076738</td>\n",
       "      <td>0.254158</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050932</td>\n",
       "      <td>0.066260</td>\n",
       "      <td>0.086211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009224</td>\n",
       "      <td>0.681995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.220810</td>\n",
       "      <td>0.220845</td>\n",
       "      <td>0.045945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.467751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        18        2d        3d    appeal      case       cir  \\\n",
       "0        5577  0.169819  0.054873  0.174669  0.080508  0.410566  0.068137   \n",
       "1        2072  0.144221  0.000000  0.000000  0.045582  0.041509  0.000000   \n",
       "2        7052  0.039509  0.031916  0.304782  0.018731  0.000000  0.253637   \n",
       "3        1582  0.000000  0.131258  0.167126  0.038516  0.052612  0.195583   \n",
       "4         587  0.076738  0.254158  0.000000  0.050932  0.066260  0.086211   \n",
       "\n",
       "    cocaine   conduct  conspiracy      ...          united  united states  \\\n",
       "0  0.000000  0.000000    0.000000      ...        0.135736       0.135757   \n",
       "1  0.000000  0.173358    0.000000      ...        0.230550       0.230587   \n",
       "2  0.535524  0.000000    0.030270      ...        0.252636       0.252676   \n",
       "3  0.653837  0.000000    0.000000      ...        0.227279       0.227315   \n",
       "4  0.000000  0.009224    0.681995      ...        0.220810       0.220845   \n",
       "\n",
       "   violation  Affirmed  AffirmedInPart  Reversed  ReversedInPart  Vacated  \\\n",
       "0   0.118620       0.0             0.0       0.0             1.0      0.0   \n",
       "1   0.095943       1.0             0.0       0.0             0.0      0.0   \n",
       "2   0.019713       1.0             0.0       0.0             0.0      0.0   \n",
       "3   0.000000       1.0             0.0       0.0             0.0      0.0   \n",
       "4   0.045945       0.0             0.0       0.0             0.0      0.0   \n",
       "\n",
       "   VacatedInPart  length_3m_dif  \n",
       "0            0.0      -0.766357  \n",
       "1            0.0       0.608711  \n",
       "2            0.0       0.228592  \n",
       "3            0.0      -3.150298  \n",
       "4            0.0      -2.467751  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6326 entries, 0 to 6325\n",
      "Data columns (total 58 columns):\n",
      "Unnamed: 0        6326 non-null int64\n",
      "18                6326 non-null float64\n",
      "2d                6326 non-null float64\n",
      "3d                6326 non-null float64\n",
      "appeal            6326 non-null float64\n",
      "case              6326 non-null float64\n",
      "cir               6326 non-null float64\n",
      "cocaine           6326 non-null float64\n",
      "conduct           6326 non-null float64\n",
      "conspiracy        6326 non-null float64\n",
      "conviction        6326 non-null float64\n",
      "counsel           6326 non-null float64\n",
      "court             6326 non-null float64\n",
      "criminal          6326 non-null float64\n",
      "ct                6326 non-null float64\n",
      "defendant         6326 non-null float64\n",
      "defendants        6326 non-null float64\n",
      "denied            6326 non-null float64\n",
      "did               6326 non-null float64\n",
      "district          6326 non-null float64\n",
      "district court    6326 non-null float64\n",
      "does              6326 non-null float64\n",
      "drug              6326 non-null float64\n",
      "ed                6326 non-null float64\n",
      "ed 2d             6326 non-null float64\n",
      "error             6326 non-null float64\n",
      "evidence          6326 non-null float64\n",
      "federal           6326 non-null float64\n",
      "government        6326 non-null float64\n",
      "guidelines        6326 non-null float64\n",
      "guilty            6326 non-null float64\n",
      "id                6326 non-null float64\n",
      "issue             6326 non-null float64\n",
      "judge             6326 non-null float64\n",
      "jury              6326 non-null float64\n",
      "law               6326 non-null float64\n",
      "motion            6326 non-null float64\n",
      "mr                6326 non-null float64\n",
      "offense           6326 non-null float64\n",
      "reasonable        6326 non-null float64\n",
      "rule              6326 non-null float64\n",
      "sentence          6326 non-null float64\n",
      "sentencing        6326 non-null float64\n",
      "state             6326 non-null float64\n",
      "states            6326 non-null float64\n",
      "testimony         6326 non-null float64\n",
      "time              6326 non-null float64\n",
      "trial             6326 non-null float64\n",
      "united            6326 non-null float64\n",
      "united states     6326 non-null float64\n",
      "violation         6326 non-null float64\n",
      "Affirmed          6326 non-null float64\n",
      "AffirmedInPart    6326 non-null float64\n",
      "Reversed          6326 non-null float64\n",
      "ReversedInPart    6326 non-null float64\n",
      "Vacated           6326 non-null float64\n",
      "VacatedInPart     6326 non-null float64\n",
      "length_3m_dif     6326 non-null float64\n",
      "dtypes: float64(57), int64(1)\n",
      "memory usage: 2.8 MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic nn architecture:\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#used for batch training:\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in data\n",
    "xy_train = np.loadtxt(\"nn_prepared_train.csv\", delimiter = ',' ,skiprows=1, dtype= np.float64 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xy_test = np.loadtxt(\"nn_prepared_test.csv\", delimiter = ',', skiprows =1 ,dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = torch.from_numpy(xy_test[:,1:-1])\n",
    "y_test = torch.from_numpy(xy_test[:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = Variable(x_test.float())\n",
    "y_test = Variable(y_test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.from_numpy(xy_train[:,1:-1])\n",
    "y_train = torch.from_numpy(xy_train[:,[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = Variable(x_train.float())\n",
    "y_train = Variable(y_train.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define our own nn:\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_features, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden=torch.nn.Linear(n_features,n_hidden) \n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.predict=torch.nn.Linear(n_hidden,1) \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.hidden(x)) \n",
    "        x=self.hidden2(x)\n",
    "        x=self.predict(x) \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initialize the net:\n",
    "n_features = 56\n",
    "net = Net(56, 20, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=torch.optim.SGD(net.parameters(),lr=0.3)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "loss_func_MAe=torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (hidden): Linear (56 -> 20)\n",
      "  (hidden2): Linear (20 -> 20)\n",
      "  (predict): Linear (20 -> 1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3.239598274230957\n",
      "1\n",
      "3.1036558151245117\n",
      "2\n",
      "3.103203535079956\n",
      "3\n",
      "3.1031510829925537\n",
      "4\n",
      "3.103101968765259\n",
      "5\n",
      "3.1030478477478027\n",
      "6\n",
      "3.1030044555664062\n",
      "7\n",
      "3.102952241897583\n",
      "8\n",
      "3.1029138565063477\n",
      "9\n",
      "3.1028659343719482\n",
      "10\n",
      "3.102818489074707\n",
      "11\n",
      "3.1027672290802\n",
      "12\n",
      "3.102726459503174\n",
      "13\n",
      "3.1026837825775146\n",
      "14\n",
      "3.102633237838745\n",
      "15\n",
      "3.102586269378662\n",
      "16\n",
      "3.102531671524048\n",
      "17\n",
      "3.102492332458496\n",
      "18\n",
      "3.102454423904419\n",
      "19\n",
      "3.102400779724121\n",
      "20\n",
      "3.1023619174957275\n",
      "21\n",
      "3.1023142337799072\n",
      "22\n",
      "3.102264642715454\n",
      "23\n",
      "3.1022262573242188\n",
      "24\n",
      "3.1021738052368164\n",
      "25\n",
      "3.102128744125366\n",
      "26\n",
      "3.1020700931549072\n",
      "27\n",
      "3.1020348072052\n",
      "28\n",
      "3.1019887924194336\n",
      "29\n",
      "3.101942777633667\n",
      "30\n",
      "3.101902961730957\n",
      "31\n",
      "3.1018640995025635\n",
      "32\n",
      "3.1018190383911133\n",
      "33\n",
      "3.1017730236053467\n",
      "34\n",
      "3.1017355918884277\n",
      "35\n",
      "3.1016805171966553\n",
      "36\n",
      "3.1016335487365723\n",
      "37\n",
      "3.101593017578125\n",
      "38\n",
      "3.101541757583618\n",
      "39\n",
      "3.1015028953552246\n",
      "40\n",
      "3.101447343826294\n",
      "41\n",
      "3.101409435272217\n",
      "42\n",
      "3.101364850997925\n",
      "43\n",
      "3.1013078689575195\n",
      "44\n",
      "3.1012606620788574\n",
      "45\n",
      "3.1012160778045654\n",
      "46\n",
      "3.101154327392578\n",
      "47\n",
      "3.101104974746704\n",
      "48\n",
      "3.1010494232177734\n",
      "49\n",
      "3.1010048389434814\n",
      "50\n",
      "3.1009490489959717\n",
      "51\n",
      "3.1008901596069336\n",
      "52\n",
      "3.1008379459381104\n",
      "53\n",
      "3.100778341293335\n",
      "54\n",
      "3.1007187366485596\n",
      "55\n",
      "3.1006596088409424\n",
      "56\n",
      "3.1005992889404297\n",
      "57\n",
      "3.1005442142486572\n",
      "58\n",
      "3.1004889011383057\n",
      "59\n",
      "3.1004257202148438\n",
      "60\n",
      "3.100372076034546\n",
      "61\n",
      "3.1003105640411377\n",
      "62\n",
      "3.100245714187622\n",
      "63\n",
      "3.100184679031372\n",
      "64\n",
      "3.1001181602478027\n",
      "65\n",
      "3.100053548812866\n",
      "66\n",
      "3.099990129470825\n",
      "67\n",
      "3.0999345779418945\n",
      "68\n",
      "3.0998752117156982\n",
      "69\n",
      "3.099811553955078\n",
      "70\n",
      "3.0997462272644043\n",
      "71\n",
      "3.099674940109253\n",
      "72\n",
      "3.0996079444885254\n",
      "73\n",
      "3.0995421409606934\n",
      "74\n",
      "3.099473237991333\n",
      "75\n",
      "3.099409580230713\n",
      "76\n",
      "3.0993475914001465\n",
      "77\n",
      "3.0992815494537354\n",
      "78\n",
      "3.0992019176483154\n",
      "79\n",
      "3.099135160446167\n",
      "80\n",
      "3.099074602127075\n",
      "81\n",
      "3.0989978313446045\n",
      "82\n",
      "3.0989227294921875\n",
      "83\n",
      "3.0988526344299316\n",
      "84\n",
      "3.0987796783447266\n",
      "85\n",
      "3.0987071990966797\n",
      "86\n",
      "3.0986328125\n",
      "87\n",
      "3.0985605716705322\n",
      "88\n",
      "3.0984842777252197\n",
      "89\n",
      "3.0984160900115967\n",
      "90\n",
      "3.0983409881591797\n",
      "91\n",
      "3.098256826400757\n",
      "92\n",
      "3.098191022872925\n",
      "93\n",
      "3.0981171131134033\n",
      "94\n",
      "3.0980384349823\n",
      "95\n",
      "3.097954750061035\n",
      "96\n",
      "3.097874879837036\n",
      "97\n",
      "3.097794771194458\n",
      "98\n",
      "3.097705602645874\n",
      "99\n",
      "3.097620964050293\n",
      "100\n",
      "3.0975372791290283\n",
      "101\n",
      "3.097452402114868\n",
      "102\n",
      "3.097379684448242\n",
      "103\n",
      "3.097297430038452\n",
      "104\n",
      "3.097215414047241\n",
      "105\n",
      "3.0971336364746094\n",
      "106\n",
      "3.097057819366455\n",
      "107\n",
      "3.09698224067688\n",
      "108\n",
      "3.096904993057251\n",
      "109\n",
      "3.0968244075775146\n",
      "110\n",
      "3.096742630004883\n",
      "111\n",
      "3.096665382385254\n",
      "112\n",
      "3.0965840816497803\n",
      "113\n",
      "3.0964975357055664\n",
      "114\n",
      "3.096419334411621\n",
      "115\n",
      "3.096339702606201\n",
      "116\n",
      "3.096250295639038\n",
      "117\n",
      "3.0961685180664062\n",
      "118\n",
      "3.0960798263549805\n",
      "119\n",
      "3.0960073471069336\n",
      "120\n",
      "3.095925807952881\n",
      "121\n",
      "3.0958385467529297\n",
      "122\n",
      "3.0957534313201904\n",
      "123\n",
      "3.095672845840454\n",
      "124\n",
      "3.0955910682678223\n",
      "125\n",
      "3.0955076217651367\n",
      "126\n",
      "3.09541654586792\n",
      "127\n",
      "3.0953376293182373\n",
      "128\n",
      "3.0952486991882324\n",
      "129\n",
      "3.095165967941284\n",
      "130\n",
      "3.0950775146484375\n",
      "131\n",
      "3.0949931144714355\n",
      "132\n",
      "3.094904661178589\n",
      "133\n",
      "3.0948197841644287\n",
      "134\n",
      "3.0947389602661133\n",
      "135\n",
      "3.0946385860443115\n",
      "136\n",
      "3.0945611000061035\n",
      "137\n",
      "3.094470977783203\n",
      "138\n",
      "3.09438419342041\n",
      "139\n",
      "3.0942904949188232\n",
      "140\n",
      "3.0941920280456543\n",
      "141\n",
      "3.094104766845703\n",
      "142\n",
      "3.094025135040283\n",
      "143\n",
      "3.0939254760742188\n",
      "144\n",
      "3.0938408374786377\n",
      "145\n",
      "3.0937485694885254\n",
      "146\n",
      "3.093656539916992\n",
      "147\n",
      "3.0935556888580322\n",
      "148\n",
      "3.0934677124023438\n",
      "149\n",
      "3.093372106552124\n",
      "150\n",
      "3.0932817459106445\n",
      "151\n",
      "3.0931882858276367\n",
      "152\n",
      "3.0930895805358887\n",
      "153\n",
      "3.092993974685669\n",
      "154\n",
      "3.0929033756256104\n",
      "155\n",
      "3.0928103923797607\n",
      "156\n",
      "3.092714786529541\n",
      "157\n",
      "3.0926125049591064\n",
      "158\n",
      "3.092517137527466\n",
      "159\n",
      "3.0924229621887207\n",
      "160\n",
      "3.0923266410827637\n",
      "161\n",
      "3.092216968536377\n",
      "162\n",
      "3.092128276824951\n",
      "163\n",
      "3.092027425765991\n",
      "164\n",
      "3.091925859451294\n",
      "165\n",
      "3.091825246810913\n",
      "166\n",
      "3.0917181968688965\n",
      "167\n",
      "3.091623067855835\n",
      "168\n",
      "3.09151291847229\n",
      "169\n",
      "3.0914127826690674\n",
      "170\n",
      "3.091313123703003\n",
      "171\n",
      "3.09120774269104\n",
      "172\n",
      "3.09110689163208\n",
      "173\n",
      "3.091000556945801\n",
      "174\n",
      "3.090881824493408\n",
      "175\n",
      "3.090777635574341\n",
      "176\n",
      "3.0906689167022705\n",
      "177\n",
      "3.0905656814575195\n",
      "178\n",
      "3.0904510021209717\n",
      "179\n",
      "3.090341329574585\n",
      "180\n",
      "3.090230703353882\n",
      "181\n",
      "3.090123176574707\n",
      "182\n",
      "3.0900089740753174\n",
      "183\n",
      "3.089898109436035\n",
      "184\n",
      "3.0897841453552246\n",
      "185\n",
      "3.0896639823913574\n",
      "186\n",
      "3.0895440578460693\n",
      "187\n",
      "3.089423894882202\n",
      "188\n",
      "3.0893118381500244\n",
      "189\n",
      "3.089189291000366\n",
      "190\n",
      "3.0890724658966064\n",
      "191\n",
      "3.088953733444214\n",
      "192\n",
      "3.0888335704803467\n",
      "193\n",
      "3.088710069656372\n",
      "194\n",
      "3.0885848999023438\n",
      "195\n",
      "3.088459014892578\n",
      "196\n",
      "3.0883309841156006\n",
      "197\n",
      "3.08821177482605\n",
      "198\n",
      "3.088080406188965\n",
      "199\n",
      "3.087953805923462\n",
      "200\n",
      "3.0878255367279053\n",
      "201\n",
      "3.087690591812134\n",
      "202\n",
      "3.087557792663574\n",
      "203\n",
      "3.087430953979492\n",
      "204\n",
      "3.0873003005981445\n",
      "205\n",
      "3.087164878845215\n",
      "206\n",
      "3.0870282649993896\n",
      "207\n",
      "3.08689546585083\n",
      "208\n",
      "3.086759328842163\n",
      "209\n",
      "3.086622714996338\n",
      "210\n",
      "3.0864808559417725\n",
      "211\n",
      "3.0863418579101562\n",
      "212\n",
      "3.086207151412964\n",
      "213\n",
      "3.0860655307769775\n",
      "214\n",
      "3.085923910140991\n",
      "215\n",
      "3.0857763290405273\n",
      "216\n",
      "3.085637331008911\n",
      "217\n",
      "3.0855000019073486\n",
      "218\n",
      "3.0853490829467773\n",
      "219\n",
      "3.085209846496582\n",
      "220\n",
      "3.0850563049316406\n",
      "221\n",
      "3.084911346435547\n",
      "222\n",
      "3.0847654342651367\n",
      "223\n",
      "3.0846152305603027\n",
      "224\n",
      "3.084458351135254\n",
      "225\n",
      "3.0843148231506348\n",
      "226\n",
      "3.0841598510742188\n",
      "227\n",
      "3.0840060710906982\n",
      "228\n",
      "3.083850145339966\n",
      "229\n",
      "3.083709239959717\n",
      "230\n",
      "3.083570957183838\n",
      "231\n",
      "3.083414316177368\n",
      "232\n",
      "3.08328914642334\n",
      "233\n",
      "3.0831692218780518\n",
      "234\n",
      "3.083066463470459\n",
      "235\n",
      "3.082998275756836\n",
      "236\n",
      "3.0829803943634033\n",
      "237\n",
      "3.0830793380737305\n",
      "238\n",
      "3.083277940750122\n",
      "239\n",
      "3.0837583541870117\n",
      "240\n",
      "3.0846545696258545\n",
      "241\n",
      "3.0862061977386475\n",
      "242\n",
      "3.0889978408813477\n",
      "243\n",
      "3.09354305267334\n",
      "244\n",
      "3.1013405323028564\n",
      "245\n",
      "3.1120986938476562\n",
      "246\n",
      "3.127338171005249\n",
      "247\n",
      "3.139042377471924\n",
      "248\n",
      "3.14898943901062\n",
      "249\n",
      "3.1379220485687256\n",
      "250\n",
      "3.1272737979888916\n",
      "251\n",
      "3.1084446907043457\n",
      "252\n",
      "3.098961114883423\n",
      "253\n",
      "3.091698169708252\n",
      "254\n",
      "3.0882039070129395\n",
      "255\n",
      "3.085991621017456\n",
      "256\n",
      "3.0846805572509766\n",
      "257\n",
      "3.0838677883148193\n",
      "258\n",
      "3.083244562149048\n",
      "259\n",
      "3.082794427871704\n",
      "260\n",
      "3.0824382305145264\n",
      "261\n",
      "3.0821304321289062\n",
      "262\n",
      "3.0818731784820557\n",
      "263\n",
      "3.081637382507324\n",
      "264\n",
      "3.0814363956451416\n",
      "265\n",
      "3.0812463760375977\n",
      "266\n",
      "3.081102132797241\n",
      "267\n",
      "3.0809669494628906\n",
      "268\n",
      "3.0808844566345215\n",
      "269\n",
      "3.0808520317077637\n",
      "270\n",
      "3.0809342861175537\n",
      "271\n",
      "3.0810904502868652\n",
      "272\n",
      "3.0815320014953613\n",
      "273\n",
      "3.082148551940918\n",
      "274\n",
      "3.0833187103271484\n",
      "275\n",
      "3.085024118423462\n",
      "276\n",
      "3.087690591812134\n",
      "277\n",
      "3.091684103012085\n",
      "278\n",
      "3.097567081451416\n",
      "279\n",
      "3.104405403137207\n",
      "280\n",
      "3.1131067276000977\n",
      "281\n",
      "3.118903160095215\n",
      "282\n",
      "3.12396502494812\n",
      "283\n",
      "3.119426727294922\n",
      "284\n",
      "3.115236759185791\n",
      "285\n",
      "3.1049039363861084\n",
      "286\n",
      "3.098525047302246\n",
      "287\n",
      "3.0922584533691406\n",
      "288\n",
      "3.0886006355285645\n",
      "289\n",
      "3.085827589035034\n",
      "290\n",
      "3.084249973297119\n",
      "291\n",
      "3.0829827785491943\n",
      "292\n",
      "3.082301139831543\n",
      "293\n",
      "3.0817384719848633\n",
      "294\n",
      "3.081355333328247\n",
      "295\n",
      "3.081084966659546\n",
      "296\n",
      "3.080944776535034\n",
      "297\n",
      "3.0809707641601562\n",
      "298\n",
      "3.081058979034424\n",
      "299\n",
      "3.0814316272735596\n",
      "300\n",
      "3.081857681274414\n",
      "301\n",
      "3.0827012062072754\n",
      "302\n",
      "3.083726167678833\n",
      "303\n",
      "3.085355758666992\n",
      "304\n",
      "3.087563991546631\n",
      "305\n",
      "3.090172052383423\n",
      "306\n",
      "3.0936052799224854\n",
      "307\n",
      "3.0970282554626465\n",
      "308\n",
      "3.1010143756866455\n",
      "309\n",
      "3.103193521499634\n",
      "310\n",
      "3.105534553527832\n",
      "311\n",
      "3.1039090156555176\n",
      "312\n",
      "3.102985143661499\n",
      "313\n",
      "3.0990681648254395\n",
      "314\n",
      "3.0962142944335938\n",
      "315\n",
      "3.092144012451172\n",
      "316\n",
      "3.0895349979400635\n",
      "317\n",
      "3.087019443511963\n",
      "318\n",
      "3.0857529640197754\n",
      "319\n",
      "3.084486722946167\n",
      "320\n",
      "3.083909273147583\n",
      "321\n",
      "3.08325457572937\n",
      "322\n",
      "3.083036422729492\n",
      "323\n",
      "3.0826449394226074\n",
      "324\n",
      "3.0827713012695312\n",
      "325\n",
      "3.0828003883361816\n",
      "326\n",
      "3.083242416381836\n",
      "327\n",
      "3.083878517150879\n",
      "328\n",
      "3.0848278999328613\n",
      "329\n",
      "3.0859365463256836\n",
      "330\n",
      "3.087390422821045\n",
      "331\n",
      "3.0890378952026367\n",
      "332\n",
      "3.090977668762207\n",
      "333\n",
      "3.092254638671875\n",
      "334\n",
      "3.0945985317230225\n",
      "335\n",
      "3.095745325088501\n",
      "336\n",
      "3.096670150756836\n",
      "337\n",
      "3.095358371734619\n",
      "338\n",
      "3.0951759815216064\n",
      "339\n",
      "3.0929360389709473\n",
      "340\n",
      "3.0919089317321777\n",
      "341\n",
      "3.089580774307251\n",
      "342\n",
      "3.088514804840088\n",
      "343\n",
      "3.086725950241089\n",
      "344\n",
      "3.085996389389038\n",
      "345\n",
      "3.0851097106933594\n",
      "346\n",
      "3.084829330444336\n",
      "347\n",
      "3.0843117237091064\n",
      "348\n",
      "3.084432601928711\n",
      "349\n",
      "3.0843558311462402\n",
      "350\n",
      "3.084742307662964\n",
      "351\n",
      "3.084974765777588\n",
      "352\n",
      "3.085869550704956\n",
      "353\n",
      "3.0864524841308594\n",
      "354\n",
      "3.087702989578247\n",
      "355\n",
      "3.0885019302368164\n",
      "356\n",
      "3.0897443294525146\n",
      "357\n",
      "3.089916706085205\n",
      "358\n",
      "3.0907158851623535\n",
      "359\n",
      "3.090458393096924\n",
      "360\n",
      "3.0907645225524902\n",
      "361\n",
      "3.089714765548706\n",
      "362\n",
      "3.0894243717193604\n",
      "363\n",
      "3.0882928371429443\n",
      "364\n",
      "3.0878615379333496\n",
      "365\n",
      "3.0869503021240234\n",
      "366\n",
      "3.086717367172241\n",
      "367\n",
      "3.086183786392212\n",
      "368\n",
      "3.085831642150879\n",
      "369\n",
      "3.0850563049316406\n",
      "370\n",
      "3.085106611251831\n",
      "371\n",
      "3.0853517055511475\n",
      "372\n",
      "3.0854811668395996\n",
      "373\n",
      "3.0851151943206787\n",
      "374\n",
      "3.085688591003418\n",
      "375\n",
      "3.0861737728118896\n",
      "376\n",
      "3.086609363555908\n",
      "377\n",
      "3.0861964225769043\n",
      "378\n",
      "3.0868782997131348\n",
      "379\n",
      "3.0869832038879395\n",
      "380\n",
      "3.0873076915740967\n",
      "381\n",
      "3.0868375301361084\n",
      "382\n",
      "3.0874087810516357\n",
      "383\n",
      "3.0873701572418213\n",
      "384\n",
      "3.0875821113586426\n",
      "385\n",
      "3.086878538131714\n",
      "386\n",
      "3.086836338043213\n",
      "387\n",
      "3.0863873958587646\n",
      "388\n",
      "3.086395263671875\n",
      "389\n",
      "3.085681915283203\n",
      "390\n",
      "3.0858938694000244\n",
      "391\n",
      "3.085937261581421\n",
      "392\n",
      "3.0858089923858643\n",
      "393\n",
      "3.0852041244506836\n",
      "394\n",
      "3.0854389667510986\n",
      "395\n",
      "3.0854029655456543\n",
      "396\n",
      "3.0855185985565186\n",
      "397\n",
      "3.084960699081421\n",
      "398\n",
      "3.0853114128112793\n",
      "399\n",
      "3.0852999687194824\n",
      "400\n",
      "3.085597515106201\n",
      "401\n",
      "3.085130214691162\n",
      "402\n",
      "3.085561513900757\n",
      "403\n",
      "3.0856258869171143\n",
      "404\n",
      "3.08581805229187\n",
      "405\n",
      "3.0852603912353516\n",
      "406\n",
      "3.0857388973236084\n",
      "407\n",
      "3.085646390914917\n",
      "408\n",
      "3.085890769958496\n",
      "409\n",
      "3.085313558578491\n",
      "410\n",
      "3.0857176780700684\n",
      "411\n",
      "3.085677146911621\n",
      "412\n",
      "3.0856475830078125\n",
      "413\n",
      "3.0848445892333984\n",
      "414\n",
      "3.084716796875\n",
      "415\n",
      "3.0842630863189697\n",
      "416\n",
      "3.0841403007507324\n",
      "417\n",
      "3.083517551422119\n",
      "418\n",
      "3.0834109783172607\n",
      "419\n",
      "3.0832321643829346\n",
      "420\n",
      "3.083306074142456\n",
      "421\n",
      "3.083319902420044\n",
      "422\n",
      "3.083932876586914\n",
      "423\n",
      "3.0842485427856445\n",
      "424\n",
      "3.084652900695801\n",
      "425\n",
      "3.084557056427002\n",
      "426\n",
      "3.084763765335083\n",
      "427\n",
      "3.084400177001953\n",
      "428\n",
      "3.0846545696258545\n",
      "429\n",
      "3.084554672241211\n",
      "430\n",
      "3.0844855308532715\n",
      "431\n",
      "3.0836265087127686\n",
      "432\n",
      "3.0836338996887207\n",
      "433\n",
      "3.083369255065918\n",
      "434\n",
      "3.083200454711914\n",
      "435\n",
      "3.0829217433929443\n",
      "436\n",
      "3.0828938484191895\n",
      "437\n",
      "3.0822768211364746\n",
      "438\n",
      "3.082233190536499\n",
      "439\n",
      "3.0819263458251953\n",
      "440\n",
      "3.0819084644317627\n",
      "441\n",
      "3.081331253051758\n",
      "442\n",
      "3.0816595554351807\n",
      "443\n",
      "3.0817043781280518\n",
      "444\n",
      "3.082190990447998\n",
      "445\n",
      "3.0825130939483643\n",
      "446\n",
      "3.0829720497131348\n",
      "447\n",
      "3.0830907821655273\n",
      "448\n",
      "3.083798885345459\n",
      "449\n",
      "3.0841145515441895\n",
      "450\n",
      "3.084643840789795\n",
      "451\n",
      "3.084388017654419\n",
      "452\n",
      "3.0841665267944336\n",
      "453\n",
      "3.0831544399261475\n",
      "454\n",
      "3.08268666267395\n",
      "455\n",
      "3.0819215774536133\n",
      "456\n",
      "3.081392765045166\n",
      "457\n",
      "3.080707311630249\n",
      "458\n",
      "3.0802338123321533\n",
      "459\n",
      "3.079840660095215\n",
      "460\n",
      "3.07989501953125\n",
      "461\n",
      "3.0796632766723633\n",
      "462\n",
      "3.0799548625946045\n",
      "463\n",
      "3.0803029537200928\n",
      "464\n",
      "3.0806756019592285\n",
      "465\n",
      "3.0810046195983887\n",
      "466\n",
      "3.081568479537964\n",
      "467\n",
      "3.0821096897125244\n",
      "468\n",
      "3.0825815200805664\n",
      "469\n",
      "3.082855224609375\n",
      "470\n",
      "3.082914352416992\n",
      "471\n",
      "3.0824782848358154\n",
      "472\n",
      "3.0821540355682373\n",
      "473\n",
      "3.0817530155181885\n",
      "474\n",
      "3.0811359882354736\n",
      "475\n",
      "3.0803792476654053\n",
      "476\n",
      "3.079705238342285\n",
      "477\n",
      "3.079129934310913\n",
      "478\n",
      "3.07878041267395\n",
      "479\n",
      "3.078538417816162\n",
      "480\n",
      "3.07840633392334\n",
      "481\n",
      "3.0785467624664307\n",
      "482\n",
      "3.0787878036499023\n",
      "483\n",
      "3.079263687133789\n",
      "484\n",
      "3.079929828643799\n",
      "485\n",
      "3.080775022506714\n",
      "486\n",
      "3.081388473510742\n",
      "487\n",
      "3.081831693649292\n",
      "488\n",
      "3.0822205543518066\n",
      "489\n",
      "3.0821778774261475\n",
      "490\n",
      "3.0820939540863037\n",
      "491\n",
      "3.0817863941192627\n",
      "492\n",
      "3.0811774730682373\n",
      "493\n",
      "3.0803017616271973\n",
      "494\n",
      "3.0795156955718994\n",
      "495\n",
      "3.079176664352417\n",
      "496\n",
      "3.0785140991210938\n",
      "497\n",
      "3.078338623046875\n",
      "498\n",
      "3.0778517723083496\n",
      "499\n",
      "3.077862024307251\n",
      "500\n",
      "3.0775399208068848\n",
      "501\n",
      "3.0779480934143066\n",
      "502\n",
      "3.0782158374786377\n",
      "503\n",
      "3.0789051055908203\n",
      "504\n",
      "3.0792481899261475\n",
      "505\n",
      "3.0800139904022217\n",
      "506\n",
      "3.0804121494293213\n",
      "507\n",
      "3.0811774730682373\n",
      "508\n",
      "3.0813069343566895\n",
      "509\n",
      "3.081220865249634\n",
      "510\n",
      "3.080637216567993\n",
      "511\n",
      "3.0803236961364746\n",
      "512\n",
      "3.0793066024780273\n",
      "513\n",
      "3.0786304473876953\n",
      "514\n",
      "3.077794075012207\n",
      "515\n",
      "3.0776162147521973\n",
      "516\n",
      "3.0769894123077393\n",
      "517\n",
      "3.0769684314727783\n",
      "518\n",
      "3.0768117904663086\n",
      "519\n",
      "3.0773916244506836\n",
      "520\n",
      "3.077343463897705\n",
      "521\n",
      "3.078169822692871\n",
      "522\n",
      "3.078256130218506\n",
      "523\n",
      "3.0794854164123535\n",
      "524\n",
      "3.0792746543884277\n",
      "525\n",
      "3.0787758827209473\n",
      "526\n",
      "3.078192949295044\n",
      "527\n",
      "3.078014850616455\n",
      "528\n",
      "3.0772130489349365\n",
      "529\n",
      "3.0769989490509033\n",
      "530\n",
      "3.0767340660095215\n",
      "531\n",
      "3.0772252082824707\n",
      "532\n",
      "3.076512098312378\n",
      "533\n",
      "3.076359510421753\n",
      "534\n",
      "3.0764694213867188\n",
      "535\n",
      "3.0773136615753174\n",
      "536\n",
      "3.0767674446105957\n",
      "537\n",
      "3.0772852897644043\n",
      "538\n",
      "3.0770716667175293\n",
      "539\n",
      "3.077939033508301\n",
      "540\n",
      "3.0778958797454834\n",
      "541\n",
      "3.0787220001220703\n",
      "542\n",
      "3.078115940093994\n",
      "543\n",
      "3.078488349914551\n",
      "544\n",
      "3.0776476860046387\n",
      "545\n",
      "3.0779166221618652\n",
      "546\n",
      "3.076747417449951\n",
      "547\n",
      "3.076887845993042\n",
      "548\n",
      "3.0760996341705322\n",
      "549\n",
      "3.076678514480591\n",
      "550\n",
      "3.075681686401367\n",
      "551\n",
      "3.076310873031616\n",
      "552\n",
      "3.0760433673858643\n",
      "553\n",
      "3.0766398906707764\n",
      "554\n",
      "3.075953245162964\n",
      "555\n",
      "3.076995372772217\n",
      "556\n",
      "3.0763661861419678\n",
      "557\n",
      "3.0771684646606445\n",
      "558\n",
      "3.076681613922119\n",
      "559\n",
      "3.07780122756958\n",
      "560\n",
      "3.0765702724456787\n",
      "561\n",
      "3.076767683029175\n",
      "562\n",
      "3.07576060295105\n",
      "563\n",
      "3.0767388343811035\n",
      "564\n",
      "3.075773239135742\n",
      "565\n",
      "3.0758376121520996\n",
      "566\n",
      "3.0748584270477295\n",
      "567\n",
      "3.0758309364318848\n",
      "568\n",
      "3.0747299194335938\n",
      "569\n",
      "3.0752291679382324\n",
      "570\n",
      "3.0743865966796875\n",
      "571\n",
      "3.075881242752075\n",
      "572\n",
      "3.075221300125122\n",
      "573\n",
      "3.076148748397827\n",
      "574\n",
      "3.075221538543701\n",
      "575\n",
      "3.076481342315674\n",
      "576\n",
      "3.0749664306640625\n",
      "577\n",
      "3.0752956867218018\n",
      "578\n",
      "3.0740232467651367\n",
      "579\n",
      "3.075364828109741\n",
      "580\n",
      "3.073821783065796\n",
      "581\n",
      "3.0741126537323\n",
      "582\n",
      "3.073026418685913\n",
      "583\n",
      "3.0736072063446045\n",
      "584\n",
      "3.0727691650390625\n",
      "585\n",
      "3.0740625858306885\n",
      "586\n",
      "3.073155641555786\n",
      "587\n",
      "3.0741677284240723\n",
      "588\n",
      "3.073500871658325\n",
      "589\n",
      "3.0747482776641846\n",
      "590\n",
      "3.074126720428467\n",
      "591\n",
      "3.075860023498535\n",
      "592\n",
      "3.0748302936553955\n",
      "593\n",
      "3.075261116027832\n",
      "594\n",
      "3.074188709259033\n",
      "595\n",
      "3.075648784637451\n",
      "596\n",
      "3.0742008686065674\n",
      "597\n",
      "3.0737550258636475\n",
      "598\n",
      "3.0724077224731445\n",
      "599\n",
      "3.0738072395324707\n",
      "600\n",
      "3.072517156600952\n",
      "601\n",
      "3.072516441345215\n",
      "602\n",
      "3.071854591369629\n",
      "603\n",
      "3.074143171310425\n",
      "604\n",
      "3.0731277465820312\n",
      "605\n",
      "3.0733721256256104\n",
      "606\n",
      "3.0722904205322266\n",
      "607\n",
      "3.0740106105804443\n",
      "608\n",
      "3.073119878768921\n",
      "609\n",
      "3.0734732151031494\n",
      "610\n",
      "3.0724878311157227\n",
      "611\n",
      "3.074434518814087\n",
      "612\n",
      "3.073045492172241\n",
      "613\n",
      "3.0736987590789795\n",
      "614\n",
      "3.0726888179779053\n",
      "615\n",
      "3.074158191680908\n",
      "616\n",
      "3.0729448795318604\n",
      "617\n",
      "3.0737383365631104\n",
      "618\n",
      "3.0728225708007812\n",
      "619\n",
      "3.0736169815063477\n",
      "620\n",
      "3.072214126586914\n",
      "621\n",
      "3.0722873210906982\n",
      "622\n",
      "3.0708954334259033\n",
      "623\n",
      "3.0714223384857178\n",
      "624\n",
      "3.070324182510376\n",
      "625\n",
      "3.071143865585327\n",
      "626\n",
      "3.07033634185791\n",
      "627\n",
      "3.0716028213500977\n",
      "628\n",
      "3.0708365440368652\n",
      "629\n",
      "3.0718700885772705\n",
      "630\n",
      "3.0711517333984375\n",
      "631\n",
      "3.072671890258789\n",
      "632\n",
      "3.0717318058013916\n",
      "633\n",
      "3.0719387531280518\n",
      "634\n",
      "3.0710291862487793\n",
      "635\n",
      "3.0721912384033203\n",
      "636\n",
      "3.0705409049987793\n",
      "637\n",
      "3.0703647136688232\n",
      "638\n",
      "3.069253921508789\n",
      "639\n",
      "3.070242404937744\n",
      "640\n",
      "3.0689587593078613\n",
      "641\n",
      "3.06958270072937\n",
      "642\n",
      "3.0685949325561523\n",
      "643\n",
      "3.070307493209839\n",
      "644\n",
      "3.069523572921753\n",
      "645\n",
      "3.070418119430542\n",
      "646\n",
      "3.0696702003479004\n",
      "647\n",
      "3.0706255435943604\n",
      "648\n",
      "3.0696024894714355\n",
      "649\n",
      "3.0702507495880127\n",
      "650\n",
      "3.0690598487854004\n",
      "651\n",
      "3.0697410106658936\n",
      "652\n",
      "3.068495273590088\n",
      "653\n",
      "3.0698740482330322\n",
      "654\n",
      "3.069047212600708\n",
      "655\n",
      "3.070448875427246\n",
      "656\n",
      "3.0698015689849854\n",
      "657\n",
      "3.0724143981933594\n",
      "658\n",
      "3.071042776107788\n",
      "659\n",
      "3.0727293491363525\n",
      "660\n",
      "3.0711863040924072\n",
      "661\n",
      "3.0725035667419434\n",
      "662\n",
      "3.070650577545166\n",
      "663\n",
      "3.0715832710266113\n",
      "664\n",
      "3.069969892501831\n",
      "665\n",
      "3.0706827640533447\n",
      "666\n",
      "3.0688557624816895\n",
      "667\n",
      "3.069838047027588\n",
      "668\n",
      "3.0685484409332275\n",
      "669\n",
      "3.0697245597839355\n",
      "670\n",
      "3.0687360763549805\n",
      "671\n",
      "3.070669174194336\n",
      "672\n",
      "3.069401741027832\n",
      "673\n",
      "3.07045578956604\n",
      "674\n",
      "3.06868577003479\n",
      "675\n",
      "3.0698046684265137\n",
      "676\n",
      "3.0688986778259277\n",
      "677\n",
      "3.0702409744262695\n",
      "678\n",
      "3.0686817169189453\n",
      "679\n",
      "3.070469379425049\n",
      "680\n",
      "3.0689938068389893\n",
      "681\n",
      "3.069963216781616\n",
      "682\n",
      "3.067946195602417\n",
      "683\n",
      "3.0695197582244873\n",
      "684\n",
      "3.0685935020446777\n",
      "685\n",
      "3.0704185962677\n",
      "686\n",
      "3.0690324306488037\n",
      "687\n",
      "3.070737600326538\n",
      "688\n",
      "3.0690395832061768\n",
      "689\n",
      "3.069981098175049\n",
      "690\n",
      "3.068347930908203\n",
      "691\n",
      "3.069629192352295\n",
      "692\n",
      "3.0678298473358154\n",
      "693\n",
      "3.069521188735962\n",
      "694\n",
      "3.067845106124878\n",
      "695\n",
      "3.069248914718628\n",
      "696\n",
      "3.0676310062408447\n",
      "697\n",
      "3.0689332485198975\n",
      "698\n",
      "3.0672836303710938\n",
      "699\n",
      "3.067873001098633\n",
      "700\n",
      "3.066096782684326\n",
      "701\n",
      "3.0667059421539307\n",
      "702\n",
      "3.065075635910034\n",
      "703\n",
      "3.0662479400634766\n",
      "704\n",
      "3.0649964809417725\n",
      "705\n",
      "3.066605806350708\n",
      "706\n",
      "3.06632137298584\n",
      "707\n",
      "3.0686190128326416\n",
      "708\n",
      "3.068833827972412\n",
      "709\n",
      "3.0719473361968994\n",
      "710\n",
      "3.0707991123199463\n",
      "711\n",
      "3.072286605834961\n",
      "712\n",
      "3.068854808807373\n",
      "713\n",
      "3.0683109760284424\n",
      "714\n",
      "3.0657992362976074\n",
      "715\n",
      "3.0656449794769287\n",
      "716\n",
      "3.063655376434326\n",
      "717\n",
      "3.0639870166778564\n",
      "718\n",
      "3.0624823570251465\n",
      "719\n",
      "3.0634303092956543\n",
      "720\n",
      "3.0623083114624023\n",
      "721\n",
      "3.0635759830474854\n",
      "722\n",
      "3.063554286956787\n",
      "723\n",
      "3.0663349628448486\n",
      "724\n",
      "3.066734790802002\n",
      "725\n",
      "3.0707597732543945\n",
      "726\n",
      "3.069430351257324\n",
      "727\n",
      "3.070094585418701\n",
      "728\n",
      "3.0665740966796875\n",
      "729\n",
      "3.065772771835327\n",
      "730\n",
      "3.0627071857452393\n",
      "731\n",
      "3.063227653503418\n",
      "732\n",
      "3.0616633892059326\n",
      "733\n",
      "3.0630905628204346\n",
      "734\n",
      "3.0630056858062744\n",
      "735\n",
      "3.066176652908325\n",
      "736\n",
      "3.0658464431762695\n",
      "737\n",
      "3.0677950382232666\n",
      "738\n",
      "3.0659732818603516\n",
      "739\n",
      "3.0670650005340576\n",
      "740\n",
      "3.0650599002838135\n",
      "741\n",
      "3.066783905029297\n",
      "742\n",
      "3.0649402141571045\n",
      "743\n",
      "3.066006660461426\n",
      "744\n",
      "3.063715934753418\n",
      "745\n",
      "3.0639283657073975\n",
      "746\n",
      "3.0627553462982178\n",
      "747\n",
      "3.0641586780548096\n",
      "748\n",
      "3.062838077545166\n",
      "749\n",
      "3.064523935317993\n",
      "750\n",
      "3.0640649795532227\n",
      "751\n",
      "3.066267967224121\n",
      "752\n",
      "3.0645058155059814\n",
      "753\n",
      "3.065835952758789\n",
      "754\n",
      "3.065208911895752\n",
      "755\n",
      "3.0664920806884766\n",
      "756\n",
      "3.0633373260498047\n",
      "757\n",
      "3.063230514526367\n",
      "758\n",
      "3.0612576007843018\n",
      "759\n",
      "3.0623342990875244\n",
      "760\n",
      "3.060875177383423\n",
      "761\n",
      "3.062441349029541\n",
      "762\n",
      "3.062241315841675\n",
      "763\n",
      "3.065195083618164\n",
      "764\n",
      "3.0650205612182617\n",
      "765\n",
      "3.06772518157959\n",
      "766\n",
      "3.0653951168060303\n",
      "767\n",
      "3.065272331237793\n",
      "768\n",
      "3.0624992847442627\n",
      "769\n",
      "3.0634114742279053\n",
      "770\n",
      "3.0618298053741455\n",
      "771\n",
      "3.062514543533325\n",
      "772\n",
      "3.0607709884643555\n",
      "773\n",
      "3.062089443206787\n",
      "774\n",
      "3.0608503818511963\n",
      "775\n",
      "3.0630717277526855\n",
      "776\n",
      "3.062232732772827\n",
      "777\n",
      "3.064016580581665\n",
      "778\n",
      "3.061856985092163\n",
      "779\n",
      "3.0628139972686768\n",
      "780\n",
      "3.0608744621276855\n",
      "781\n",
      "3.0620646476745605\n",
      "782\n",
      "3.0608646869659424\n",
      "783\n",
      "3.062560796737671\n",
      "784\n",
      "3.060903549194336\n",
      "785\n",
      "3.0628421306610107\n",
      "786\n",
      "3.0612106323242188\n",
      "787\n",
      "3.062736749649048\n",
      "788\n",
      "3.060159921646118\n",
      "789\n",
      "3.0606913566589355\n",
      "790\n",
      "3.0590341091156006\n",
      "791\n",
      "3.0612432956695557\n",
      "792\n",
      "3.0594351291656494\n",
      "793\n",
      "3.06062650680542\n",
      "794\n",
      "3.059324026107788\n",
      "795\n",
      "3.0615315437316895\n",
      "796\n",
      "3.060192346572876\n",
      "797\n",
      "3.061481475830078\n",
      "798\n",
      "3.0602729320526123\n",
      "799\n",
      "3.0622739791870117\n",
      "800\n",
      "3.059720277786255\n",
      "801\n",
      "3.0600757598876953\n",
      "802\n",
      "3.0583393573760986\n",
      "803\n",
      "3.059790849685669\n",
      "804\n",
      "3.0579497814178467\n",
      "805\n",
      "3.0598714351654053\n",
      "806\n",
      "3.0587024688720703\n",
      "807\n",
      "3.0610949993133545\n",
      "808\n",
      "3.05971360206604\n",
      "809\n",
      "3.0618836879730225\n",
      "810\n",
      "3.0597496032714844\n",
      "811\n",
      "3.061387538909912\n",
      "812\n",
      "3.058265447616577\n",
      "813\n",
      "3.0598292350769043\n",
      "814\n",
      "3.0579795837402344\n",
      "815\n",
      "3.059870719909668\n",
      "816\n",
      "3.0572071075439453\n",
      "817\n",
      "3.058992624282837\n",
      "818\n",
      "3.057405471801758\n",
      "819\n",
      "3.05991792678833\n",
      "820\n",
      "3.0575344562530518\n",
      "821\n",
      "3.0599074363708496\n",
      "822\n",
      "3.0584557056427\n",
      "823\n",
      "3.0613045692443848\n",
      "824\n",
      "3.058029890060425\n",
      "825\n",
      "3.059379816055298\n",
      "826\n",
      "3.0566604137420654\n",
      "827\n",
      "3.058784246444702\n",
      "828\n",
      "3.0562076568603516\n",
      "829\n",
      "3.0587949752807617\n",
      "830\n",
      "3.0575170516967773\n",
      "831\n",
      "3.0605618953704834\n",
      "832\n",
      "3.058246374130249\n",
      "833\n",
      "3.059882402420044\n",
      "834\n",
      "3.0565991401672363\n",
      "835\n",
      "3.0583653450012207\n",
      "836\n",
      "3.056257724761963\n",
      "837\n",
      "3.0590226650238037\n",
      "838\n",
      "3.056899309158325\n",
      "839\n",
      "3.059720754623413\n",
      "840\n",
      "3.0569772720336914\n",
      "841\n",
      "3.0586795806884766\n",
      "842\n",
      "3.056459426879883\n",
      "843\n",
      "3.0585505962371826\n",
      "844\n",
      "3.0557844638824463\n",
      "845\n",
      "3.058544635772705\n",
      "846\n",
      "3.0560317039489746\n",
      "847\n",
      "3.0570664405822754\n",
      "848\n",
      "3.0548582077026367\n",
      "849\n",
      "3.057307004928589\n",
      "850\n",
      "3.0548081398010254\n",
      "851\n",
      "3.0570766925811768\n",
      "852\n",
      "3.0550143718719482\n",
      "853\n",
      "3.057271718978882\n",
      "854\n",
      "3.0551064014434814\n",
      "855\n",
      "3.057588577270508\n",
      "856\n",
      "3.0560269355773926\n",
      "857\n",
      "3.0583202838897705\n",
      "858\n",
      "3.055696487426758\n",
      "859\n",
      "3.0584845542907715\n",
      "860\n",
      "3.0567069053649902\n",
      "861\n",
      "3.0595712661743164\n",
      "862\n",
      "3.0562009811401367\n",
      "863\n",
      "3.0576283931732178\n",
      "864\n",
      "3.053558349609375\n",
      "865\n",
      "3.0551962852478027\n",
      "866\n",
      "3.053349733352661\n",
      "867\n",
      "3.054880142211914\n",
      "868\n",
      "3.0518455505371094\n",
      "869\n",
      "3.0536506175994873\n",
      "870\n",
      "3.0534040927886963\n",
      "871\n",
      "3.0578601360321045\n",
      "872\n",
      "3.0553572177886963\n",
      "873\n",
      "3.059035062789917\n",
      "874\n",
      "3.0565812587738037\n",
      "875\n",
      "3.0581722259521484\n",
      "876\n",
      "3.0541326999664307\n",
      "877\n",
      "3.055227756500244\n",
      "878\n",
      "3.0517849922180176\n",
      "879\n",
      "3.0539443492889404\n",
      "880\n",
      "3.0515003204345703\n",
      "881\n",
      "3.052942991256714\n",
      "882\n",
      "3.0498435497283936\n",
      "883\n",
      "3.052320957183838\n",
      "884\n",
      "3.0515503883361816\n",
      "885\n",
      "3.0556674003601074\n",
      "886\n",
      "3.0535995960235596\n",
      "887\n",
      "3.05737566947937\n",
      "888\n",
      "3.055711030960083\n",
      "889\n",
      "3.0593090057373047\n",
      "890\n",
      "3.053927421569824\n",
      "891\n",
      "3.054685592651367\n",
      "892\n",
      "3.0513694286346436\n",
      "893\n",
      "3.052396297454834\n",
      "894\n",
      "3.0488855838775635\n",
      "895\n",
      "3.0517923831939697\n",
      "896\n",
      "3.0505385398864746\n",
      "897\n",
      "3.054070234298706\n",
      "898\n",
      "3.051449775695801\n",
      "899\n",
      "3.0539660453796387\n",
      "900\n",
      "3.0518293380737305\n",
      "901\n",
      "3.056002140045166\n",
      "902\n",
      "3.053395986557007\n",
      "903\n",
      "3.0564026832580566\n",
      "904\n",
      "3.052680253982544\n",
      "905\n",
      "3.054656982421875\n",
      "906\n",
      "3.050558567047119\n",
      "907\n",
      "3.0523595809936523\n",
      "908\n",
      "3.0497751235961914\n",
      "909\n",
      "3.052218437194824\n",
      "910\n",
      "3.0494611263275146\n",
      "911\n",
      "3.0521106719970703\n",
      "912\n",
      "3.049801826477051\n",
      "913\n",
      "3.054677724838257\n",
      "914\n",
      "3.052091598510742\n",
      "915\n",
      "3.0548291206359863\n",
      "916\n",
      "3.051604986190796\n",
      "917\n",
      "3.0547258853912354\n",
      "918\n",
      "3.0506820678710938\n",
      "919\n",
      "3.0522279739379883\n",
      "920\n",
      "3.0479393005371094\n",
      "921\n",
      "3.0496113300323486\n",
      "922\n",
      "3.0462303161621094\n",
      "923\n",
      "3.0488834381103516\n",
      "924\n",
      "3.0465149879455566\n",
      "925\n",
      "3.0496938228607178\n",
      "926\n",
      "3.048295736312866\n",
      "927\n",
      "3.0539767742156982\n",
      "928\n",
      "3.05204439163208\n",
      "929\n",
      "3.056194543838501\n",
      "930\n",
      "3.0502610206604004\n",
      "931\n",
      "3.0512783527374268\n",
      "932\n",
      "3.0475656986236572\n",
      "933\n",
      "3.048914670944214\n",
      "934\n",
      "3.0457141399383545\n",
      "935\n",
      "3.0492475032806396\n",
      "936\n",
      "3.0460760593414307\n",
      "937\n",
      "3.049877882003784\n",
      "938\n",
      "3.0481631755828857\n",
      "939\n",
      "3.0520646572113037\n",
      "940\n",
      "3.048743724822998\n",
      "941\n",
      "3.052306652069092\n",
      "942\n",
      "3.0497283935546875\n",
      "943\n",
      "3.053842067718506\n",
      "944\n",
      "3.048612117767334\n",
      "945\n",
      "3.050865888595581\n",
      "946\n",
      "3.0457234382629395\n",
      "947\n",
      "3.0479331016540527\n",
      "948\n",
      "3.044381618499756\n",
      "949\n",
      "3.0478179454803467\n",
      "950\n",
      "3.046304702758789\n",
      "951\n",
      "3.051687002182007\n",
      "952\n",
      "3.048189878463745\n",
      "953\n",
      "3.052347183227539\n",
      "954\n",
      "3.047874927520752\n",
      "955\n",
      "3.0501186847686768\n",
      "956\n",
      "3.0454680919647217\n",
      "957\n",
      "3.0482699871063232\n",
      "958\n",
      "3.044189929962158\n",
      "959\n",
      "3.046565294265747\n",
      "960\n",
      "3.0428626537323\n",
      "961\n",
      "3.0468413829803467\n",
      "962\n",
      "3.0451345443725586\n",
      "963\n",
      "3.050820827484131\n",
      "964\n",
      "3.0473532676696777\n",
      "965\n",
      "3.0522971153259277\n",
      "966\n",
      "3.049133777618408\n",
      "967\n",
      "3.052750825881958\n",
      "968\n",
      "3.0458145141601562\n",
      "969\n",
      "3.046566963195801\n",
      "970\n",
      "3.041840076446533\n",
      "971\n",
      "3.045633316040039\n",
      "972\n",
      "3.042048692703247\n",
      "973\n",
      "3.0451507568359375\n",
      "974\n",
      "3.0430052280426025\n",
      "975\n",
      "3.0496933460235596\n",
      "976\n",
      "3.046804666519165\n",
      "977\n",
      "3.0496201515197754\n",
      "978\n",
      "3.044293165206909\n",
      "979\n",
      "3.0483274459838867\n",
      "980\n",
      "3.043184280395508\n",
      "981\n",
      "3.045180082321167\n",
      "982\n",
      "3.0416393280029297\n",
      "983\n",
      "3.047060012817383\n",
      "984\n",
      "3.044067859649658\n",
      "985\n",
      "3.048306941986084\n",
      "986\n",
      "3.0445170402526855\n",
      "987\n",
      "3.049237012863159\n",
      "988\n",
      "3.0442843437194824\n",
      "989\n",
      "3.0471692085266113\n",
      "990\n",
      "3.0413806438446045\n",
      "991\n",
      "3.043811559677124\n",
      "992\n",
      "3.0393218994140625\n",
      "993\n",
      "3.043395757675171\n",
      "994\n",
      "3.041673183441162\n",
      "995\n",
      "3.0475575923919678\n",
      "996\n",
      "3.044508457183838\n",
      "997\n",
      "3.0497708320617676\n",
      "998\n",
      "3.044739007949829\n",
      "999\n",
      "3.0472590923309326\n",
      "1000\n",
      "3.041257381439209\n",
      "1001\n",
      "3.0435242652893066\n",
      "1002\n",
      "3.0386593341827393\n",
      "1003\n",
      "3.042452812194824\n",
      "1004\n",
      "3.0405266284942627\n",
      "1005\n",
      "3.0457763671875\n",
      "1006\n",
      "3.0418684482574463\n",
      "1007\n",
      "3.047459363937378\n",
      "1008\n",
      "3.044018268585205\n",
      "1009\n",
      "3.0480148792266846\n",
      "1010\n",
      "3.0424160957336426\n",
      "1011\n",
      "3.0445218086242676\n",
      "1012\n",
      "3.0387349128723145\n",
      "1013\n",
      "3.0423617362976074\n",
      "1014\n",
      "3.0384185314178467\n",
      "1015\n",
      "3.042976140975952\n",
      "1016\n",
      "3.0403993129730225\n",
      "1017\n",
      "3.0458264350891113\n",
      "1018\n",
      "3.0415377616882324\n",
      "1019\n",
      "3.0455479621887207\n",
      "1020\n",
      "3.0417659282684326\n",
      "1021\n",
      "3.0462749004364014\n",
      "1022\n",
      "3.0408170223236084\n",
      "1023\n",
      "3.043954372406006\n",
      "1024\n",
      "3.038113832473755\n",
      "1025\n",
      "3.040243148803711\n",
      "1026\n",
      "3.0366530418395996\n",
      "1027\n",
      "3.041459321975708\n",
      "1028\n",
      "3.0383520126342773\n",
      "1029\n",
      "3.0445024967193604\n",
      "1030\n",
      "3.0417532920837402\n",
      "1031\n",
      "3.0460708141326904\n",
      "1032\n",
      "3.039872646331787\n",
      "1033\n",
      "3.0437421798706055\n",
      "1034\n",
      "3.0396432876586914\n",
      "1035\n",
      "3.0436885356903076\n",
      "1036\n",
      "3.037156343460083\n",
      "1037\n",
      "3.039909601211548\n",
      "1038\n",
      "3.0345306396484375\n",
      "1039\n",
      "3.0384628772735596\n",
      "1040\n",
      "3.035827875137329\n",
      "1041\n",
      "3.0429046154022217\n",
      "1042\n",
      "3.0400381088256836\n",
      "1043\n",
      "3.0445635318756104\n",
      "1044\n",
      "3.039869785308838\n",
      "1045\n",
      "3.044393539428711\n",
      "1046\n",
      "3.039501905441284\n",
      "1047\n",
      "3.0441629886627197\n",
      "1048\n",
      "3.038801908493042\n",
      "1049\n",
      "3.040767192840576\n",
      "1050\n",
      "3.0345473289489746\n",
      "1051\n",
      "3.037614107131958\n",
      "1052\n",
      "3.033756971359253\n",
      "1053\n",
      "3.038745641708374\n",
      "1054\n",
      "3.0362679958343506\n",
      "1055\n",
      "3.0423059463500977\n",
      "1056\n",
      "3.0383036136627197\n",
      "1057\n",
      "3.0437629222869873\n",
      "1058\n",
      "3.0382022857666016\n",
      "1059\n",
      "3.0429623126983643\n",
      "1060\n",
      "3.0370075702667236\n",
      "1061\n",
      "3.0391762256622314\n",
      "1062\n",
      "3.0328662395477295\n",
      "1063\n",
      "3.0356838703155518\n",
      "1064\n",
      "3.030802011489868\n",
      "1065\n",
      "3.035369396209717\n",
      "1066\n",
      "3.0336554050445557\n",
      "1067\n",
      "3.042034387588501\n",
      "1068\n",
      "3.0396881103515625\n",
      "1069\n",
      "3.0460128784179688\n",
      "1070\n",
      "3.0398001670837402\n",
      "1071\n",
      "3.043152093887329\n",
      "1072\n",
      "3.0346555709838867\n",
      "1073\n",
      "3.0351791381835938\n",
      "1074\n",
      "3.0296311378479004\n",
      "1075\n",
      "3.0343098640441895\n",
      "1076\n",
      "3.0305745601654053\n",
      "1077\n",
      "3.03493332862854\n",
      "1078\n",
      "3.033515214920044\n",
      "1079\n",
      "3.0421578884124756\n",
      "1080\n",
      "3.038486957550049\n",
      "1081\n",
      "3.0446548461914062\n",
      "1082\n",
      "3.037771224975586\n",
      "1083\n",
      "3.0396101474761963\n",
      "1084\n",
      "3.0323591232299805\n",
      "1085\n",
      "3.0357069969177246\n",
      "1086\n",
      "3.031156539916992\n",
      "1087\n",
      "3.0355799198150635\n",
      "1088\n",
      "3.0319509506225586\n",
      "1089\n",
      "3.0368564128875732\n",
      "1090\n",
      "3.0333545207977295\n",
      "1091\n",
      "3.0402281284332275\n",
      "1092\n",
      "3.0358834266662598\n",
      "1093\n",
      "3.040821075439453\n",
      "1094\n",
      "3.0350303649902344\n",
      "1095\n",
      "3.038069009780884\n",
      "1096\n",
      "3.0308425426483154\n",
      "1097\n",
      "3.0340936183929443\n",
      "1098\n",
      "3.029472827911377\n",
      "1099\n",
      "3.03355073928833\n",
      "1100\n",
      "3.031132936477661\n",
      "1101\n",
      "3.038790225982666\n",
      "1102\n",
      "3.034931182861328\n",
      "1103\n",
      "3.0401549339294434\n",
      "1104\n",
      "3.0339701175689697\n",
      "1105\n",
      "3.0376622676849365\n",
      "1106\n",
      "3.031951427459717\n",
      "1107\n",
      "3.036391019821167\n",
      "1108\n",
      "3.031036376953125\n",
      "1109\n",
      "3.035405158996582\n",
      "1110\n",
      "3.031221866607666\n",
      "1111\n",
      "3.035348892211914\n",
      "1112\n",
      "3.030062675476074\n",
      "1113\n",
      "3.0332303047180176\n",
      "1114\n",
      "3.029125213623047\n",
      "1115\n",
      "3.0365023612976074\n",
      "1116\n",
      "3.033656358718872\n",
      "1117\n",
      "3.03945255279541\n",
      "1118\n",
      "3.0335917472839355\n",
      "1119\n",
      "3.0388920307159424\n",
      "1120\n",
      "3.0327322483062744\n",
      "1121\n",
      "3.034900188446045\n",
      "1122\n",
      "3.0276036262512207\n",
      "1123\n",
      "3.028864622116089\n",
      "1124\n",
      "3.0244593620300293\n",
      "1125\n",
      "3.0308473110198975\n",
      "1126\n",
      "3.029935121536255\n",
      "1127\n",
      "3.037437915802002\n",
      "1128\n",
      "3.033292055130005\n",
      "1129\n",
      "3.0398600101470947\n",
      "1130\n",
      "3.034804582595825\n",
      "1131\n",
      "3.038658857345581\n",
      "1132\n",
      "3.0311577320098877\n",
      "1133\n",
      "3.033458709716797\n",
      "1134\n",
      "3.0265657901763916\n",
      "1135\n",
      "3.0288989543914795\n",
      "1136\n",
      "3.023850679397583\n",
      "1137\n",
      "3.02927303314209\n",
      "1138\n",
      "3.0274569988250732\n",
      "1139\n",
      "3.0345566272735596\n",
      "1140\n",
      "3.032719612121582\n",
      "1141\n",
      "3.041624069213867\n",
      "1142\n",
      "3.033954381942749\n",
      "1143\n",
      "3.0367202758789062\n",
      "1144\n",
      "3.028269052505493\n",
      "1145\n",
      "3.028120279312134\n",
      "1146\n",
      "3.0226714611053467\n",
      "1147\n",
      "3.0271427631378174\n",
      "1148\n",
      "3.0257487297058105\n",
      "1149\n",
      "3.0342767238616943\n",
      "1150\n",
      "3.0315134525299072\n",
      "1151\n",
      "3.0388073921203613\n",
      "1152\n",
      "3.033928871154785\n",
      "1153\n",
      "3.0375425815582275\n",
      "1154\n",
      "3.0288963317871094\n",
      "1155\n",
      "3.031690835952759\n",
      "1156\n",
      "3.02543044090271\n",
      "1157\n",
      "3.0281200408935547\n",
      "1158\n",
      "3.0226545333862305\n",
      "1159\n",
      "3.027315855026245\n",
      "1160\n",
      "3.02512264251709\n",
      "1161\n",
      "3.0321295261383057\n",
      "1162\n",
      "3.0286669731140137\n",
      "1163\n",
      "3.0355489253997803\n",
      "1164\n",
      "3.0312561988830566\n",
      "1165\n",
      "3.03665828704834\n",
      "1166\n",
      "3.0303313732147217\n",
      "1167\n",
      "3.0332491397857666\n",
      "1168\n",
      "3.0264792442321777\n",
      "1169\n",
      "3.0299320220947266\n",
      "1170\n",
      "3.02333664894104\n",
      "1171\n",
      "3.026676654815674\n",
      "1172\n",
      "3.023439645767212\n",
      "1173\n",
      "3.0283043384552\n",
      "1174\n",
      "3.0240323543548584\n",
      "1175\n",
      "3.030517578125\n",
      "1176\n",
      "3.028860569000244\n",
      "1177\n",
      "3.036674737930298\n",
      "1178\n",
      "3.0307929515838623\n",
      "1179\n",
      "3.034928321838379\n",
      "1180\n",
      "3.027015209197998\n",
      "1181\n",
      "3.0288188457489014\n",
      "1182\n",
      "3.021294116973877\n",
      "1183\n",
      "3.023148536682129\n",
      "1184\n",
      "3.0199830532073975\n",
      "1185\n",
      "3.026775598526001\n",
      "1186\n",
      "3.0243849754333496\n",
      "1187\n",
      "3.031876802444458\n",
      "1188\n",
      "3.0294644832611084\n",
      "1189\n",
      "3.037026882171631\n",
      "1190\n",
      "3.0290772914886475\n",
      "1191\n",
      "3.0320987701416016\n",
      "1192\n",
      "3.024684190750122\n",
      "1193\n",
      "3.0272610187530518\n",
      "1194\n",
      "3.0207033157348633\n",
      "1195\n",
      "3.024109363555908\n",
      "1196\n",
      "3.019895553588867\n",
      "1197\n",
      "3.026040554046631\n",
      "1198\n",
      "3.0236008167266846\n",
      "1199\n",
      "3.032806158065796\n",
      "1200\n",
      "3.0289502143859863\n",
      "1201\n",
      "3.033867597579956\n",
      "1202\n",
      "3.0258939266204834\n",
      "1203\n",
      "3.0289628505706787\n",
      "1204\n",
      "3.022031784057617\n",
      "1205\n",
      "3.024501085281372\n",
      "1206\n",
      "3.019305944442749\n",
      "1207\n",
      "3.025304079055786\n",
      "1208\n",
      "3.021697759628296\n",
      "1209\n",
      "3.0292975902557373\n",
      "1210\n",
      "3.0263490676879883\n",
      "1211\n",
      "3.0336902141571045\n",
      "1212\n",
      "3.0251049995422363\n",
      "1213\n",
      "3.028095245361328\n",
      "1214\n",
      "3.02236008644104\n",
      "1215\n",
      "3.0268349647521973\n",
      "1216\n",
      "3.0199265480041504\n",
      "1217\n",
      "3.0231823921203613\n",
      "1218\n",
      "3.020587682723999\n",
      "1219\n",
      "3.0291240215301514\n",
      "1220\n",
      "3.024674654006958\n",
      "1221\n",
      "3.0309946537017822\n",
      "1222\n",
      "3.0247323513031006\n",
      "1223\n",
      "3.0297751426696777\n",
      "1224\n",
      "3.0216948986053467\n",
      "1225\n",
      "3.024064302444458\n",
      "1226\n",
      "3.018714427947998\n",
      "1227\n",
      "3.0246548652648926\n",
      "1228\n",
      "3.018948554992676\n",
      "1229\n",
      "3.0236270427703857\n",
      "1230\n",
      "3.021625518798828\n",
      "1231\n",
      "3.0308923721313477\n",
      "1232\n",
      "3.024442195892334\n",
      "1233\n",
      "3.0282273292541504\n",
      "1234\n",
      "3.0212204456329346\n",
      "1235\n",
      "3.0250701904296875\n",
      "1236\n",
      "3.0174880027770996\n",
      "1237\n",
      "3.020181179046631\n",
      "1238\n",
      "3.017292022705078\n",
      "1239\n",
      "3.0273513793945312\n",
      "1240\n",
      "3.0229814052581787\n",
      "1241\n",
      "3.0279014110565186\n",
      "1242\n",
      "3.0226669311523438\n",
      "1243\n",
      "3.0291409492492676\n",
      "1244\n",
      "3.0207321643829346\n",
      "1245\n",
      "3.0230650901794434\n",
      "1246\n",
      "3.017788887023926\n",
      "1247\n",
      "3.023376703262329\n",
      "1248\n",
      "3.017232656478882\n",
      "1249\n",
      "3.023120403289795\n",
      "1250\n",
      "3.0205628871917725\n",
      "1251\n",
      "3.029107093811035\n",
      "1252\n",
      "3.020375967025757\n",
      "1253\n",
      "3.0243990421295166\n",
      "1254\n",
      "3.0198683738708496\n",
      "1255\n",
      "3.0253236293792725\n",
      "1256\n",
      "3.016353130340576\n",
      "1257\n",
      "3.018848419189453\n",
      "1258\n",
      "3.0144217014312744\n",
      "1259\n",
      "3.0215914249420166\n",
      "1260\n",
      "3.0178606510162354\n",
      "1261\n",
      "3.025838851928711\n",
      "1262\n",
      "3.0231597423553467\n",
      "1263\n",
      "3.0324110984802246\n",
      "1264\n",
      "3.023059844970703\n",
      "1265\n",
      "3.024141550064087\n",
      "1266\n",
      "3.0156519412994385\n",
      "1267\n",
      "3.0183022022247314\n",
      "1268\n",
      "3.0104994773864746\n",
      "1269\n",
      "3.0140089988708496\n",
      "1270\n",
      "3.0119540691375732\n",
      "1271\n",
      "3.023315191268921\n",
      "1272\n",
      "3.020813465118408\n",
      "1273\n",
      "3.0300257205963135\n",
      "1274\n",
      "3.024921417236328\n",
      "1275\n",
      "3.032280921936035\n",
      "1276\n",
      "3.0214641094207764\n",
      "1277\n",
      "3.017439842224121\n",
      "1278\n",
      "3.0078577995300293\n",
      "1279\n",
      "3.010681390762329\n",
      "1280\n",
      "3.0074548721313477\n",
      "1281\n",
      "3.015223741531372\n",
      "1282\n",
      "3.0156571865081787\n",
      "1283\n",
      "3.0303955078125\n",
      "1284\n",
      "3.0272257328033447\n",
      "1285\n",
      "3.032553195953369\n",
      "1286\n",
      "3.0197157859802246\n",
      "1287\n",
      "3.0184402465820312\n",
      "1288\n",
      "3.011035919189453\n",
      "1289\n",
      "3.016220808029175\n",
      "1290\n",
      "3.011425733566284\n",
      "1291\n",
      "3.0184433460235596\n",
      "1292\n",
      "3.0165395736694336\n",
      "1293\n",
      "3.0262184143066406\n",
      "1294\n",
      "3.0198121070861816\n",
      "1295\n",
      "3.02510142326355\n",
      "1296\n",
      "3.0184826850891113\n",
      "1297\n",
      "3.0237386226654053\n",
      "1298\n",
      "3.0145187377929688\n",
      "1299\n",
      "3.0166916847229004\n",
      "1300\n",
      "3.010279893875122\n",
      "1301\n",
      "3.0154261589050293\n",
      "1302\n",
      "3.0113930702209473\n",
      "1303\n",
      "3.0200588703155518\n",
      "1304\n",
      "3.018209934234619\n",
      "1305\n",
      "3.0266122817993164\n",
      "1306\n",
      "3.0184764862060547\n",
      "1307\n",
      "3.0233314037323\n",
      "1308\n",
      "3.015134334564209\n",
      "1309\n",
      "3.0171420574188232\n",
      "1310\n",
      "3.010293483734131\n",
      "1311\n",
      "3.015974998474121\n",
      "1312\n",
      "3.0110816955566406\n",
      "1313\n",
      "3.0177454948425293\n",
      "1314\n",
      "3.0142154693603516\n",
      "1315\n",
      "3.0243608951568604\n",
      "1316\n",
      "3.018861770629883\n",
      "1317\n",
      "3.0238864421844482\n",
      "1318\n",
      "3.014376163482666\n",
      "1319\n",
      "3.017564296722412\n",
      "1320\n",
      "3.0085947513580322\n",
      "1321\n",
      "3.0105881690979004\n",
      "1322\n",
      "3.0074150562286377\n",
      "1323\n",
      "3.0167133808135986\n",
      "1324\n",
      "3.015202283859253\n",
      "1325\n",
      "3.0242578983306885\n",
      "1326\n",
      "3.019163131713867\n",
      "1327\n",
      "3.0269603729248047\n",
      "1328\n",
      "3.015735626220703\n",
      "1329\n",
      "3.014023542404175\n",
      "1330\n",
      "3.0060579776763916\n",
      "1331\n",
      "3.011958122253418\n",
      "1332\n",
      "3.0062756538391113\n",
      "1333\n",
      "3.012246608734131\n",
      "1334\n",
      "3.01174259185791\n",
      "1335\n",
      "3.0249016284942627\n",
      "1336\n",
      "3.020090103149414\n",
      "1337\n",
      "3.0253872871398926\n",
      "1338\n",
      "3.0149080753326416\n",
      "1339\n",
      "3.017559289932251\n",
      "1340\n",
      "3.0073487758636475\n",
      "1341\n",
      "3.008768081665039\n",
      "1342\n",
      "3.0048677921295166\n",
      "1343\n",
      "3.015437602996826\n",
      "1344\n",
      "3.0140280723571777\n",
      "1345\n",
      "3.0213072299957275\n",
      "1346\n",
      "3.0144660472869873\n",
      "1347\n",
      "3.022559642791748\n",
      "1348\n",
      "3.014995574951172\n",
      "1349\n",
      "3.0183329582214355\n",
      "1350\n",
      "3.0080478191375732\n",
      "1351\n",
      "3.0097036361694336\n",
      "1352\n",
      "3.0038225650787354\n",
      "1353\n",
      "3.0118117332458496\n",
      "1354\n",
      "3.0075573921203613\n",
      "1355\n",
      "3.0170376300811768\n",
      "1356\n",
      "3.016470432281494\n",
      "1357\n",
      "3.0279130935668945\n",
      "1358\n",
      "3.0160181522369385\n",
      "1359\n",
      "3.016310214996338\n",
      "1360\n",
      "3.0057644844055176\n",
      "1361\n",
      "3.0094680786132812\n",
      "1362\n",
      "3.0026655197143555\n",
      "1363\n",
      "3.008435010910034\n",
      "1364\n",
      "3.005758762359619\n",
      "1365\n",
      "3.0168464183807373\n",
      "1366\n",
      "3.014486074447632\n",
      "1367\n",
      "3.0242557525634766\n",
      "1368\n",
      "3.0172128677368164\n",
      "1369\n",
      "3.022129535675049\n",
      "1370\n",
      "3.007814407348633\n",
      "1371\n",
      "3.0077145099639893\n",
      "1372\n",
      "3.0003440380096436\n",
      "1373\n",
      "3.0041356086730957\n",
      "1374\n",
      "3.000225067138672\n",
      "1375\n",
      "3.010761022567749\n",
      "1376\n",
      "3.0121653079986572\n",
      "1377\n",
      "3.027578115463257\n",
      "1378\n",
      "3.018972158432007\n",
      "1379\n",
      "3.021165132522583\n",
      "1380\n",
      "3.008793592453003\n",
      "1381\n",
      "3.0105085372924805\n",
      "1382\n",
      "3.001211166381836\n",
      "1383\n",
      "3.0050272941589355\n",
      "1384\n",
      "3.0029258728027344\n",
      "1385\n",
      "3.015559434890747\n",
      "1386\n",
      "3.010152578353882\n",
      "1387\n",
      "3.0190484523773193\n",
      "1388\n",
      "3.013967990875244\n",
      "1389\n",
      "3.0205695629119873\n",
      "1390\n",
      "3.0079550743103027\n",
      "1391\n",
      "3.008908748626709\n",
      "1392\n",
      "3.0020058155059814\n",
      "1393\n",
      "3.008254289627075\n",
      "1394\n",
      "3.0031208992004395\n",
      "1395\n",
      "3.011653184890747\n",
      "1396\n",
      "3.009298086166382\n",
      "1397\n",
      "3.021223783493042\n",
      "1398\n",
      "3.011610746383667\n",
      "1399\n",
      "3.014085292816162\n",
      "1400\n",
      "3.0039050579071045\n",
      "1401\n",
      "3.0071663856506348\n",
      "1402\n",
      "2.9999892711639404\n",
      "1403\n",
      "3.0065629482269287\n",
      "1404\n",
      "3.003877639770508\n",
      "1405\n",
      "3.0152859687805176\n",
      "1406\n",
      "3.0110554695129395\n",
      "1407\n",
      "3.0207278728485107\n",
      "1408\n",
      "3.012291669845581\n",
      "1409\n",
      "3.0163652896881104\n",
      "1410\n",
      "3.003264904022217\n",
      "1411\n",
      "3.0042946338653564\n",
      "1412\n",
      "2.9981958866119385\n",
      "1413\n",
      "3.0057828426361084\n",
      "1414\n",
      "3.0018608570098877\n",
      "1415\n",
      "3.0130136013031006\n",
      "1416\n",
      "3.0097177028656006\n",
      "1417\n",
      "3.0195093154907227\n",
      "1418\n",
      "3.009293794631958\n",
      "1419\n",
      "3.0125763416290283\n",
      "1420\n",
      "3.002948760986328\n",
      "1421\n",
      "3.0063512325286865\n",
      "1422\n",
      "2.9992990493774414\n",
      "1423\n",
      "3.0069973468780518\n",
      "1424\n",
      "3.003706455230713\n",
      "1425\n",
      "3.0161805152893066\n",
      "1426\n",
      "3.00899600982666\n",
      "1427\n",
      "3.013575315475464\n",
      "1428\n",
      "3.004176616668701\n",
      "1429\n",
      "3.0093636512756348\n",
      "1430\n",
      "3.0003232955932617\n",
      "1431\n",
      "3.004676580429077\n",
      "1432\n",
      "3.0004429817199707\n",
      "1433\n",
      "3.0113632678985596\n",
      "1434\n",
      "3.0064361095428467\n",
      "1435\n",
      "3.015436887741089\n",
      "1436\n",
      "3.0066275596618652\n",
      "1437\n",
      "3.0122244358062744\n",
      "1438\n",
      "3.0007622241973877\n",
      "1439\n",
      "3.0040132999420166\n",
      "1440\n",
      "2.9980971813201904\n",
      "1441\n",
      "3.0067050457000732\n",
      "1442\n",
      "3.002223014831543\n",
      "1443\n",
      "3.0113890171051025\n",
      "1444\n",
      "3.0066137313842773\n",
      "1445\n",
      "3.017360210418701\n",
      "1446\n",
      "3.005218982696533\n",
      "1447\n",
      "3.007018804550171\n",
      "1448\n",
      "2.997208595275879\n",
      "1449\n",
      "3.0017950534820557\n",
      "1450\n",
      "2.9954957962036133\n",
      "1451\n",
      "3.004411458969116\n",
      "1452\n",
      "3.004477024078369\n",
      "1453\n",
      "3.0204343795776367\n",
      "1454\n",
      "3.0108189582824707\n",
      "1455\n",
      "3.013953685760498\n",
      "1456\n",
      "2.9998762607574463\n",
      "1457\n",
      "2.999682664871216\n",
      "1458\n",
      "2.989717960357666\n",
      "1459\n",
      "2.9945216178894043\n",
      "1460\n",
      "2.995009422302246\n",
      "1461\n",
      "3.0127949714660645\n",
      "1462\n",
      "3.0122156143188477\n",
      "1463\n",
      "3.024353265762329\n",
      "1464\n",
      "3.0087900161743164\n",
      "1465\n",
      "3.008288621902466\n",
      "1466\n",
      "2.992255926132202\n",
      "1467\n",
      "2.9918556213378906\n",
      "1468\n",
      "2.986745834350586\n",
      "1469\n",
      "2.998671770095825\n",
      "1470\n",
      "3.0008935928344727\n",
      "1471\n",
      "3.0179693698883057\n",
      "1472\n",
      "3.015395402908325\n",
      "1473\n",
      "3.026625871658325\n",
      "1474\n",
      "3.0052876472473145\n",
      "1475\n",
      "2.995662212371826\n",
      "1476\n",
      "2.981353282928467\n",
      "1477\n",
      "2.983997344970703\n",
      "1478\n",
      "2.9862935543060303\n",
      "1479\n",
      "3.0099899768829346\n",
      "1480\n",
      "3.0190789699554443\n",
      "1481\n",
      "3.0395750999450684\n",
      "1482\n",
      "3.016559362411499\n",
      "1483\n",
      "3.004361391067505\n",
      "1484\n",
      "2.9824838638305664\n",
      "1485\n",
      "2.9784023761749268\n",
      "1486\n",
      "2.97594952583313\n",
      "1487\n",
      "2.9926676750183105\n",
      "1488\n",
      "3.00162672996521\n",
      "1489\n",
      "3.0292632579803467\n",
      "1490\n",
      "3.02791428565979\n",
      "1491\n",
      "3.032179355621338\n",
      "1492\n",
      "2.9968085289001465\n",
      "1493\n",
      "2.9821550846099854\n",
      "1494\n",
      "2.9694879055023193\n",
      "1495\n",
      "2.9723150730133057\n",
      "1496\n",
      "2.9806153774261475\n",
      "1497\n",
      "3.013437032699585\n",
      "1498\n",
      "3.032001256942749\n",
      "1499\n",
      "3.059516191482544\n",
      "1500\n",
      "3.0210812091827393\n",
      "1501\n",
      "2.995020866394043\n",
      "1502\n",
      "2.969900131225586\n",
      "1503\n",
      "2.9629251956939697\n",
      "1504\n",
      "2.961879253387451\n",
      "1505\n",
      "2.9804697036743164\n",
      "1506\n",
      "2.999539375305176\n",
      "1507\n",
      "3.047675848007202\n",
      "1508\n",
      "3.0499026775360107\n",
      "1509\n",
      "3.0411391258239746\n",
      "1510\n",
      "2.99224591255188\n",
      "1511\n",
      "2.9713528156280518\n",
      "1512\n",
      "2.957815408706665\n",
      "1513\n",
      "2.9581573009490967\n",
      "1514\n",
      "2.9664411544799805\n",
      "1515\n",
      "3.001720905303955\n",
      "1516\n",
      "3.0347330570220947\n",
      "1517\n",
      "3.0862197875976562\n",
      "1518\n",
      "3.0427486896514893\n",
      "1519\n",
      "2.999706268310547\n",
      "1520\n",
      "2.9652678966522217\n",
      "1521\n",
      "2.9531514644622803\n",
      "1522\n",
      "2.9473819732666016\n",
      "1523\n",
      "2.954979658126831\n",
      "1524\n",
      "2.975008249282837\n",
      "1525\n",
      "3.034536361694336\n",
      "1526\n",
      "3.076087236404419\n",
      "1527\n",
      "3.1012349128723145\n",
      "1528\n",
      "3.0146894454956055\n",
      "1529\n",
      "2.969297170639038\n",
      "1530\n",
      "2.948204755783081\n",
      "1531\n",
      "2.939966917037964\n",
      "1532\n",
      "2.937018871307373\n",
      "1533\n",
      "2.9445278644561768\n",
      "1534\n",
      "2.969454050064087\n",
      "1535\n",
      "3.0466091632843018\n",
      "1536\n",
      "3.1123881340026855\n",
      "1537\n",
      "3.1403231620788574\n",
      "1538\n",
      "3.016524314880371\n",
      "1539\n",
      "2.965080976486206\n",
      "1540\n",
      "2.944338321685791\n",
      "1541\n",
      "2.9339592456817627\n",
      "1542\n",
      "2.928119659423828\n",
      "1543\n",
      "2.925929546356201\n",
      "1544\n",
      "2.931595802307129\n",
      "1545\n",
      "2.9601104259490967\n",
      "1546\n",
      "3.034039258956909\n",
      "1547\n",
      "3.2032952308654785\n",
      "1548\n",
      "3.162353038787842\n",
      "1549\n",
      "3.032186985015869\n",
      "1550\n",
      "2.969594717025757\n",
      "1551\n",
      "2.9496846199035645\n",
      "1552\n",
      "2.9364280700683594\n",
      "1553\n",
      "2.9274702072143555\n",
      "1554\n",
      "2.9216866493225098\n",
      "1555\n",
      "2.918280839920044\n",
      "1556\n",
      "2.916048049926758\n",
      "1557\n",
      "2.916837453842163\n",
      "1558\n",
      "2.9272677898406982\n",
      "1559\n",
      "2.982527256011963\n",
      "1560\n",
      "3.151179552078247\n",
      "1561\n",
      "3.4288463592529297\n",
      "1562\n",
      "3.159879207611084\n",
      "1563\n",
      "3.016888380050659\n",
      "1564\n",
      "3.001704216003418\n",
      "1565\n",
      "2.9855828285217285\n",
      "1566\n",
      "2.968289613723755\n",
      "1567\n",
      "2.952641487121582\n",
      "1568\n",
      "2.94130539894104\n",
      "1569\n",
      "2.9332075119018555\n",
      "1570\n",
      "2.9267020225524902\n",
      "1571\n",
      "2.9214751720428467\n",
      "1572\n",
      "2.9169607162475586\n",
      "1573\n",
      "2.9133665561676025\n",
      "1574\n",
      "2.910381555557251\n",
      "1575\n",
      "2.9095618724823\n",
      "1576\n",
      "2.916337013244629\n",
      "1577\n",
      "2.9604880809783936\n",
      "1578\n",
      "3.13407039642334\n",
      "1579\n",
      "3.5239856243133545\n",
      "1580\n",
      "3.2298314571380615\n",
      "1581\n",
      "3.0307581424713135\n",
      "1582\n",
      "3.0179340839385986\n",
      "1583\n",
      "3.0057454109191895\n",
      "1584\n",
      "2.9914305210113525\n",
      "1585\n",
      "2.9761271476745605\n",
      "1586\n",
      "2.962355375289917\n",
      "1587\n",
      "2.950124979019165\n",
      "1588\n",
      "2.9391016960144043\n",
      "1589\n",
      "2.9299464225769043\n",
      "1590\n",
      "2.923046350479126\n",
      "1591\n",
      "2.918288230895996\n",
      "1592\n",
      "2.9144320487976074\n",
      "1593\n",
      "2.911113977432251\n",
      "1594\n",
      "2.908607244491577\n",
      "1595\n",
      "2.906809091567993\n",
      "1596\n",
      "2.9067091941833496\n",
      "1597\n",
      "2.917384386062622\n",
      "1598\n",
      "2.972728729248047\n",
      "1599\n",
      "3.2293856143951416\n",
      "1600\n",
      "3.449955940246582\n",
      "1601\n",
      "3.288557767868042\n",
      "1602\n",
      "3.0285818576812744\n",
      "1603\n",
      "3.018794536590576\n",
      "1604\n",
      "3.0073916912078857\n",
      "1605\n",
      "2.9935126304626465\n",
      "1606\n",
      "2.977576971054077\n",
      "1607\n",
      "2.961397647857666\n",
      "1608\n",
      "2.947265863418579\n",
      "1609\n",
      "2.9360687732696533\n",
      "1610\n",
      "2.926129102706909\n",
      "1611\n",
      "2.9181089401245117\n",
      "1612\n",
      "2.912100315093994\n",
      "1613\n",
      "2.9076859951019287\n",
      "1614\n",
      "2.9046812057495117\n",
      "1615\n",
      "2.9032740592956543\n",
      "1616\n",
      "2.9072091579437256\n",
      "1617\n",
      "2.939340114593506\n",
      "1618\n",
      "3.07383131980896\n",
      "1619\n",
      "3.449942111968994\n",
      "1620\n",
      "3.3009488582611084\n",
      "1621\n",
      "3.0385286808013916\n",
      "1622\n",
      "3.0083062648773193\n",
      "1623\n",
      "2.9941000938415527\n",
      "1624\n",
      "2.978728771209717\n",
      "1625\n",
      "2.9637527465820312\n",
      "1626\n",
      "2.950731039047241\n",
      "1627\n",
      "2.9389288425445557\n",
      "1628\n",
      "2.9279110431671143\n",
      "1629\n",
      "2.9184935092926025\n",
      "1630\n",
      "2.9120664596557617\n",
      "1631\n",
      "2.908289670944214\n",
      "1632\n",
      "2.9054837226867676\n",
      "1633\n",
      "2.9040331840515137\n",
      "1634\n",
      "2.9086263179779053\n",
      "1635\n",
      "2.935166597366333\n",
      "1636\n",
      "3.0686323642730713\n",
      "1637\n",
      "3.3280324935913086\n",
      "1638\n",
      "3.4645981788635254\n",
      "1639\n",
      "3.0537893772125244\n",
      "1640\n",
      "3.018700122833252\n",
      "1641\n",
      "3.0074899196624756\n",
      "1642\n",
      "2.9955313205718994\n",
      "1643\n",
      "2.9830288887023926\n",
      "1644\n",
      "2.969538927078247\n",
      "1645\n",
      "2.9556760787963867\n",
      "1646\n",
      "2.9421658515930176\n",
      "1647\n",
      "2.93137264251709\n",
      "1648\n",
      "2.9239566326141357\n",
      "1649\n",
      "2.9166698455810547\n",
      "1650\n",
      "2.9109559059143066\n",
      "1651\n",
      "2.90676212310791\n",
      "1652\n",
      "2.9042274951934814\n",
      "1653\n",
      "2.9058780670166016\n",
      "1654\n",
      "2.925398826599121\n",
      "1655\n",
      "3.0247719287872314\n",
      "1656\n",
      "3.295945644378662\n",
      "1657\n",
      "3.5007147789001465\n",
      "1658\n",
      "3.0886428356170654\n",
      "1659\n",
      "3.021376848220825\n",
      "1660\n",
      "3.0094125270843506\n",
      "1661\n",
      "2.9969422817230225\n",
      "1662\n",
      "2.983152151107788\n",
      "1663\n",
      "2.9680824279785156\n",
      "1664\n",
      "2.9521734714508057\n",
      "1665\n",
      "2.9366090297698975\n",
      "1666\n",
      "2.9236018657684326\n",
      "1667\n",
      "2.913647174835205\n",
      "1668\n",
      "2.906273365020752\n",
      "1669\n",
      "2.901050329208374\n",
      "1670\n",
      "2.8978240489959717\n",
      "1671\n",
      "2.8970947265625\n",
      "1672\n",
      "2.9044620990753174\n",
      "1673\n",
      "2.952847719192505\n",
      "1674\n",
      "3.1545920372009277\n",
      "1675\n",
      "3.554868459701538\n",
      "1676\n",
      "3.2066845893859863\n",
      "1677\n",
      "3.0264229774475098\n",
      "1678\n",
      "3.0158908367156982\n",
      "1679\n",
      "3.0039007663726807\n",
      "1680\n",
      "2.989720106124878\n",
      "1681\n",
      "2.9736149311065674\n",
      "1682\n",
      "2.9560294151306152\n",
      "1683\n",
      "2.9402811527252197\n",
      "1684\n",
      "2.928129196166992\n",
      "1685\n",
      "2.9190752506256104\n",
      "1686\n",
      "2.9125053882598877\n",
      "1687\n",
      "2.906960964202881\n",
      "1688\n",
      "2.902869939804077\n",
      "1689\n",
      "2.9004080295562744\n",
      "1690\n",
      "2.9011266231536865\n",
      "1691\n",
      "2.916902542114258\n",
      "1692\n",
      "2.9872093200683594\n",
      "1693\n",
      "3.244589328765869\n",
      "1694\n",
      "3.3234212398529053\n",
      "1695\n",
      "3.156144618988037\n",
      "1696\n",
      "2.992443561553955\n",
      "1697\n",
      "2.9744362831115723\n",
      "1698\n",
      "2.95592999458313\n",
      "1699\n",
      "2.938016414642334\n",
      "1700\n",
      "2.923442840576172\n",
      "1701\n",
      "2.9112977981567383\n",
      "1702\n",
      "2.9030251502990723\n",
      "1703\n",
      "2.8980417251586914\n",
      "1704\n",
      "2.8955078125\n",
      "1705\n",
      "2.8982431888580322\n",
      "1706\n",
      "2.924334764480591\n",
      "1707\n",
      "3.0317039489746094\n",
      "1708\n",
      "3.351919412612915\n",
      "1709\n",
      "3.2807936668395996\n",
      "1710\n",
      "3.0439460277557373\n",
      "1711\n",
      "2.9856679439544678\n",
      "1712\n",
      "2.967684268951416\n",
      "1713\n",
      "2.949705123901367\n",
      "1714\n",
      "2.934095859527588\n",
      "1715\n",
      "2.9205970764160156\n",
      "1716\n",
      "2.909255027770996\n",
      "1717\n",
      "2.9015233516693115\n",
      "1718\n",
      "2.896667242050171\n",
      "1719\n",
      "2.89302396774292\n",
      "1720\n",
      "2.8906168937683105\n",
      "1721\n",
      "2.8915536403656006\n",
      "1722\n",
      "2.9084362983703613\n",
      "1723\n",
      "2.992753505706787\n",
      "1724\n",
      "3.3200666904449463\n",
      "1725\n",
      "3.3797848224639893\n",
      "1726\n",
      "3.120419979095459\n",
      "1727\n",
      "2.9989683628082275\n",
      "1728\n",
      "2.982353687286377\n",
      "1729\n",
      "2.9647812843322754\n",
      "1730\n",
      "2.9470407962799072\n",
      "1731\n",
      "2.9310531616210938\n",
      "1732\n",
      "2.9172134399414062\n",
      "1733\n",
      "2.906247615814209\n",
      "1734\n",
      "2.8987066745758057\n",
      "1735\n",
      "2.8941993713378906\n",
      "1736\n",
      "2.891993284225464\n",
      "1737\n",
      "2.898071050643921\n",
      "1738\n",
      "2.9288320541381836\n",
      "1739\n",
      "3.069373607635498\n",
      "1740\n",
      "3.308624505996704\n",
      "1741\n",
      "3.3824973106384277\n",
      "1742\n",
      "3.0310964584350586\n",
      "1743\n",
      "2.999807596206665\n",
      "1744\n",
      "2.9858946800231934\n",
      "1745\n",
      "2.970975160598755\n",
      "1746\n",
      "2.955254316329956\n",
      "1747\n",
      "2.9389142990112305\n",
      "1748\n",
      "2.923011302947998\n",
      "1749\n",
      "2.910719394683838\n",
      "1750\n",
      "2.9025139808654785\n",
      "1751\n",
      "2.896493673324585\n",
      "1752\n",
      "2.891688823699951\n",
      "1753\n",
      "2.887744426727295\n",
      "1754\n",
      "2.8862297534942627\n",
      "1755\n",
      "2.8943755626678467\n",
      "1756\n",
      "2.947395086288452\n",
      "1757\n",
      "3.1658389568328857\n",
      "1758\n",
      "3.6071619987487793\n",
      "1759\n",
      "3.215250253677368\n",
      "1760\n",
      "3.0281496047973633\n",
      "1761\n",
      "3.0184521675109863\n",
      "1762\n",
      "3.0070550441741943\n",
      "1763\n",
      "2.9929957389831543\n",
      "1764\n",
      "2.977170467376709\n",
      "1765\n",
      "2.9598135948181152\n",
      "1766\n",
      "2.9421515464782715\n",
      "1767\n",
      "2.92635178565979\n",
      "1768\n",
      "2.913381338119507\n",
      "1769\n",
      "2.9039053916931152\n",
      "1770\n",
      "2.8972442150115967\n",
      "1771\n",
      "2.892059803009033\n",
      "1772\n",
      "2.8878843784332275\n",
      "1773\n",
      "2.8847689628601074\n",
      "1774\n",
      "2.8821804523468018\n",
      "1775\n",
      "2.8804521560668945\n",
      "1776\n",
      "2.8815345764160156\n",
      "1777\n",
      "2.8995442390441895\n",
      "1778\n",
      "3.0135271549224854\n",
      "1779\n",
      "3.482546806335449\n",
      "1780\n",
      "3.4555857181549072\n",
      "1781\n",
      "3.095834732055664\n",
      "1782\n",
      "3.0163614749908447\n",
      "1783\n",
      "3.002551555633545\n",
      "1784\n",
      "2.987893581390381\n",
      "1785\n",
      "2.972348213195801\n",
      "1786\n",
      "2.9567415714263916\n",
      "1787\n",
      "2.9426677227020264\n",
      "1788\n",
      "2.9288547039031982\n",
      "1789\n",
      "2.916766881942749\n",
      "1790\n",
      "2.9058096408843994\n",
      "1791\n",
      "2.895977735519409\n",
      "1792\n",
      "2.888688802719116\n",
      "1793\n",
      "2.884582757949829\n",
      "1794\n",
      "2.884615182876587\n",
      "1795\n",
      "2.904047727584839\n",
      "1796\n",
      "2.9948506355285645\n",
      "1797\n",
      "3.328049898147583\n",
      "1798\n",
      "3.334798574447632\n",
      "1799\n",
      "3.0972089767456055\n",
      "1800\n",
      "2.9853851795196533\n",
      "1801\n",
      "2.9669458866119385\n",
      "1802\n",
      "2.947554349899292\n",
      "1803\n",
      "2.929006814956665\n",
      "1804\n",
      "2.9134457111358643\n",
      "1805\n",
      "2.9006872177124023\n",
      "1806\n",
      "2.8911616802215576\n",
      "1807\n",
      "2.8853447437286377\n",
      "1808\n",
      "2.8816874027252197\n",
      "1809\n",
      "2.8809409141540527\n",
      "1810\n",
      "2.891425848007202\n",
      "1811\n",
      "2.9493649005889893\n",
      "1812\n",
      "3.1897170543670654\n",
      "1813\n",
      "3.3646295070648193\n",
      "1814\n",
      "3.234848976135254\n",
      "1815\n",
      "2.998547315597534\n",
      "1816\n",
      "2.9816324710845947\n",
      "1817\n",
      "2.963378667831421\n",
      "1818\n",
      "2.94354248046875\n",
      "1819\n",
      "2.9246883392333984\n",
      "1820\n",
      "2.908994436264038\n",
      "1821\n",
      "2.8974435329437256\n",
      "1822\n",
      "2.8875913619995117\n",
      "1823\n",
      "2.8812382221221924\n",
      "1824\n",
      "2.8776731491088867\n",
      "1825\n",
      "2.8769540786743164\n",
      "1826\n",
      "2.8876960277557373\n",
      "1827\n",
      "2.952451229095459\n",
      "1828\n",
      "3.1823227405548096\n",
      "1829\n",
      "3.5884945392608643\n",
      "1830\n",
      "3.164313793182373\n",
      "1831\n",
      "3.0185422897338867\n",
      "1832\n",
      "3.0065226554870605\n",
      "1833\n",
      "2.9925696849823\n",
      "1834\n",
      "2.975797176361084\n",
      "1835\n",
      "2.957754135131836\n",
      "1836\n",
      "2.939722776412964\n",
      "1837\n",
      "2.922785758972168\n",
      "1838\n",
      "2.909552574157715\n",
      "1839\n",
      "2.899963617324829\n",
      "1840\n",
      "2.892979860305786\n",
      "1841\n",
      "2.887561082839966\n",
      "1842\n",
      "2.882807493209839\n",
      "1843\n",
      "2.8787074089050293\n",
      "1844\n",
      "2.8753247261047363\n",
      "1845\n",
      "2.876767873764038\n",
      "1846\n",
      "2.903203248977661\n",
      "1847\n",
      "3.0335519313812256\n",
      "1848\n",
      "3.4856977462768555\n",
      "1849\n",
      "3.3742966651916504\n",
      "1850\n",
      "3.0558667182922363\n",
      "1851\n",
      "3.0007035732269287\n",
      "1852\n",
      "2.9847655296325684\n",
      "1853\n",
      "2.967362880706787\n",
      "1854\n",
      "2.9494314193725586\n",
      "1855\n",
      "2.9327518939971924\n",
      "1856\n",
      "2.9173920154571533\n",
      "1857\n",
      "2.9038498401641846\n",
      "1858\n",
      "2.892411708831787\n",
      "1859\n",
      "2.883283853530884\n",
      "1860\n",
      "2.877772808074951\n",
      "1861\n",
      "2.8736724853515625\n",
      "1862\n",
      "2.8703746795654297\n",
      "1863\n",
      "2.8676226139068604\n",
      "1864\n",
      "2.8652420043945312\n",
      "1865\n",
      "2.8637776374816895\n",
      "1866\n",
      "2.867894411087036\n",
      "1867\n",
      "2.9080681800842285\n",
      "1868\n",
      "3.1442768573760986\n",
      "1869\n",
      "3.8696353435516357\n",
      "1870\n",
      "3.3032419681549072\n",
      "1871\n",
      "3.0465714931488037\n",
      "1872\n",
      "3.039860725402832\n",
      "1873\n",
      "3.0321199893951416\n",
      "1874\n",
      "3.0233449935913086\n",
      "1875\n",
      "3.013169765472412\n",
      "1876\n",
      "3.002164363861084\n",
      "1877\n",
      "2.9902052879333496\n",
      "1878\n",
      "2.977248430252075\n",
      "1879\n",
      "2.9640297889709473\n",
      "1880\n",
      "2.9514243602752686\n",
      "1881\n",
      "2.93904709815979\n",
      "1882\n",
      "2.9272212982177734\n",
      "1883\n",
      "2.915841817855835\n",
      "1884\n",
      "2.9053709506988525\n",
      "1885\n",
      "2.895806074142456\n",
      "1886\n",
      "2.8889596462249756\n",
      "1887\n",
      "2.883852958679199\n",
      "1888\n",
      "2.88138484954834\n",
      "1889\n",
      "2.887028217315674\n",
      "1890\n",
      "2.9196925163269043\n",
      "1891\n",
      "3.0743987560272217\n",
      "1892\n",
      "3.302658796310425\n",
      "1893\n",
      "3.4793174266815186\n",
      "1894\n",
      "3.018221139907837\n",
      "1895\n",
      "2.990678548812866\n",
      "1896\n",
      "2.9748175144195557\n",
      "1897\n",
      "2.956477642059326\n",
      "1898\n",
      "2.937091588973999\n",
      "1899\n",
      "2.919485569000244\n",
      "1900\n",
      "2.9057981967926025\n",
      "1901\n",
      "2.89486026763916\n",
      "1902\n",
      "2.8845460414886475\n",
      "1903\n",
      "2.876758575439453\n",
      "1904\n",
      "2.8717596530914307\n",
      "1905\n",
      "2.868809223175049\n",
      "1906\n",
      "2.8717503547668457\n",
      "1907\n",
      "2.9048361778259277\n",
      "1908\n",
      "3.0448665618896484\n",
      "1909\n",
      "3.4957003593444824\n",
      "1910\n",
      "3.279824733734131\n",
      "1911\n",
      "3.0268936157226562\n",
      "1912\n",
      "2.984771728515625\n",
      "1913\n",
      "2.967109441757202\n",
      "1914\n",
      "2.9484336376190186\n",
      "1915\n",
      "2.929874897003174\n",
      "1916\n",
      "2.912834644317627\n",
      "1917\n",
      "2.8976762294769287\n",
      "1918\n",
      "2.884671449661255\n",
      "1919\n",
      "2.875262498855591\n",
      "1920\n",
      "2.8693459033966064\n",
      "1921\n",
      "2.865269660949707\n",
      "1922\n",
      "2.862470865249634\n",
      "1923\n",
      "2.8633522987365723\n",
      "1924\n",
      "2.8816816806793213\n",
      "1925\n",
      "2.9826431274414062\n",
      "1926\n",
      "3.4161996841430664\n",
      "1927\n",
      "3.4229743480682373\n",
      "1928\n",
      "3.126215934753418\n",
      "1929\n",
      "2.998842716217041\n",
      "1930\n",
      "2.9812681674957275\n",
      "1931\n",
      "2.9626553058624268\n",
      "1932\n",
      "2.943423271179199\n",
      "1933\n",
      "2.9249229431152344\n",
      "1934\n",
      "2.9071505069732666\n",
      "1935\n",
      "2.891634225845337\n",
      "1936\n",
      "2.87972092628479\n",
      "1937\n",
      "2.8711841106414795\n",
      "1938\n",
      "2.8658554553985596\n",
      "1939\n",
      "2.8618686199188232\n",
      "1940\n",
      "2.8592495918273926\n",
      "1941\n",
      "2.860419750213623\n",
      "1942\n",
      "2.8771026134490967\n",
      "1943\n",
      "2.9856534004211426\n",
      "1944\n",
      "3.3362345695495605\n",
      "1945\n",
      "3.7614402770996094\n",
      "1946\n",
      "3.086212635040283\n",
      "1947\n",
      "3.035187005996704\n",
      "1948\n",
      "3.024282455444336\n",
      "1949\n",
      "3.013430118560791\n",
      "1950\n",
      "3.001558542251587\n",
      "1951\n",
      "2.9886748790740967\n",
      "1952\n",
      "2.9744207859039307\n",
      "1953\n",
      "2.95914626121521\n",
      "1954\n",
      "2.943732976913452\n",
      "1955\n",
      "2.928079128265381\n",
      "1956\n",
      "2.913235902786255\n",
      "1957\n",
      "2.901777505874634\n",
      "1958\n",
      "2.892855644226074\n",
      "1959\n",
      "2.8847219944000244\n",
      "1960\n",
      "2.877741575241089\n",
      "1961\n",
      "2.873201847076416\n",
      "1962\n",
      "2.872030735015869\n",
      "1963\n",
      "2.8799519538879395\n",
      "1964\n",
      "2.9239096641540527\n",
      "1965\n",
      "3.110440254211426\n",
      "1966\n",
      "3.358747959136963\n",
      "1967\n",
      "3.3502814769744873\n",
      "1968\n",
      "2.998988151550293\n",
      "1969\n",
      "2.9754726886749268\n",
      "1970\n",
      "2.9565393924713135\n",
      "1971\n",
      "2.9359049797058105\n",
      "1972\n",
      "2.915853261947632\n",
      "1973\n",
      "2.8993844985961914\n",
      "1974\n",
      "2.8876280784606934\n",
      "1975\n",
      "2.8765032291412354\n",
      "1976\n",
      "2.8676111698150635\n",
      "1977\n",
      "2.8623597621917725\n",
      "1978\n",
      "2.8606181144714355\n",
      "1979\n",
      "2.8681602478027344\n",
      "1980\n",
      "2.9140124320983887\n",
      "1981\n",
      "3.128068685531616\n",
      "1982\n",
      "3.445847511291504\n",
      "1983\n",
      "3.3735179901123047\n",
      "1984\n",
      "3.0117697715759277\n",
      "1985\n",
      "2.9968998432159424\n",
      "1986\n",
      "2.9819490909576416\n",
      "1987\n",
      "2.965060234069824\n",
      "1988\n",
      "2.946831226348877\n",
      "1989\n",
      "2.9282703399658203\n",
      "1990\n",
      "2.911156177520752\n",
      "1991\n",
      "2.898508310317993\n",
      "1992\n",
      "2.889188051223755\n",
      "1993\n",
      "2.8803904056549072\n",
      "1994\n",
      "2.87247633934021\n",
      "1995\n",
      "2.866365671157837\n",
      "1996\n",
      "2.8623714447021484\n",
      "1997\n",
      "2.86167049407959\n",
      "1998\n",
      "2.8734591007232666\n",
      "1999\n",
      "2.9516639709472656\n"
     ]
    }
   ],
   "source": [
    "#direct training:\n",
    "for t in range(2000):\n",
    "    print(t)\n",
    "    prediction= net(x_train)\n",
    "    loss=loss_func(prediction,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()   \n",
    "    print(loss.data[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#batch training:\n",
    "BATCH_SIZE = 500\n",
    "x_train = torch.from_numpy(xy_train[:,1:-1])\n",
    "y_train = torch.from_numpy(xy_train[:,[-1]])\n",
    "\n",
    "torch_dataset = Data.TensorDataset(data_tensor = x_train, target_tensor = y_train)\n",
    "loader = Data.DataLoader(\n",
    "    dataset = torch_dataset,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = 2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Step:  0\n",
      "training loss is:  2.2752249240875244\n",
      "Epoch: 0 | Step:  1\n",
      "training loss is:  2.5938479900360107\n",
      "Epoch: 0 | Step:  2\n",
      "training loss is:  2.104236602783203\n",
      "Epoch: 0 | Step:  3\n",
      "training loss is:  2.7231695652008057\n",
      "Epoch: 0 | Step:  4\n",
      "training loss is:  2.723329782485962\n",
      "Epoch: 0 | Step:  5\n",
      "training loss is:  2.6355268955230713\n",
      "Epoch: 0 | Step:  6\n",
      "training loss is:  2.542234420776367\n",
      "Epoch: 0 | Step:  7\n",
      "training loss is:  2.2264935970306396\n",
      "Epoch: 0 | Step:  8\n",
      "training loss is:  2.5353524684906006\n",
      "Epoch: 0 | Step:  9\n",
      "training loss is:  2.1147537231445312\n",
      "Epoch: 0 | Step:  10\n",
      "training loss is:  2.6018459796905518\n",
      "Epoch: 0 | Step:  11\n",
      "training loss is:  2.4730992317199707\n",
      "Epoch: 0 | Step:  12\n",
      "training loss is:  2.6352806091308594\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5746769905090332\n",
      "MSE is: 4.9648003578186035\n",
      "\n",
      "\n",
      "Epoch: 1 | Step:  0\n",
      "training loss is:  2.450000286102295\n",
      "Epoch: 1 | Step:  1\n",
      "training loss is:  2.395557165145874\n",
      "Epoch: 1 | Step:  2\n",
      "training loss is:  2.725883722305298\n",
      "Epoch: 1 | Step:  3\n",
      "training loss is:  2.6152002811431885\n",
      "Epoch: 1 | Step:  4\n",
      "training loss is:  2.4235339164733887\n",
      "Epoch: 1 | Step:  5\n",
      "training loss is:  2.5007846355438232\n",
      "Epoch: 1 | Step:  6\n",
      "training loss is:  2.3116776943206787\n",
      "Epoch: 1 | Step:  7\n",
      "training loss is:  2.532076835632324\n",
      "Epoch: 1 | Step:  8\n",
      "training loss is:  2.3661065101623535\n",
      "Epoch: 1 | Step:  9\n",
      "training loss is:  2.4474949836730957\n",
      "Epoch: 1 | Step:  10\n",
      "training loss is:  2.2115118503570557\n",
      "Epoch: 1 | Step:  11\n",
      "training loss is:  2.4378371238708496\n",
      "Epoch: 1 | Step:  12\n",
      "training loss is:  2.422433376312256\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5628077983856201\n",
      "MSE is: 4.878604412078857\n",
      "\n",
      "\n",
      "Epoch: 2 | Step:  0\n",
      "training loss is:  2.466782808303833\n",
      "Epoch: 2 | Step:  1\n",
      "training loss is:  2.160378932952881\n",
      "Epoch: 2 | Step:  2\n",
      "training loss is:  2.644115924835205\n",
      "Epoch: 2 | Step:  3\n",
      "training loss is:  2.60823917388916\n",
      "Epoch: 2 | Step:  4\n",
      "training loss is:  2.3471479415893555\n",
      "Epoch: 2 | Step:  5\n",
      "training loss is:  2.893172264099121\n",
      "Epoch: 2 | Step:  6\n",
      "training loss is:  2.236572265625\n",
      "Epoch: 2 | Step:  7\n",
      "training loss is:  2.4479293823242188\n",
      "Epoch: 2 | Step:  8\n",
      "training loss is:  2.7567849159240723\n",
      "Epoch: 2 | Step:  9\n",
      "training loss is:  2.936939239501953\n",
      "Epoch: 2 | Step:  10\n",
      "training loss is:  2.2794313430786133\n",
      "Epoch: 2 | Step:  11\n",
      "training loss is:  2.7209272384643555\n",
      "Epoch: 2 | Step:  12\n",
      "training loss is:  2.8013997077941895\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4427950382232666\n",
      "MSE is: 4.299988269805908\n",
      "\n",
      "\n",
      "Epoch: 3 | Step:  0\n",
      "training loss is:  2.766209125518799\n",
      "Epoch: 3 | Step:  1\n",
      "training loss is:  2.30086612701416\n",
      "Epoch: 3 | Step:  2\n",
      "training loss is:  2.188323974609375\n",
      "Epoch: 3 | Step:  3\n",
      "training loss is:  2.490940809249878\n",
      "Epoch: 3 | Step:  4\n",
      "training loss is:  2.329094409942627\n",
      "Epoch: 3 | Step:  5\n",
      "training loss is:  2.3432254791259766\n",
      "Epoch: 3 | Step:  6\n",
      "training loss is:  2.3630402088165283\n",
      "Epoch: 3 | Step:  7\n",
      "training loss is:  2.4731345176696777\n",
      "Epoch: 3 | Step:  8\n",
      "training loss is:  2.552442789077759\n",
      "Epoch: 3 | Step:  9\n",
      "training loss is:  2.265789270401001\n",
      "Epoch: 3 | Step:  10\n",
      "training loss is:  2.3230278491973877\n",
      "Epoch: 3 | Step:  11\n",
      "training loss is:  2.626420497894287\n",
      "Epoch: 3 | Step:  12\n",
      "training loss is:  2.780205011367798\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5300012826919556\n",
      "MSE is: 4.752754211425781\n",
      "\n",
      "\n",
      "Epoch: 4 | Step:  0\n",
      "training loss is:  2.5368964672088623\n",
      "Epoch: 4 | Step:  1\n",
      "training loss is:  2.1446332931518555\n",
      "Epoch: 4 | Step:  2\n",
      "training loss is:  2.3747687339782715\n",
      "Epoch: 4 | Step:  3\n",
      "training loss is:  2.836148500442505\n",
      "Epoch: 4 | Step:  4\n",
      "training loss is:  2.5008254051208496\n",
      "Epoch: 4 | Step:  5\n",
      "training loss is:  2.422987222671509\n",
      "Epoch: 4 | Step:  6\n",
      "training loss is:  2.846097469329834\n",
      "Epoch: 4 | Step:  7\n",
      "training loss is:  2.471059560775757\n",
      "Epoch: 4 | Step:  8\n",
      "training loss is:  2.418534517288208\n",
      "Epoch: 4 | Step:  9\n",
      "training loss is:  2.6730313301086426\n",
      "Epoch: 4 | Step:  10\n",
      "training loss is:  2.5427026748657227\n",
      "Epoch: 4 | Step:  11\n",
      "training loss is:  2.564676523208618\n",
      "Epoch: 4 | Step:  12\n",
      "training loss is:  2.619180917739868\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5272014141082764\n",
      "MSE is: 4.665560245513916\n",
      "\n",
      "\n",
      "Epoch: 5 | Step:  0\n",
      "training loss is:  2.8708314895629883\n",
      "Epoch: 5 | Step:  1\n",
      "training loss is:  2.6234211921691895\n",
      "Epoch: 5 | Step:  2\n",
      "training loss is:  2.5955631732940674\n",
      "Epoch: 5 | Step:  3\n",
      "training loss is:  2.4031801223754883\n",
      "Epoch: 5 | Step:  4\n",
      "training loss is:  2.3889505863189697\n",
      "Epoch: 5 | Step:  5\n",
      "training loss is:  2.4230029582977295\n",
      "Epoch: 5 | Step:  6\n",
      "training loss is:  2.3443045616149902\n",
      "Epoch: 5 | Step:  7\n",
      "training loss is:  2.4983904361724854\n",
      "Epoch: 5 | Step:  8\n",
      "training loss is:  2.1555700302124023\n",
      "Epoch: 5 | Step:  9\n",
      "training loss is:  2.4500207901000977\n",
      "Epoch: 5 | Step:  10\n",
      "training loss is:  2.6530325412750244\n",
      "Epoch: 5 | Step:  11\n",
      "training loss is:  2.3530991077423096\n",
      "Epoch: 5 | Step:  12\n",
      "training loss is:  2.198702335357666\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5319342613220215\n",
      "MSE is: 4.753918647766113\n",
      "\n",
      "\n",
      "Epoch: 6 | Step:  0\n",
      "training loss is:  1.8819465637207031\n",
      "Epoch: 6 | Step:  1\n",
      "training loss is:  2.3699827194213867\n",
      "Epoch: 6 | Step:  2\n",
      "training loss is:  2.3949315547943115\n",
      "Epoch: 6 | Step:  3\n",
      "training loss is:  2.4272539615631104\n",
      "Epoch: 6 | Step:  4\n",
      "training loss is:  2.407902240753174\n",
      "Epoch: 6 | Step:  5\n",
      "training loss is:  2.9232423305511475\n",
      "Epoch: 6 | Step:  6\n",
      "training loss is:  2.4058871269226074\n",
      "Epoch: 6 | Step:  7\n",
      "training loss is:  2.67637300491333\n",
      "Epoch: 6 | Step:  8\n",
      "training loss is:  2.495255947113037\n",
      "Epoch: 6 | Step:  9\n",
      "training loss is:  2.612860679626465\n",
      "Epoch: 6 | Step:  10\n",
      "training loss is:  2.698969841003418\n",
      "Epoch: 6 | Step:  11\n",
      "training loss is:  2.3683550357818604\n",
      "Epoch: 6 | Step:  12\n",
      "training loss is:  2.431150197982788\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5327980518341064\n",
      "MSE is: 4.72289514541626\n",
      "\n",
      "\n",
      "Epoch: 7 | Step:  0\n",
      "training loss is:  2.5584187507629395\n",
      "Epoch: 7 | Step:  1\n",
      "training loss is:  2.2281668186187744\n",
      "Epoch: 7 | Step:  2\n",
      "training loss is:  2.4477083683013916\n",
      "Epoch: 7 | Step:  3\n",
      "training loss is:  2.7933945655822754\n",
      "Epoch: 7 | Step:  4\n",
      "training loss is:  2.86189341545105\n",
      "Epoch: 7 | Step:  5\n",
      "training loss is:  3.1701674461364746\n",
      "Epoch: 7 | Step:  6\n",
      "training loss is:  2.486178159713745\n",
      "Epoch: 7 | Step:  7\n",
      "training loss is:  2.1555137634277344\n",
      "Epoch: 7 | Step:  8\n",
      "training loss is:  2.452599287033081\n",
      "Epoch: 7 | Step:  9\n",
      "training loss is:  2.415374755859375\n",
      "Epoch: 7 | Step:  10\n",
      "training loss is:  2.280007839202881\n",
      "Epoch: 7 | Step:  11\n",
      "training loss is:  2.5808677673339844\n",
      "Epoch: 7 | Step:  12\n",
      "training loss is:  2.2200539112091064\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5226173400878906\n",
      "MSE is: 4.721134662628174\n",
      "\n",
      "\n",
      "Epoch: 8 | Step:  0\n",
      "training loss is:  2.3660640716552734\n",
      "Epoch: 8 | Step:  1\n",
      "training loss is:  2.230834722518921\n",
      "Epoch: 8 | Step:  2\n",
      "training loss is:  2.731166362762451\n",
      "Epoch: 8 | Step:  3\n",
      "training loss is:  2.455183267593384\n",
      "Epoch: 8 | Step:  4\n",
      "training loss is:  2.395928382873535\n",
      "Epoch: 8 | Step:  5\n",
      "training loss is:  3.108630657196045\n",
      "Epoch: 8 | Step:  6\n",
      "training loss is:  2.481158494949341\n",
      "Epoch: 8 | Step:  7\n",
      "training loss is:  2.497751235961914\n",
      "Epoch: 8 | Step:  8\n",
      "training loss is:  2.481672763824463\n",
      "Epoch: 8 | Step:  9\n",
      "training loss is:  2.3283944129943848\n",
      "Epoch: 8 | Step:  10\n",
      "training loss is:  2.0299911499023438\n",
      "Epoch: 8 | Step:  11\n",
      "training loss is:  2.455151319503784\n",
      "Epoch: 8 | Step:  12\n",
      "training loss is:  2.857764482498169\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.6495369672775269\n",
      "MSE is: 5.378270626068115\n",
      "\n",
      "\n",
      "Epoch: 9 | Step:  0\n",
      "training loss is:  2.4331884384155273\n",
      "Epoch: 9 | Step:  1\n",
      "training loss is:  2.422339916229248\n",
      "Epoch: 9 | Step:  2\n",
      "training loss is:  2.6003663539886475\n",
      "Epoch: 9 | Step:  3\n",
      "training loss is:  2.583435297012329\n",
      "Epoch: 9 | Step:  4\n",
      "training loss is:  2.5227489471435547\n",
      "Epoch: 9 | Step:  5\n",
      "training loss is:  2.6201844215393066\n",
      "Epoch: 9 | Step:  6\n",
      "training loss is:  2.7968366146087646\n",
      "Epoch: 9 | Step:  7\n",
      "training loss is:  2.5857417583465576\n",
      "Epoch: 9 | Step:  8\n",
      "training loss is:  2.420239210128784\n",
      "Epoch: 9 | Step:  9\n",
      "training loss is:  2.367231845855713\n",
      "Epoch: 9 | Step:  10\n",
      "training loss is:  2.536294460296631\n",
      "Epoch: 9 | Step:  11\n",
      "training loss is:  2.5465922355651855\n",
      "Epoch: 9 | Step:  12\n",
      "training loss is:  2.307229518890381\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.466295599937439\n",
      "MSE is: 4.406162738800049\n",
      "\n",
      "\n",
      "Epoch: 10 | Step:  0\n",
      "training loss is:  2.4642393589019775\n",
      "Epoch: 10 | Step:  1\n",
      "training loss is:  2.734548807144165\n",
      "Epoch: 10 | Step:  2\n",
      "training loss is:  2.2664718627929688\n",
      "Epoch: 10 | Step:  3\n",
      "training loss is:  2.1536293029785156\n",
      "Epoch: 10 | Step:  4\n",
      "training loss is:  2.402597665786743\n",
      "Epoch: 10 | Step:  5\n",
      "training loss is:  2.2022311687469482\n",
      "Epoch: 10 | Step:  6\n",
      "training loss is:  2.528369426727295\n",
      "Epoch: 10 | Step:  7\n",
      "training loss is:  2.284062385559082\n",
      "Epoch: 10 | Step:  8\n",
      "training loss is:  2.3292858600616455\n",
      "Epoch: 10 | Step:  9\n",
      "training loss is:  2.546924114227295\n",
      "Epoch: 10 | Step:  10\n",
      "training loss is:  2.717527151107788\n",
      "Epoch: 10 | Step:  11\n",
      "training loss is:  2.445586681365967\n",
      "Epoch: 10 | Step:  12\n",
      "training loss is:  2.5241165161132812\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5532797574996948\n",
      "MSE is: 4.819635391235352\n",
      "\n",
      "\n",
      "Epoch: 11 | Step:  0\n",
      "training loss is:  2.474865198135376\n",
      "Epoch: 11 | Step:  1\n",
      "training loss is:  2.2887489795684814\n",
      "Epoch: 11 | Step:  2\n",
      "training loss is:  2.312309980392456\n",
      "Epoch: 11 | Step:  3\n",
      "training loss is:  2.219843864440918\n",
      "Epoch: 11 | Step:  4\n",
      "training loss is:  2.372692823410034\n",
      "Epoch: 11 | Step:  5\n",
      "training loss is:  2.396444320678711\n",
      "Epoch: 11 | Step:  6\n",
      "training loss is:  2.71221661567688\n",
      "Epoch: 11 | Step:  7\n",
      "training loss is:  2.778520345687866\n",
      "Epoch: 11 | Step:  8\n",
      "training loss is:  2.9689953327178955\n",
      "Epoch: 11 | Step:  9\n",
      "training loss is:  2.6797420978546143\n",
      "Epoch: 11 | Step:  10\n",
      "training loss is:  2.3114092350006104\n",
      "Epoch: 11 | Step:  11\n",
      "training loss is:  2.7833824157714844\n",
      "Epoch: 11 | Step:  12\n",
      "training loss is:  2.547844171524048\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4942642450332642\n",
      "MSE is: 4.55686092376709\n",
      "\n",
      "\n",
      "Epoch: 12 | Step:  0\n",
      "training loss is:  2.8619778156280518\n",
      "Epoch: 12 | Step:  1\n",
      "training loss is:  2.480428457260132\n",
      "Epoch: 12 | Step:  2\n",
      "training loss is:  2.5720880031585693\n",
      "Epoch: 12 | Step:  3\n",
      "training loss is:  2.5204787254333496\n",
      "Epoch: 12 | Step:  4\n",
      "training loss is:  2.454148530960083\n",
      "Epoch: 12 | Step:  5\n",
      "training loss is:  2.174220085144043\n",
      "Epoch: 12 | Step:  6\n",
      "training loss is:  2.652890920639038\n",
      "Epoch: 12 | Step:  7\n",
      "training loss is:  2.3767335414886475\n",
      "Epoch: 12 | Step:  8\n",
      "training loss is:  2.416442632675171\n",
      "Epoch: 12 | Step:  9\n",
      "training loss is:  2.4548022747039795\n",
      "Epoch: 12 | Step:  10\n",
      "training loss is:  2.558286666870117\n",
      "Epoch: 12 | Step:  11\n",
      "training loss is:  2.6784839630126953\n",
      "Epoch: 12 | Step:  12\n",
      "training loss is:  2.209501028060913\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.554233193397522\n",
      "MSE is: 4.86564826965332\n",
      "\n",
      "\n",
      "Epoch: 13 | Step:  0\n",
      "training loss is:  2.102741003036499\n",
      "Epoch: 13 | Step:  1\n",
      "training loss is:  2.1680750846862793\n",
      "Epoch: 13 | Step:  2\n",
      "training loss is:  2.7537992000579834\n",
      "Epoch: 13 | Step:  3\n",
      "training loss is:  2.5317885875701904\n",
      "Epoch: 13 | Step:  4\n",
      "training loss is:  3.099658727645874\n",
      "Epoch: 13 | Step:  5\n",
      "training loss is:  3.4081366062164307\n",
      "Epoch: 13 | Step:  6\n",
      "training loss is:  2.871774911880493\n",
      "Epoch: 13 | Step:  7\n",
      "training loss is:  2.2454798221588135\n",
      "Epoch: 13 | Step:  8\n",
      "training loss is:  2.501126527786255\n",
      "Epoch: 13 | Step:  9\n",
      "training loss is:  2.284250259399414\n",
      "Epoch: 13 | Step:  10\n",
      "training loss is:  2.599996566772461\n",
      "Epoch: 13 | Step:  11\n",
      "training loss is:  2.6262729167938232\n",
      "Epoch: 13 | Step:  12\n",
      "training loss is:  2.077711820602417\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.513249397277832\n",
      "MSE is: 4.676506519317627\n",
      "\n",
      "\n",
      "Epoch: 14 | Step:  0\n",
      "training loss is:  2.54876708984375\n",
      "Epoch: 14 | Step:  1\n",
      "training loss is:  2.56093692779541\n",
      "Epoch: 14 | Step:  2\n",
      "training loss is:  2.177987813949585\n",
      "Epoch: 14 | Step:  3\n",
      "training loss is:  2.3398303985595703\n",
      "Epoch: 14 | Step:  4\n",
      "training loss is:  2.1084816455841064\n",
      "Epoch: 14 | Step:  5\n",
      "training loss is:  2.2995991706848145\n",
      "Epoch: 14 | Step:  6\n",
      "training loss is:  2.399984836578369\n",
      "Epoch: 14 | Step:  7\n",
      "training loss is:  2.6575887203216553\n",
      "Epoch: 14 | Step:  8\n",
      "training loss is:  2.3419904708862305\n",
      "Epoch: 14 | Step:  9\n",
      "training loss is:  2.586188554763794\n",
      "Epoch: 14 | Step:  10\n",
      "training loss is:  2.473367691040039\n",
      "Epoch: 14 | Step:  11\n",
      "training loss is:  2.6064059734344482\n",
      "Epoch: 14 | Step:  12\n",
      "training loss is:  2.8827288150787354\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5528512001037598\n",
      "MSE is: 4.78176212310791\n",
      "\n",
      "\n",
      "Epoch: 15 | Step:  0\n",
      "training loss is:  2.608555316925049\n",
      "Epoch: 15 | Step:  1\n",
      "training loss is:  2.208998441696167\n",
      "Epoch: 15 | Step:  2\n",
      "training loss is:  2.659919261932373\n",
      "Epoch: 15 | Step:  3\n",
      "training loss is:  2.4177136421203613\n",
      "Epoch: 15 | Step:  4\n",
      "training loss is:  2.4786369800567627\n",
      "Epoch: 15 | Step:  5\n",
      "training loss is:  2.3672938346862793\n",
      "Epoch: 15 | Step:  6\n",
      "training loss is:  2.351205348968506\n",
      "Epoch: 15 | Step:  7\n",
      "training loss is:  2.656480073928833\n",
      "Epoch: 15 | Step:  8\n",
      "training loss is:  2.9160408973693848\n",
      "Epoch: 15 | Step:  9\n",
      "training loss is:  2.2236328125\n",
      "Epoch: 15 | Step:  10\n",
      "training loss is:  2.4260287284851074\n",
      "Epoch: 15 | Step:  11\n",
      "training loss is:  2.6065492630004883\n",
      "Epoch: 15 | Step:  12\n",
      "training loss is:  2.443889856338501\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4950929880142212\n",
      "MSE is: 4.53100061416626\n",
      "\n",
      "\n",
      "Epoch: 16 | Step:  0\n",
      "training loss is:  2.163445234298706\n",
      "Epoch: 16 | Step:  1\n",
      "training loss is:  2.6871700286865234\n",
      "Epoch: 16 | Step:  2\n",
      "training loss is:  2.416077136993408\n",
      "Epoch: 16 | Step:  3\n",
      "training loss is:  2.706638813018799\n",
      "Epoch: 16 | Step:  4\n",
      "training loss is:  2.282714605331421\n",
      "Epoch: 16 | Step:  5\n",
      "training loss is:  2.5627694129943848\n",
      "Epoch: 16 | Step:  6\n",
      "training loss is:  2.1389899253845215\n",
      "Epoch: 16 | Step:  7\n",
      "training loss is:  2.3757622241973877\n",
      "Epoch: 16 | Step:  8\n",
      "training loss is:  2.5806210041046143\n",
      "Epoch: 16 | Step:  9\n",
      "training loss is:  2.379302740097046\n",
      "Epoch: 16 | Step:  10\n",
      "training loss is:  2.7915163040161133\n",
      "Epoch: 16 | Step:  11\n",
      "training loss is:  2.343139171600342\n",
      "Epoch: 16 | Step:  12\n",
      "training loss is:  2.1024575233459473\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5821096897125244\n",
      "MSE is: 5.011368751525879\n",
      "\n",
      "\n",
      "Epoch: 17 | Step:  0\n",
      "training loss is:  2.4644229412078857\n",
      "Epoch: 17 | Step:  1\n",
      "training loss is:  2.770698070526123\n",
      "Epoch: 17 | Step:  2\n",
      "training loss is:  2.5853114128112793\n",
      "Epoch: 17 | Step:  3\n",
      "training loss is:  2.109612226486206\n",
      "Epoch: 17 | Step:  4\n",
      "training loss is:  1.9510924816131592\n",
      "Epoch: 17 | Step:  5\n",
      "training loss is:  2.373561382293701\n",
      "Epoch: 17 | Step:  6\n",
      "training loss is:  2.562516927719116\n",
      "Epoch: 17 | Step:  7\n",
      "training loss is:  2.35799241065979\n",
      "Epoch: 17 | Step:  8\n",
      "training loss is:  2.498948574066162\n",
      "Epoch: 17 | Step:  9\n",
      "training loss is:  2.490233898162842\n",
      "Epoch: 17 | Step:  10\n",
      "training loss is:  2.595259428024292\n",
      "Epoch: 17 | Step:  11\n",
      "training loss is:  3.000079870223999\n",
      "Epoch: 17 | Step:  12\n",
      "training loss is:  2.0658984184265137\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.613921880722046\n",
      "MSE is: 5.164902687072754\n",
      "\n",
      "\n",
      "Epoch: 18 | Step:  0\n",
      "training loss is:  3.0144524574279785\n",
      "Epoch: 18 | Step:  1\n",
      "training loss is:  2.8620736598968506\n",
      "Epoch: 18 | Step:  2\n",
      "training loss is:  2.9713029861450195\n",
      "Epoch: 18 | Step:  3\n",
      "training loss is:  2.1233489513397217\n",
      "Epoch: 18 | Step:  4\n",
      "training loss is:  2.3328020572662354\n",
      "Epoch: 18 | Step:  5\n",
      "training loss is:  2.4584848880767822\n",
      "Epoch: 18 | Step:  6\n",
      "training loss is:  2.6842477321624756\n",
      "Epoch: 18 | Step:  7\n",
      "training loss is:  2.280278205871582\n",
      "Epoch: 18 | Step:  8\n",
      "training loss is:  2.0822410583496094\n",
      "Epoch: 18 | Step:  9\n",
      "training loss is:  2.4092230796813965\n",
      "Epoch: 18 | Step:  10\n",
      "training loss is:  2.36177134513855\n",
      "Epoch: 18 | Step:  11\n",
      "training loss is:  2.29524827003479\n",
      "Epoch: 18 | Step:  12\n",
      "training loss is:  2.8954784870147705\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5281435251235962\n",
      "MSE is: 4.7291579246521\n",
      "\n",
      "\n",
      "Epoch: 19 | Step:  0\n",
      "training loss is:  2.205592155456543\n",
      "Epoch: 19 | Step:  1\n",
      "training loss is:  2.1895883083343506\n",
      "Epoch: 19 | Step:  2\n",
      "training loss is:  2.6382946968078613\n",
      "Epoch: 19 | Step:  3\n",
      "training loss is:  2.2597081661224365\n",
      "Epoch: 19 | Step:  4\n",
      "training loss is:  2.585197687149048\n",
      "Epoch: 19 | Step:  5\n",
      "training loss is:  2.237260580062866\n",
      "Epoch: 19 | Step:  6\n",
      "training loss is:  2.4444844722747803\n",
      "Epoch: 19 | Step:  7\n",
      "training loss is:  2.304939031600952\n",
      "Epoch: 19 | Step:  8\n",
      "training loss is:  2.5927252769470215\n",
      "Epoch: 19 | Step:  9\n",
      "training loss is:  2.3785240650177\n",
      "Epoch: 19 | Step:  10\n",
      "training loss is:  2.4587514400482178\n",
      "Epoch: 19 | Step:  11\n",
      "training loss is:  2.3066937923431396\n",
      "Epoch: 19 | Step:  12\n",
      "training loss is:  2.7621846199035645\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5262137651443481\n",
      "MSE is: 4.688939094543457\n",
      "\n",
      "\n",
      "Epoch: 20 | Step:  0\n",
      "training loss is:  2.4956490993499756\n",
      "Epoch: 20 | Step:  1\n",
      "training loss is:  2.0679233074188232\n",
      "Epoch: 20 | Step:  2\n",
      "training loss is:  2.6184134483337402\n",
      "Epoch: 20 | Step:  3\n",
      "training loss is:  2.568146228790283\n",
      "Epoch: 20 | Step:  4\n",
      "training loss is:  2.4243056774139404\n",
      "Epoch: 20 | Step:  5\n",
      "training loss is:  2.2085535526275635\n",
      "Epoch: 20 | Step:  6\n",
      "training loss is:  2.3405518531799316\n",
      "Epoch: 20 | Step:  7\n",
      "training loss is:  2.2941343784332275\n",
      "Epoch: 20 | Step:  8\n",
      "training loss is:  2.7914633750915527\n",
      "Epoch: 20 | Step:  9\n",
      "training loss is:  2.4194252490997314\n",
      "Epoch: 20 | Step:  10\n",
      "training loss is:  2.4570038318634033\n",
      "Epoch: 20 | Step:  11\n",
      "training loss is:  2.6599831581115723\n",
      "Epoch: 20 | Step:  12\n",
      "training loss is:  2.5166773796081543\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4857404232025146\n",
      "MSE is: 4.4581499099731445\n",
      "\n",
      "\n",
      "Epoch: 21 | Step:  0\n",
      "training loss is:  2.517301559448242\n",
      "Epoch: 21 | Step:  1\n",
      "training loss is:  2.4824130535125732\n",
      "Epoch: 21 | Step:  2\n",
      "training loss is:  2.3440380096435547\n",
      "Epoch: 21 | Step:  3\n",
      "training loss is:  2.364084482192993\n",
      "Epoch: 21 | Step:  4\n",
      "training loss is:  2.4780638217926025\n",
      "Epoch: 21 | Step:  5\n",
      "training loss is:  2.141339063644409\n",
      "Epoch: 21 | Step:  6\n",
      "training loss is:  2.0412557125091553\n",
      "Epoch: 21 | Step:  7\n",
      "training loss is:  2.527059555053711\n",
      "Epoch: 21 | Step:  8\n",
      "training loss is:  2.544747829437256\n",
      "Epoch: 21 | Step:  9\n",
      "training loss is:  2.1996376514434814\n",
      "Epoch: 21 | Step:  10\n",
      "training loss is:  2.8962056636810303\n",
      "Epoch: 21 | Step:  11\n",
      "training loss is:  3.042177200317383\n",
      "Epoch: 21 | Step:  12\n",
      "training loss is:  2.920147657394409\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.475332498550415\n",
      "MSE is: 4.46732759475708\n",
      "\n",
      "\n",
      "Epoch: 22 | Step:  0\n",
      "training loss is:  2.329949140548706\n",
      "Epoch: 22 | Step:  1\n",
      "training loss is:  2.605893135070801\n",
      "Epoch: 22 | Step:  2\n",
      "training loss is:  2.855491876602173\n",
      "Epoch: 22 | Step:  3\n",
      "training loss is:  2.2368571758270264\n",
      "Epoch: 22 | Step:  4\n",
      "training loss is:  2.2910499572753906\n",
      "Epoch: 22 | Step:  5\n",
      "training loss is:  2.6073858737945557\n",
      "Epoch: 22 | Step:  6\n",
      "training loss is:  2.609132766723633\n",
      "Epoch: 22 | Step:  7\n",
      "training loss is:  2.49495530128479\n",
      "Epoch: 22 | Step:  8\n",
      "training loss is:  2.3306374549865723\n",
      "Epoch: 22 | Step:  9\n",
      "training loss is:  2.2347986698150635\n",
      "Epoch: 22 | Step:  10\n",
      "training loss is:  2.1427767276763916\n",
      "Epoch: 22 | Step:  11\n",
      "training loss is:  2.3760669231414795\n",
      "Epoch: 22 | Step:  12\n",
      "training loss is:  2.4745583534240723\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5586873292922974\n",
      "MSE is: 4.866571426391602\n",
      "\n",
      "\n",
      "Epoch: 23 | Step:  0\n",
      "training loss is:  2.3617184162139893\n",
      "Epoch: 23 | Step:  1\n",
      "training loss is:  2.18148136138916\n",
      "Epoch: 23 | Step:  2\n",
      "training loss is:  2.3632497787475586\n",
      "Epoch: 23 | Step:  3\n",
      "training loss is:  2.518409252166748\n",
      "Epoch: 23 | Step:  4\n",
      "training loss is:  2.942124843597412\n",
      "Epoch: 23 | Step:  5\n",
      "training loss is:  2.677487850189209\n",
      "Epoch: 23 | Step:  6\n",
      "training loss is:  2.66499924659729\n",
      "Epoch: 23 | Step:  7\n",
      "training loss is:  2.230137825012207\n",
      "Epoch: 23 | Step:  8\n",
      "training loss is:  2.645646572113037\n",
      "Epoch: 23 | Step:  9\n",
      "training loss is:  2.5791263580322266\n",
      "Epoch: 23 | Step:  10\n",
      "training loss is:  2.2946839332580566\n",
      "Epoch: 23 | Step:  11\n",
      "training loss is:  2.7806382179260254\n",
      "Epoch: 23 | Step:  12\n",
      "training loss is:  3.1055045127868652\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4397625923156738\n",
      "MSE is: 4.299417972564697\n",
      "\n",
      "\n",
      "Epoch: 24 | Step:  0\n",
      "training loss is:  2.808243751525879\n",
      "Epoch: 24 | Step:  1\n",
      "training loss is:  2.5351521968841553\n",
      "Epoch: 24 | Step:  2\n",
      "training loss is:  2.3498873710632324\n",
      "Epoch: 24 | Step:  3\n",
      "training loss is:  2.3469765186309814\n",
      "Epoch: 24 | Step:  4\n",
      "training loss is:  2.420940399169922\n",
      "Epoch: 24 | Step:  5\n",
      "training loss is:  2.2157840728759766\n",
      "Epoch: 24 | Step:  6\n",
      "training loss is:  2.3376786708831787\n",
      "Epoch: 24 | Step:  7\n",
      "training loss is:  2.1876890659332275\n",
      "Epoch: 24 | Step:  8\n",
      "training loss is:  2.46663236618042\n",
      "Epoch: 24 | Step:  9\n",
      "training loss is:  2.3935317993164062\n",
      "Epoch: 24 | Step:  10\n",
      "training loss is:  2.3570759296417236\n",
      "Epoch: 24 | Step:  11\n",
      "training loss is:  2.3875701427459717\n",
      "Epoch: 24 | Step:  12\n",
      "training loss is:  2.5267083644866943\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5062565803527832\n",
      "MSE is: 4.598970890045166\n",
      "\n",
      "\n",
      "Epoch: 25 | Step:  0\n",
      "training loss is:  2.518317461013794\n",
      "Epoch: 25 | Step:  1\n",
      "training loss is:  2.6373724937438965\n",
      "Epoch: 25 | Step:  2\n",
      "training loss is:  2.411038398742676\n",
      "Epoch: 25 | Step:  3\n",
      "training loss is:  2.5907459259033203\n",
      "Epoch: 25 | Step:  4\n",
      "training loss is:  2.902604818344116\n",
      "Epoch: 25 | Step:  5\n",
      "training loss is:  2.9740941524505615\n",
      "Epoch: 25 | Step:  6\n",
      "training loss is:  2.3808751106262207\n",
      "Epoch: 25 | Step:  7\n",
      "training loss is:  2.438772678375244\n",
      "Epoch: 25 | Step:  8\n",
      "training loss is:  2.4743106365203857\n",
      "Epoch: 25 | Step:  9\n",
      "training loss is:  2.1255528926849365\n",
      "Epoch: 25 | Step:  10\n",
      "training loss is:  2.3365252017974854\n",
      "Epoch: 25 | Step:  11\n",
      "training loss is:  2.340566635131836\n",
      "Epoch: 25 | Step:  12\n",
      "training loss is:  2.630373001098633\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5250232219696045\n",
      "MSE is: 4.697976589202881\n",
      "\n",
      "\n",
      "Epoch: 26 | Step:  0\n",
      "training loss is:  2.87056827545166\n",
      "Epoch: 26 | Step:  1\n",
      "training loss is:  2.5802206993103027\n",
      "Epoch: 26 | Step:  2\n",
      "training loss is:  2.5572328567504883\n",
      "Epoch: 26 | Step:  3\n",
      "training loss is:  2.0303118228912354\n",
      "Epoch: 26 | Step:  4\n",
      "training loss is:  2.5723049640655518\n",
      "Epoch: 26 | Step:  5\n",
      "training loss is:  3.047133684158325\n",
      "Epoch: 26 | Step:  6\n",
      "training loss is:  3.1955721378326416\n",
      "Epoch: 26 | Step:  7\n",
      "training loss is:  3.1193978786468506\n",
      "Epoch: 26 | Step:  8\n",
      "training loss is:  2.4394125938415527\n",
      "Epoch: 26 | Step:  9\n",
      "training loss is:  2.796316385269165\n",
      "Epoch: 26 | Step:  10\n",
      "training loss is:  2.626084089279175\n",
      "Epoch: 26 | Step:  11\n",
      "training loss is:  2.0403716564178467\n",
      "Epoch: 26 | Step:  12\n",
      "training loss is:  1.9811961650848389\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4996166229248047\n",
      "MSE is: 4.60152530670166\n",
      "\n",
      "\n",
      "Epoch: 27 | Step:  0\n",
      "training loss is:  2.165684461593628\n",
      "Epoch: 27 | Step:  1\n",
      "training loss is:  2.239452838897705\n",
      "Epoch: 27 | Step:  2\n",
      "training loss is:  2.3685507774353027\n",
      "Epoch: 27 | Step:  3\n",
      "training loss is:  2.575615167617798\n",
      "Epoch: 27 | Step:  4\n",
      "training loss is:  2.5723044872283936\n",
      "Epoch: 27 | Step:  5\n",
      "training loss is:  2.6498680114746094\n",
      "Epoch: 27 | Step:  6\n",
      "training loss is:  2.4805896282196045\n",
      "Epoch: 27 | Step:  7\n",
      "training loss is:  2.7604706287384033\n",
      "Epoch: 27 | Step:  8\n",
      "training loss is:  2.3365166187286377\n",
      "Epoch: 27 | Step:  9\n",
      "training loss is:  2.467345714569092\n",
      "Epoch: 27 | Step:  10\n",
      "training loss is:  2.255293130874634\n",
      "Epoch: 27 | Step:  11\n",
      "training loss is:  2.3248393535614014\n",
      "Epoch: 27 | Step:  12\n",
      "training loss is:  2.6260499954223633\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5473843812942505\n",
      "MSE is: 4.809370517730713\n",
      "\n",
      "\n",
      "Epoch: 28 | Step:  0\n",
      "training loss is:  2.3799290657043457\n",
      "Epoch: 28 | Step:  1\n",
      "training loss is:  2.5776548385620117\n",
      "Epoch: 28 | Step:  2\n",
      "training loss is:  2.6079697608947754\n",
      "Epoch: 28 | Step:  3\n",
      "training loss is:  2.857405424118042\n",
      "Epoch: 28 | Step:  4\n",
      "training loss is:  2.5017101764678955\n",
      "Epoch: 28 | Step:  5\n",
      "training loss is:  2.439764976501465\n",
      "Epoch: 28 | Step:  6\n",
      "training loss is:  2.138916254043579\n",
      "Epoch: 28 | Step:  7\n",
      "training loss is:  2.356447458267212\n",
      "Epoch: 28 | Step:  8\n",
      "training loss is:  2.343815326690674\n",
      "Epoch: 28 | Step:  9\n",
      "training loss is:  2.6985034942626953\n",
      "Epoch: 28 | Step:  10\n",
      "training loss is:  2.429269552230835\n",
      "Epoch: 28 | Step:  11\n",
      "training loss is:  2.4278945922851562\n",
      "Epoch: 28 | Step:  12\n",
      "training loss is:  2.5603249073028564\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4984121322631836\n",
      "MSE is: 4.5946455001831055\n",
      "\n",
      "\n",
      "Epoch: 29 | Step:  0\n",
      "training loss is:  2.5971693992614746\n",
      "Epoch: 29 | Step:  1\n",
      "training loss is:  2.239093780517578\n",
      "Epoch: 29 | Step:  2\n",
      "training loss is:  2.587317943572998\n",
      "Epoch: 29 | Step:  3\n",
      "training loss is:  2.452709674835205\n",
      "Epoch: 29 | Step:  4\n",
      "training loss is:  2.100202798843384\n",
      "Epoch: 29 | Step:  5\n",
      "training loss is:  2.4286904335021973\n",
      "Epoch: 29 | Step:  6\n",
      "training loss is:  2.245629072189331\n",
      "Epoch: 29 | Step:  7\n",
      "training loss is:  2.3592405319213867\n",
      "Epoch: 29 | Step:  8\n",
      "training loss is:  2.498863458633423\n",
      "Epoch: 29 | Step:  9\n",
      "training loss is:  2.434741258621216\n",
      "Epoch: 29 | Step:  10\n",
      "training loss is:  2.2000811100006104\n",
      "Epoch: 29 | Step:  11\n",
      "training loss is:  2.6085011959075928\n",
      "Epoch: 29 | Step:  12\n",
      "training loss is:  2.6042473316192627\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5417274236679077\n",
      "MSE is: 4.8085222244262695\n",
      "\n",
      "\n",
      "Epoch: 30 | Step:  0\n",
      "training loss is:  2.0732109546661377\n",
      "Epoch: 30 | Step:  1\n",
      "training loss is:  2.635965585708618\n",
      "Epoch: 30 | Step:  2\n",
      "training loss is:  2.783946990966797\n",
      "Epoch: 30 | Step:  3\n",
      "training loss is:  2.619044303894043\n",
      "Epoch: 30 | Step:  4\n",
      "training loss is:  2.202507734298706\n",
      "Epoch: 30 | Step:  5\n",
      "training loss is:  2.7101316452026367\n",
      "Epoch: 30 | Step:  6\n",
      "training loss is:  2.6800129413604736\n",
      "Epoch: 30 | Step:  7\n",
      "training loss is:  2.556645154953003\n",
      "Epoch: 30 | Step:  8\n",
      "training loss is:  3.0187783241271973\n",
      "Epoch: 30 | Step:  9\n",
      "training loss is:  2.266829490661621\n",
      "Epoch: 30 | Step:  10\n",
      "training loss is:  2.365755319595337\n",
      "Epoch: 30 | Step:  11\n",
      "training loss is:  2.8864810466766357\n",
      "Epoch: 30 | Step:  12\n",
      "training loss is:  2.629483938217163\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5003662109375\n",
      "MSE is: 4.608220100402832\n",
      "\n",
      "\n",
      "Epoch: 31 | Step:  0\n",
      "training loss is:  2.2016351222991943\n",
      "Epoch: 31 | Step:  1\n",
      "training loss is:  2.352250576019287\n",
      "Epoch: 31 | Step:  2\n",
      "training loss is:  2.1120338439941406\n",
      "Epoch: 31 | Step:  3\n",
      "training loss is:  2.0857934951782227\n",
      "Epoch: 31 | Step:  4\n",
      "training loss is:  2.6590516567230225\n",
      "Epoch: 31 | Step:  5\n",
      "training loss is:  2.434248447418213\n",
      "Epoch: 31 | Step:  6\n",
      "training loss is:  2.5494468212127686\n",
      "Epoch: 31 | Step:  7\n",
      "training loss is:  2.646620988845825\n",
      "Epoch: 31 | Step:  8\n",
      "training loss is:  2.5085058212280273\n",
      "Epoch: 31 | Step:  9\n",
      "training loss is:  2.794434070587158\n",
      "Epoch: 31 | Step:  10\n",
      "training loss is:  2.9588844776153564\n",
      "Epoch: 31 | Step:  11\n",
      "training loss is:  2.5244503021240234\n",
      "Epoch: 31 | Step:  12\n",
      "training loss is:  2.638803005218506\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5085898637771606\n",
      "MSE is: 4.623706817626953\n",
      "\n",
      "\n",
      "Epoch: 32 | Step:  0\n",
      "training loss is:  2.492218017578125\n",
      "Epoch: 32 | Step:  1\n",
      "training loss is:  2.5716681480407715\n",
      "Epoch: 32 | Step:  2\n",
      "training loss is:  2.207395553588867\n",
      "Epoch: 32 | Step:  3\n",
      "training loss is:  2.370419979095459\n",
      "Epoch: 32 | Step:  4\n",
      "training loss is:  2.4711191654205322\n",
      "Epoch: 32 | Step:  5\n",
      "training loss is:  2.479175329208374\n",
      "Epoch: 32 | Step:  6\n",
      "training loss is:  2.4293415546417236\n",
      "Epoch: 32 | Step:  7\n",
      "training loss is:  2.449765682220459\n",
      "Epoch: 32 | Step:  8\n",
      "training loss is:  2.0296216011047363\n",
      "Epoch: 32 | Step:  9\n",
      "training loss is:  2.3564937114715576\n",
      "Epoch: 32 | Step:  10\n",
      "training loss is:  2.7880148887634277\n",
      "Epoch: 32 | Step:  11\n",
      "training loss is:  2.4259703159332275\n",
      "Epoch: 32 | Step:  12\n",
      "training loss is:  2.3804738521575928\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5312793254852295\n",
      "MSE is: 4.764950752258301\n",
      "\n",
      "\n",
      "Epoch: 33 | Step:  0\n",
      "training loss is:  2.2584547996520996\n",
      "Epoch: 33 | Step:  1\n",
      "training loss is:  2.1261045932769775\n",
      "Epoch: 33 | Step:  2\n",
      "training loss is:  2.140491485595703\n",
      "Epoch: 33 | Step:  3\n",
      "training loss is:  2.3804757595062256\n",
      "Epoch: 33 | Step:  4\n",
      "training loss is:  2.597306489944458\n",
      "Epoch: 33 | Step:  5\n",
      "training loss is:  2.472926378250122\n",
      "Epoch: 33 | Step:  6\n",
      "training loss is:  2.3078975677490234\n",
      "Epoch: 33 | Step:  7\n",
      "training loss is:  2.34159517288208\n",
      "Epoch: 33 | Step:  8\n",
      "training loss is:  2.2393763065338135\n",
      "Epoch: 33 | Step:  9\n",
      "training loss is:  2.5983896255493164\n",
      "Epoch: 33 | Step:  10\n",
      "training loss is:  2.3988680839538574\n",
      "Epoch: 33 | Step:  11\n",
      "training loss is:  2.472702741622925\n",
      "Epoch: 33 | Step:  12\n",
      "training loss is:  2.810683250427246\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.542864203453064\n",
      "MSE is: 4.8294219970703125\n",
      "\n",
      "\n",
      "Epoch: 34 | Step:  0\n",
      "training loss is:  2.4946401119232178\n",
      "Epoch: 34 | Step:  1\n",
      "training loss is:  2.456233263015747\n",
      "Epoch: 34 | Step:  2\n",
      "training loss is:  2.565138816833496\n",
      "Epoch: 34 | Step:  3\n",
      "training loss is:  3.2163381576538086\n",
      "Epoch: 34 | Step:  4\n",
      "training loss is:  3.140700340270996\n",
      "Epoch: 34 | Step:  5\n",
      "training loss is:  2.23929500579834\n",
      "Epoch: 34 | Step:  6\n",
      "training loss is:  2.2783801555633545\n",
      "Epoch: 34 | Step:  7\n",
      "training loss is:  2.3338351249694824\n",
      "Epoch: 34 | Step:  8\n",
      "training loss is:  2.0667433738708496\n",
      "Epoch: 34 | Step:  9\n",
      "training loss is:  1.9969701766967773\n",
      "Epoch: 34 | Step:  10\n",
      "training loss is:  2.507427215576172\n",
      "Epoch: 34 | Step:  11\n",
      "training loss is:  2.4555182456970215\n",
      "Epoch: 34 | Step:  12\n",
      "training loss is:  2.530256509780884\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5221226215362549\n",
      "MSE is: 4.714057922363281\n",
      "\n",
      "\n",
      "Epoch: 35 | Step:  0\n",
      "training loss is:  2.226759433746338\n",
      "Epoch: 35 | Step:  1\n",
      "training loss is:  2.3939836025238037\n",
      "Epoch: 35 | Step:  2\n",
      "training loss is:  2.3090152740478516\n",
      "Epoch: 35 | Step:  3\n",
      "training loss is:  2.5092790126800537\n",
      "Epoch: 35 | Step:  4\n",
      "training loss is:  2.305753231048584\n",
      "Epoch: 35 | Step:  5\n",
      "training loss is:  2.6031413078308105\n",
      "Epoch: 35 | Step:  6\n",
      "training loss is:  2.5312557220458984\n",
      "Epoch: 35 | Step:  7\n",
      "training loss is:  2.182621955871582\n",
      "Epoch: 35 | Step:  8\n",
      "training loss is:  2.4048192501068115\n",
      "Epoch: 35 | Step:  9\n",
      "training loss is:  2.306943416595459\n",
      "Epoch: 35 | Step:  10\n",
      "training loss is:  2.370448112487793\n",
      "Epoch: 35 | Step:  11\n",
      "training loss is:  2.975329637527466\n",
      "Epoch: 35 | Step:  12\n",
      "training loss is:  3.2774744033813477\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.662933349609375\n",
      "MSE is: 5.454841136932373\n",
      "\n",
      "\n",
      "Epoch: 36 | Step:  0\n",
      "training loss is:  2.521562099456787\n",
      "Epoch: 36 | Step:  1\n",
      "training loss is:  2.993457078933716\n",
      "Epoch: 36 | Step:  2\n",
      "training loss is:  2.4834542274475098\n",
      "Epoch: 36 | Step:  3\n",
      "training loss is:  2.6897809505462646\n",
      "Epoch: 36 | Step:  4\n",
      "training loss is:  2.366427421569824\n",
      "Epoch: 36 | Step:  5\n",
      "training loss is:  2.4279873371124268\n",
      "Epoch: 36 | Step:  6\n",
      "training loss is:  3.0651161670684814\n",
      "Epoch: 36 | Step:  7\n",
      "training loss is:  2.7318427562713623\n",
      "Epoch: 36 | Step:  8\n",
      "training loss is:  2.341789960861206\n",
      "Epoch: 36 | Step:  9\n",
      "training loss is:  2.4278528690338135\n",
      "Epoch: 36 | Step:  10\n",
      "training loss is:  2.3543503284454346\n",
      "Epoch: 36 | Step:  11\n",
      "training loss is:  2.266432523727417\n",
      "Epoch: 36 | Step:  12\n",
      "training loss is:  2.437490940093994\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4889131784439087\n",
      "MSE is: 4.56026029586792\n",
      "\n",
      "\n",
      "Epoch: 37 | Step:  0\n",
      "training loss is:  1.9149553775787354\n",
      "Epoch: 37 | Step:  1\n",
      "training loss is:  2.810330390930176\n",
      "Epoch: 37 | Step:  2\n",
      "training loss is:  2.601130723953247\n",
      "Epoch: 37 | Step:  3\n",
      "training loss is:  2.6682682037353516\n",
      "Epoch: 37 | Step:  4\n",
      "training loss is:  2.634561777114868\n",
      "Epoch: 37 | Step:  5\n",
      "training loss is:  2.561047077178955\n",
      "Epoch: 37 | Step:  6\n",
      "training loss is:  2.3819448947906494\n",
      "Epoch: 37 | Step:  7\n",
      "training loss is:  2.2989120483398438\n",
      "Epoch: 37 | Step:  8\n",
      "training loss is:  2.6395134925842285\n",
      "Epoch: 37 | Step:  9\n",
      "training loss is:  2.779416561126709\n",
      "Epoch: 37 | Step:  10\n",
      "training loss is:  2.4839067459106445\n",
      "Epoch: 37 | Step:  11\n",
      "training loss is:  2.27264142036438\n",
      "Epoch: 37 | Step:  12\n",
      "training loss is:  2.264381170272827\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5201834440231323\n",
      "MSE is: 4.701286792755127\n",
      "\n",
      "\n",
      "Epoch: 38 | Step:  0\n",
      "training loss is:  2.749748706817627\n",
      "Epoch: 38 | Step:  1\n",
      "training loss is:  2.4906997680664062\n",
      "Epoch: 38 | Step:  2\n",
      "training loss is:  2.276655912399292\n",
      "Epoch: 38 | Step:  3\n",
      "training loss is:  2.405944585800171\n",
      "Epoch: 38 | Step:  4\n",
      "training loss is:  2.6060383319854736\n",
      "Epoch: 38 | Step:  5\n",
      "training loss is:  2.2345404624938965\n",
      "Epoch: 38 | Step:  6\n",
      "training loss is:  2.563962459564209\n",
      "Epoch: 38 | Step:  7\n",
      "training loss is:  2.1937851905822754\n",
      "Epoch: 38 | Step:  8\n",
      "training loss is:  2.496119737625122\n",
      "Epoch: 38 | Step:  9\n",
      "training loss is:  2.855414628982544\n",
      "Epoch: 38 | Step:  10\n",
      "training loss is:  2.104997158050537\n",
      "Epoch: 38 | Step:  11\n",
      "training loss is:  2.5008761882781982\n",
      "Epoch: 38 | Step:  12\n",
      "training loss is:  2.6007330417633057\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.6260589361190796\n",
      "MSE is: 5.287622451782227\n",
      "\n",
      "\n",
      "Epoch: 39 | Step:  0\n",
      "training loss is:  2.2891294956207275\n",
      "Epoch: 39 | Step:  1\n",
      "training loss is:  2.737381935119629\n",
      "Epoch: 39 | Step:  2\n",
      "training loss is:  2.576298475265503\n",
      "Epoch: 39 | Step:  3\n",
      "training loss is:  2.0517818927764893\n",
      "Epoch: 39 | Step:  4\n",
      "training loss is:  2.6110692024230957\n",
      "Epoch: 39 | Step:  5\n",
      "training loss is:  2.4957873821258545\n",
      "Epoch: 39 | Step:  6\n",
      "training loss is:  2.4029316902160645\n",
      "Epoch: 39 | Step:  7\n",
      "training loss is:  2.283332586288452\n",
      "Epoch: 39 | Step:  8\n",
      "training loss is:  2.8129515647888184\n",
      "Epoch: 39 | Step:  9\n",
      "training loss is:  2.427483081817627\n",
      "Epoch: 39 | Step:  10\n",
      "training loss is:  2.4598631858825684\n",
      "Epoch: 39 | Step:  11\n",
      "training loss is:  2.8686599731445312\n",
      "Epoch: 39 | Step:  12\n",
      "training loss is:  2.375777006149292\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4428340196609497\n",
      "MSE is: 4.321502685546875\n",
      "\n",
      "\n",
      "Epoch: 40 | Step:  0\n",
      "training loss is:  2.942936658859253\n",
      "Epoch: 40 | Step:  1\n",
      "training loss is:  2.1696698665618896\n",
      "Epoch: 40 | Step:  2\n",
      "training loss is:  2.657740592956543\n",
      "Epoch: 40 | Step:  3\n",
      "training loss is:  2.355257272720337\n",
      "Epoch: 40 | Step:  4\n",
      "training loss is:  2.2704501152038574\n",
      "Epoch: 40 | Step:  5\n",
      "training loss is:  2.3582377433776855\n",
      "Epoch: 40 | Step:  6\n",
      "training loss is:  2.313103199005127\n",
      "Epoch: 40 | Step:  7\n",
      "training loss is:  2.429098129272461\n",
      "Epoch: 40 | Step:  8\n",
      "training loss is:  2.746354818344116\n",
      "Epoch: 40 | Step:  9\n",
      "training loss is:  2.526780366897583\n",
      "Epoch: 40 | Step:  10\n",
      "training loss is:  2.856600522994995\n",
      "Epoch: 40 | Step:  11\n",
      "training loss is:  2.3647429943084717\n",
      "Epoch: 40 | Step:  12\n",
      "training loss is:  2.3651554584503174\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5122647285461426\n",
      "MSE is: 4.621916770935059\n",
      "\n",
      "\n",
      "Epoch: 41 | Step:  0\n",
      "training loss is:  2.216419219970703\n",
      "Epoch: 41 | Step:  1\n",
      "training loss is:  2.7078466415405273\n",
      "Epoch: 41 | Step:  2\n",
      "training loss is:  2.8200924396514893\n",
      "Epoch: 41 | Step:  3\n",
      "training loss is:  2.300187349319458\n",
      "Epoch: 41 | Step:  4\n",
      "training loss is:  2.64206862449646\n",
      "Epoch: 41 | Step:  5\n",
      "training loss is:  2.392190933227539\n",
      "Epoch: 41 | Step:  6\n",
      "training loss is:  2.241257429122925\n",
      "Epoch: 41 | Step:  7\n",
      "training loss is:  2.494616985321045\n",
      "Epoch: 41 | Step:  8\n",
      "training loss is:  2.575349807739258\n",
      "Epoch: 41 | Step:  9\n",
      "training loss is:  2.848489284515381\n",
      "Epoch: 41 | Step:  10\n",
      "training loss is:  2.759908437728882\n",
      "Epoch: 41 | Step:  11\n",
      "training loss is:  2.430586099624634\n",
      "Epoch: 41 | Step:  12\n",
      "training loss is:  2.6040687561035156\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5269232988357544\n",
      "MSE is: 4.747411727905273\n",
      "\n",
      "\n",
      "Epoch: 42 | Step:  0\n",
      "training loss is:  2.21693754196167\n",
      "Epoch: 42 | Step:  1\n",
      "training loss is:  2.5554327964782715\n",
      "Epoch: 42 | Step:  2\n",
      "training loss is:  2.602250576019287\n",
      "Epoch: 42 | Step:  3\n",
      "training loss is:  2.5412094593048096\n",
      "Epoch: 42 | Step:  4\n",
      "training loss is:  2.3690192699432373\n",
      "Epoch: 42 | Step:  5\n",
      "training loss is:  2.5771520137786865\n",
      "Epoch: 42 | Step:  6\n",
      "training loss is:  2.4181907176971436\n",
      "Epoch: 42 | Step:  7\n",
      "training loss is:  2.671797752380371\n",
      "Epoch: 42 | Step:  8\n",
      "training loss is:  2.3156871795654297\n",
      "Epoch: 42 | Step:  9\n",
      "training loss is:  2.442650079727173\n",
      "Epoch: 42 | Step:  10\n",
      "training loss is:  2.0729072093963623\n",
      "Epoch: 42 | Step:  11\n",
      "training loss is:  2.553659677505493\n",
      "Epoch: 42 | Step:  12\n",
      "training loss is:  2.7684168815612793\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.628369927406311\n",
      "MSE is: 5.258940696716309\n",
      "\n",
      "\n",
      "Epoch: 43 | Step:  0\n",
      "training loss is:  2.4840879440307617\n",
      "Epoch: 43 | Step:  1\n",
      "training loss is:  3.1646947860717773\n",
      "Epoch: 43 | Step:  2\n",
      "training loss is:  2.8343260288238525\n",
      "Epoch: 43 | Step:  3\n",
      "training loss is:  2.7685744762420654\n",
      "Epoch: 43 | Step:  4\n",
      "training loss is:  2.0773117542266846\n",
      "Epoch: 43 | Step:  5\n",
      "training loss is:  2.3116791248321533\n",
      "Epoch: 43 | Step:  6\n",
      "training loss is:  2.448665142059326\n",
      "Epoch: 43 | Step:  7\n",
      "training loss is:  2.1033291816711426\n",
      "Epoch: 43 | Step:  8\n",
      "training loss is:  2.357703924179077\n",
      "Epoch: 43 | Step:  9\n",
      "training loss is:  2.7608659267425537\n",
      "Epoch: 43 | Step:  10\n",
      "training loss is:  2.697211503982544\n",
      "Epoch: 43 | Step:  11\n",
      "training loss is:  2.715575695037842\n",
      "Epoch: 43 | Step:  12\n",
      "training loss is:  2.7370922565460205\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4585399627685547\n",
      "MSE is: 4.385852813720703\n",
      "\n",
      "\n",
      "Epoch: 44 | Step:  0\n",
      "training loss is:  2.1768300533294678\n",
      "Epoch: 44 | Step:  1\n",
      "training loss is:  2.415079355239868\n",
      "Epoch: 44 | Step:  2\n",
      "training loss is:  2.2943789958953857\n",
      "Epoch: 44 | Step:  3\n",
      "training loss is:  2.0907375812530518\n",
      "Epoch: 44 | Step:  4\n",
      "training loss is:  2.4566705226898193\n",
      "Epoch: 44 | Step:  5\n",
      "training loss is:  2.444077253341675\n",
      "Epoch: 44 | Step:  6\n",
      "training loss is:  2.449692726135254\n",
      "Epoch: 44 | Step:  7\n",
      "training loss is:  2.703155994415283\n",
      "Epoch: 44 | Step:  8\n",
      "training loss is:  2.730041742324829\n",
      "Epoch: 44 | Step:  9\n",
      "training loss is:  2.4638235569000244\n",
      "Epoch: 44 | Step:  10\n",
      "training loss is:  2.4665756225585938\n",
      "Epoch: 44 | Step:  11\n",
      "training loss is:  2.48122239112854\n",
      "Epoch: 44 | Step:  12\n",
      "training loss is:  2.2009124755859375\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5062971115112305\n",
      "MSE is: 4.6298627853393555\n",
      "\n",
      "\n",
      "Epoch: 45 | Step:  0\n",
      "training loss is:  2.4822065830230713\n",
      "Epoch: 45 | Step:  1\n",
      "training loss is:  2.478755474090576\n",
      "Epoch: 45 | Step:  2\n",
      "training loss is:  2.0899126529693604\n",
      "Epoch: 45 | Step:  3\n",
      "training loss is:  2.504779815673828\n",
      "Epoch: 45 | Step:  4\n",
      "training loss is:  1.9920039176940918\n",
      "Epoch: 45 | Step:  5\n",
      "training loss is:  2.144355535507202\n",
      "Epoch: 45 | Step:  6\n",
      "training loss is:  2.8257813453674316\n",
      "Epoch: 45 | Step:  7\n",
      "training loss is:  2.1177096366882324\n",
      "Epoch: 45 | Step:  8\n",
      "training loss is:  2.4815120697021484\n",
      "Epoch: 45 | Step:  9\n",
      "training loss is:  2.877605676651001\n",
      "Epoch: 45 | Step:  10\n",
      "training loss is:  3.065023183822632\n",
      "Epoch: 45 | Step:  11\n",
      "training loss is:  2.3109848499298096\n",
      "Epoch: 45 | Step:  12\n",
      "training loss is:  2.8469724655151367\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.467402458190918\n",
      "MSE is: 4.403326511383057\n",
      "\n",
      "\n",
      "Epoch: 46 | Step:  0\n",
      "training loss is:  2.168585777282715\n",
      "Epoch: 46 | Step:  1\n",
      "training loss is:  2.835338830947876\n",
      "Epoch: 46 | Step:  2\n",
      "training loss is:  2.637115955352783\n",
      "Epoch: 46 | Step:  3\n",
      "training loss is:  2.203667640686035\n",
      "Epoch: 46 | Step:  4\n",
      "training loss is:  2.387303113937378\n",
      "Epoch: 46 | Step:  5\n",
      "training loss is:  2.602081060409546\n",
      "Epoch: 46 | Step:  6\n",
      "training loss is:  2.340588092803955\n",
      "Epoch: 46 | Step:  7\n",
      "training loss is:  2.3588032722473145\n",
      "Epoch: 46 | Step:  8\n",
      "training loss is:  2.396623373031616\n",
      "Epoch: 46 | Step:  9\n",
      "training loss is:  2.631221055984497\n",
      "Epoch: 46 | Step:  10\n",
      "training loss is:  2.421720027923584\n",
      "Epoch: 46 | Step:  11\n",
      "training loss is:  2.526059150695801\n",
      "Epoch: 46 | Step:  12\n",
      "training loss is:  2.4286117553710938\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4927200078964233\n",
      "MSE is: 4.53666877746582\n",
      "\n",
      "\n",
      "Epoch: 47 | Step:  0\n",
      "training loss is:  2.813127040863037\n",
      "Epoch: 47 | Step:  1\n",
      "training loss is:  2.0050904750823975\n",
      "Epoch: 47 | Step:  2\n",
      "training loss is:  1.8535125255584717\n",
      "Epoch: 47 | Step:  3\n",
      "training loss is:  2.428927421569824\n",
      "Epoch: 47 | Step:  4\n",
      "training loss is:  2.594649314880371\n",
      "Epoch: 47 | Step:  5\n",
      "training loss is:  2.524019479751587\n",
      "Epoch: 47 | Step:  6\n",
      "training loss is:  2.483224868774414\n",
      "Epoch: 47 | Step:  7\n",
      "training loss is:  2.636688470840454\n",
      "Epoch: 47 | Step:  8\n",
      "training loss is:  2.3433196544647217\n",
      "Epoch: 47 | Step:  9\n",
      "training loss is:  2.1956217288970947\n",
      "Epoch: 47 | Step:  10\n",
      "training loss is:  2.364370822906494\n",
      "Epoch: 47 | Step:  11\n",
      "training loss is:  2.5860233306884766\n",
      "Epoch: 47 | Step:  12\n",
      "training loss is:  2.067542552947998\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5694363117218018\n",
      "MSE is: 4.911624431610107\n",
      "\n",
      "\n",
      "Epoch: 48 | Step:  0\n",
      "training loss is:  2.172281503677368\n",
      "Epoch: 48 | Step:  1\n",
      "training loss is:  2.3272786140441895\n",
      "Epoch: 48 | Step:  2\n",
      "training loss is:  2.321805715560913\n",
      "Epoch: 48 | Step:  3\n",
      "training loss is:  2.5142860412597656\n",
      "Epoch: 48 | Step:  4\n",
      "training loss is:  2.307182788848877\n",
      "Epoch: 48 | Step:  5\n",
      "training loss is:  2.4550564289093018\n",
      "Epoch: 48 | Step:  6\n",
      "training loss is:  2.3577353954315186\n",
      "Epoch: 48 | Step:  7\n",
      "training loss is:  2.631659984588623\n",
      "Epoch: 48 | Step:  8\n",
      "training loss is:  2.649808406829834\n",
      "Epoch: 48 | Step:  9\n",
      "training loss is:  2.7620341777801514\n",
      "Epoch: 48 | Step:  10\n",
      "training loss is:  2.4615318775177\n",
      "Epoch: 48 | Step:  11\n",
      "training loss is:  2.531895160675049\n",
      "Epoch: 48 | Step:  12\n",
      "training loss is:  2.726986885070801\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.6077022552490234\n",
      "MSE is: 5.150084018707275\n",
      "\n",
      "\n",
      "Epoch: 49 | Step:  0\n",
      "training loss is:  2.6397488117218018\n",
      "Epoch: 49 | Step:  1\n",
      "training loss is:  2.690859317779541\n",
      "Epoch: 49 | Step:  2\n",
      "training loss is:  2.8617234230041504\n",
      "Epoch: 49 | Step:  3\n",
      "training loss is:  2.634096145629883\n",
      "Epoch: 49 | Step:  4\n",
      "training loss is:  2.370191812515259\n",
      "Epoch: 49 | Step:  5\n",
      "training loss is:  2.1486852169036865\n",
      "Epoch: 49 | Step:  6\n",
      "training loss is:  2.626723051071167\n",
      "Epoch: 49 | Step:  7\n",
      "training loss is:  2.253136396408081\n",
      "Epoch: 49 | Step:  8\n",
      "training loss is:  2.4826488494873047\n",
      "Epoch: 49 | Step:  9\n",
      "training loss is:  2.6017284393310547\n",
      "Epoch: 49 | Step:  10\n",
      "training loss is:  2.687713146209717\n",
      "Epoch: 49 | Step:  11\n",
      "training loss is:  2.584023952484131\n",
      "Epoch: 49 | Step:  12\n",
      "training loss is:  2.9041683673858643\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4136784076690674\n",
      "MSE is: 4.1440229415893555\n",
      "\n",
      "\n",
      "Epoch: 50 | Step:  0\n",
      "training loss is:  2.8738186359405518\n",
      "Epoch: 50 | Step:  1\n",
      "training loss is:  2.2734522819519043\n",
      "Epoch: 50 | Step:  2\n",
      "training loss is:  2.471541166305542\n",
      "Epoch: 50 | Step:  3\n",
      "training loss is:  2.210918664932251\n",
      "Epoch: 50 | Step:  4\n",
      "training loss is:  2.468618631362915\n",
      "Epoch: 50 | Step:  5\n",
      "training loss is:  2.3329384326934814\n",
      "Epoch: 50 | Step:  6\n",
      "training loss is:  2.5859639644622803\n",
      "Epoch: 50 | Step:  7\n",
      "training loss is:  2.431558609008789\n",
      "Epoch: 50 | Step:  8\n",
      "training loss is:  2.4945266246795654\n",
      "Epoch: 50 | Step:  9\n",
      "training loss is:  2.4907143115997314\n",
      "Epoch: 50 | Step:  10\n",
      "training loss is:  2.4794039726257324\n",
      "Epoch: 50 | Step:  11\n",
      "training loss is:  3.0300331115722656\n",
      "Epoch: 50 | Step:  12\n",
      "training loss is:  3.3517849445343018\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.399785041809082\n",
      "MSE is: 4.114748001098633\n",
      "\n",
      "\n",
      "Epoch: 51 | Step:  0\n",
      "training loss is:  3.0296523571014404\n",
      "Epoch: 51 | Step:  1\n",
      "training loss is:  2.291390895843506\n",
      "Epoch: 51 | Step:  2\n",
      "training loss is:  2.4439830780029297\n",
      "Epoch: 51 | Step:  3\n",
      "training loss is:  2.214738368988037\n",
      "Epoch: 51 | Step:  4\n",
      "training loss is:  2.496367931365967\n",
      "Epoch: 51 | Step:  5\n",
      "training loss is:  2.3223979473114014\n",
      "Epoch: 51 | Step:  6\n",
      "training loss is:  2.332404613494873\n",
      "Epoch: 51 | Step:  7\n",
      "training loss is:  2.7821693420410156\n",
      "Epoch: 51 | Step:  8\n",
      "training loss is:  2.5869076251983643\n",
      "Epoch: 51 | Step:  9\n",
      "training loss is:  2.306520462036133\n",
      "Epoch: 51 | Step:  10\n",
      "training loss is:  2.4765870571136475\n",
      "Epoch: 51 | Step:  11\n",
      "training loss is:  2.3599889278411865\n",
      "Epoch: 51 | Step:  12\n",
      "training loss is:  2.3530304431915283\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4999022483825684\n",
      "MSE is: 4.5714006423950195\n",
      "\n",
      "\n",
      "Epoch: 52 | Step:  0\n",
      "training loss is:  2.1332547664642334\n",
      "Epoch: 52 | Step:  1\n",
      "training loss is:  2.132842540740967\n",
      "Epoch: 52 | Step:  2\n",
      "training loss is:  2.4696459770202637\n",
      "Epoch: 52 | Step:  3\n",
      "training loss is:  2.7867422103881836\n",
      "Epoch: 52 | Step:  4\n",
      "training loss is:  2.235250949859619\n",
      "Epoch: 52 | Step:  5\n",
      "training loss is:  2.2034292221069336\n",
      "Epoch: 52 | Step:  6\n",
      "training loss is:  2.256941080093384\n",
      "Epoch: 52 | Step:  7\n",
      "training loss is:  2.3516151905059814\n",
      "Epoch: 52 | Step:  8\n",
      "training loss is:  2.45747447013855\n",
      "Epoch: 52 | Step:  9\n",
      "training loss is:  2.3214681148529053\n",
      "Epoch: 52 | Step:  10\n",
      "training loss is:  2.7380611896514893\n",
      "Epoch: 52 | Step:  11\n",
      "training loss is:  2.5872385501861572\n",
      "Epoch: 52 | Step:  12\n",
      "training loss is:  2.8048312664031982\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5069291591644287\n",
      "MSE is: 4.622623920440674\n",
      "\n",
      "\n",
      "Epoch: 53 | Step:  0\n",
      "training loss is:  2.617403745651245\n",
      "Epoch: 53 | Step:  1\n",
      "training loss is:  2.2565431594848633\n",
      "Epoch: 53 | Step:  2\n",
      "training loss is:  2.4707064628601074\n",
      "Epoch: 53 | Step:  3\n",
      "training loss is:  2.411512613296509\n",
      "Epoch: 53 | Step:  4\n",
      "training loss is:  2.4466683864593506\n",
      "Epoch: 53 | Step:  5\n",
      "training loss is:  2.677032947540283\n",
      "Epoch: 53 | Step:  6\n",
      "training loss is:  2.4829978942871094\n",
      "Epoch: 53 | Step:  7\n",
      "training loss is:  2.5150671005249023\n",
      "Epoch: 53 | Step:  8\n",
      "training loss is:  2.290195941925049\n",
      "Epoch: 53 | Step:  9\n",
      "training loss is:  2.635427951812744\n",
      "Epoch: 53 | Step:  10\n",
      "training loss is:  2.5022897720336914\n",
      "Epoch: 53 | Step:  11\n",
      "training loss is:  2.19602632522583\n",
      "Epoch: 53 | Step:  12\n",
      "training loss is:  2.7175967693328857\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5427560806274414\n",
      "MSE is: 4.808314323425293\n",
      "\n",
      "\n",
      "Epoch: 54 | Step:  0\n",
      "training loss is:  2.4864630699157715\n",
      "Epoch: 54 | Step:  1\n",
      "training loss is:  2.5264532566070557\n",
      "Epoch: 54 | Step:  2\n",
      "training loss is:  2.910036087036133\n",
      "Epoch: 54 | Step:  3\n",
      "training loss is:  2.5657427310943604\n",
      "Epoch: 54 | Step:  4\n",
      "training loss is:  2.4419193267822266\n",
      "Epoch: 54 | Step:  5\n",
      "training loss is:  2.189352035522461\n",
      "Epoch: 54 | Step:  6\n",
      "training loss is:  2.31400465965271\n",
      "Epoch: 54 | Step:  7\n",
      "training loss is:  2.4721691608428955\n",
      "Epoch: 54 | Step:  8\n",
      "training loss is:  2.828489065170288\n",
      "Epoch: 54 | Step:  9\n",
      "training loss is:  2.608314037322998\n",
      "Epoch: 54 | Step:  10\n",
      "training loss is:  2.6046454906463623\n",
      "Epoch: 54 | Step:  11\n",
      "training loss is:  2.5592360496520996\n",
      "Epoch: 54 | Step:  12\n",
      "training loss is:  2.4366581439971924\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.500733494758606\n",
      "MSE is: 4.591148376464844\n",
      "\n",
      "\n",
      "Epoch: 55 | Step:  0\n",
      "training loss is:  2.593893527984619\n",
      "Epoch: 55 | Step:  1\n",
      "training loss is:  2.4084274768829346\n",
      "Epoch: 55 | Step:  2\n",
      "training loss is:  2.3923845291137695\n",
      "Epoch: 55 | Step:  3\n",
      "training loss is:  2.6221957206726074\n",
      "Epoch: 55 | Step:  4\n",
      "training loss is:  2.6653506755828857\n",
      "Epoch: 55 | Step:  5\n",
      "training loss is:  2.542020797729492\n",
      "Epoch: 55 | Step:  6\n",
      "training loss is:  2.5334644317626953\n",
      "Epoch: 55 | Step:  7\n",
      "training loss is:  2.665588617324829\n",
      "Epoch: 55 | Step:  8\n",
      "training loss is:  2.6801462173461914\n",
      "Epoch: 55 | Step:  9\n",
      "training loss is:  2.2794346809387207\n",
      "Epoch: 55 | Step:  10\n",
      "training loss is:  2.264477491378784\n",
      "Epoch: 55 | Step:  11\n",
      "training loss is:  2.2793469429016113\n",
      "Epoch: 55 | Step:  12\n",
      "training loss is:  2.2965617179870605\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.534881353378296\n",
      "MSE is: 4.778506755828857\n",
      "\n",
      "\n",
      "Epoch: 56 | Step:  0\n",
      "training loss is:  2.101832866668701\n",
      "Epoch: 56 | Step:  1\n",
      "training loss is:  2.2218151092529297\n",
      "Epoch: 56 | Step:  2\n",
      "training loss is:  2.5900986194610596\n",
      "Epoch: 56 | Step:  3\n",
      "training loss is:  2.53749418258667\n",
      "Epoch: 56 | Step:  4\n",
      "training loss is:  2.1419246196746826\n",
      "Epoch: 56 | Step:  5\n",
      "training loss is:  2.304245710372925\n",
      "Epoch: 56 | Step:  6\n",
      "training loss is:  2.466837167739868\n",
      "Epoch: 56 | Step:  7\n",
      "training loss is:  2.8694393634796143\n",
      "Epoch: 56 | Step:  8\n",
      "training loss is:  2.469477653503418\n",
      "Epoch: 56 | Step:  9\n",
      "training loss is:  2.236192226409912\n",
      "Epoch: 56 | Step:  10\n",
      "training loss is:  2.603147029876709\n",
      "Epoch: 56 | Step:  11\n",
      "training loss is:  2.53079891204834\n",
      "Epoch: 56 | Step:  12\n",
      "training loss is:  3.0770397186279297\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.476941704750061\n",
      "MSE is: 4.428281307220459\n",
      "\n",
      "\n",
      "Epoch: 57 | Step:  0\n",
      "training loss is:  2.7385811805725098\n",
      "Epoch: 57 | Step:  1\n",
      "training loss is:  2.510798931121826\n",
      "Epoch: 57 | Step:  2\n",
      "training loss is:  2.782376527786255\n",
      "Epoch: 57 | Step:  3\n",
      "training loss is:  2.1023669242858887\n",
      "Epoch: 57 | Step:  4\n",
      "training loss is:  2.147918701171875\n",
      "Epoch: 57 | Step:  5\n",
      "training loss is:  2.7877302169799805\n",
      "Epoch: 57 | Step:  6\n",
      "training loss is:  2.5052151679992676\n",
      "Epoch: 57 | Step:  7\n",
      "training loss is:  2.588300943374634\n",
      "Epoch: 57 | Step:  8\n",
      "training loss is:  2.130398750305176\n",
      "Epoch: 57 | Step:  9\n",
      "training loss is:  2.571377992630005\n",
      "Epoch: 57 | Step:  10\n",
      "training loss is:  2.100586175918579\n",
      "Epoch: 57 | Step:  11\n",
      "training loss is:  2.733060359954834\n",
      "Epoch: 57 | Step:  12\n",
      "training loss is:  2.971522331237793\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5392940044403076\n",
      "MSE is: 4.759411811828613\n",
      "\n",
      "\n",
      "Epoch: 58 | Step:  0\n",
      "training loss is:  2.717849016189575\n",
      "Epoch: 58 | Step:  1\n",
      "training loss is:  2.4598824977874756\n",
      "Epoch: 58 | Step:  2\n",
      "training loss is:  2.6832687854766846\n",
      "Epoch: 58 | Step:  3\n",
      "training loss is:  2.622734546661377\n",
      "Epoch: 58 | Step:  4\n",
      "training loss is:  2.676623582839966\n",
      "Epoch: 58 | Step:  5\n",
      "training loss is:  2.2218916416168213\n",
      "Epoch: 58 | Step:  6\n",
      "training loss is:  2.460927963256836\n",
      "Epoch: 58 | Step:  7\n",
      "training loss is:  2.574364423751831\n",
      "Epoch: 58 | Step:  8\n",
      "training loss is:  2.643751859664917\n",
      "Epoch: 58 | Step:  9\n",
      "training loss is:  2.414970874786377\n",
      "Epoch: 58 | Step:  10\n",
      "training loss is:  2.184694528579712\n",
      "Epoch: 58 | Step:  11\n",
      "training loss is:  2.410329580307007\n",
      "Epoch: 58 | Step:  12\n",
      "training loss is:  2.6510848999023438\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5752272605895996\n",
      "MSE is: 4.964262008666992\n",
      "\n",
      "\n",
      "Epoch: 59 | Step:  0\n",
      "training loss is:  2.4786434173583984\n",
      "Epoch: 59 | Step:  1\n",
      "training loss is:  2.352280378341675\n",
      "Epoch: 59 | Step:  2\n",
      "training loss is:  2.42500376701355\n",
      "Epoch: 59 | Step:  3\n",
      "training loss is:  2.355036973953247\n",
      "Epoch: 59 | Step:  4\n",
      "training loss is:  2.189148426055908\n",
      "Epoch: 59 | Step:  5\n",
      "training loss is:  2.3620429039001465\n",
      "Epoch: 59 | Step:  6\n",
      "training loss is:  2.631028175354004\n",
      "Epoch: 59 | Step:  7\n",
      "training loss is:  2.448697805404663\n",
      "Epoch: 59 | Step:  8\n",
      "training loss is:  2.311290740966797\n",
      "Epoch: 59 | Step:  9\n",
      "training loss is:  2.317941188812256\n",
      "Epoch: 59 | Step:  10\n",
      "training loss is:  2.3994548320770264\n",
      "Epoch: 59 | Step:  11\n",
      "training loss is:  2.574955940246582\n",
      "Epoch: 59 | Step:  12\n",
      "training loss is:  2.3474411964416504\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.590340495109558\n",
      "MSE is: 5.064810752868652\n",
      "\n",
      "\n",
      "Epoch: 60 | Step:  0\n",
      "training loss is:  2.231548309326172\n",
      "Epoch: 60 | Step:  1\n",
      "training loss is:  2.418912887573242\n",
      "Epoch: 60 | Step:  2\n",
      "training loss is:  2.699411630630493\n",
      "Epoch: 60 | Step:  3\n",
      "training loss is:  2.727137804031372\n",
      "Epoch: 60 | Step:  4\n",
      "training loss is:  3.105617046356201\n",
      "Epoch: 60 | Step:  5\n",
      "training loss is:  2.818894863128662\n",
      "Epoch: 60 | Step:  6\n",
      "training loss is:  2.339106798171997\n",
      "Epoch: 60 | Step:  7\n",
      "training loss is:  2.2745561599731445\n",
      "Epoch: 60 | Step:  8\n",
      "training loss is:  2.5365240573883057\n",
      "Epoch: 60 | Step:  9\n",
      "training loss is:  2.287425994873047\n",
      "Epoch: 60 | Step:  10\n",
      "training loss is:  2.486954927444458\n",
      "Epoch: 60 | Step:  11\n",
      "training loss is:  2.819413423538208\n",
      "Epoch: 60 | Step:  12\n",
      "training loss is:  2.434741973876953\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4529272317886353\n",
      "MSE is: 4.351975440979004\n",
      "\n",
      "\n",
      "Epoch: 61 | Step:  0\n",
      "training loss is:  2.6494767665863037\n",
      "Epoch: 61 | Step:  1\n",
      "training loss is:  2.0922107696533203\n",
      "Epoch: 61 | Step:  2\n",
      "training loss is:  2.2888715267181396\n",
      "Epoch: 61 | Step:  3\n",
      "training loss is:  2.224236011505127\n",
      "Epoch: 61 | Step:  4\n",
      "training loss is:  2.330519199371338\n",
      "Epoch: 61 | Step:  5\n",
      "training loss is:  2.552396535873413\n",
      "Epoch: 61 | Step:  6\n",
      "training loss is:  2.1647791862487793\n",
      "Epoch: 61 | Step:  7\n",
      "training loss is:  2.2805089950561523\n",
      "Epoch: 61 | Step:  8\n",
      "training loss is:  3.1887097358703613\n",
      "Epoch: 61 | Step:  9\n",
      "training loss is:  2.688925266265869\n",
      "Epoch: 61 | Step:  10\n",
      "training loss is:  2.5968096256256104\n",
      "Epoch: 61 | Step:  11\n",
      "training loss is:  2.297992706298828\n",
      "Epoch: 61 | Step:  12\n",
      "training loss is:  2.772751569747925\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5153436660766602\n",
      "MSE is: 4.682327747344971\n",
      "\n",
      "\n",
      "Epoch: 62 | Step:  0\n",
      "training loss is:  2.671733856201172\n",
      "Epoch: 62 | Step:  1\n",
      "training loss is:  1.9651356935501099\n",
      "Epoch: 62 | Step:  2\n",
      "training loss is:  2.3477911949157715\n",
      "Epoch: 62 | Step:  3\n",
      "training loss is:  2.2293612957000732\n",
      "Epoch: 62 | Step:  4\n",
      "training loss is:  2.2048892974853516\n",
      "Epoch: 62 | Step:  5\n",
      "training loss is:  2.5867626667022705\n",
      "Epoch: 62 | Step:  6\n",
      "training loss is:  2.291543960571289\n",
      "Epoch: 62 | Step:  7\n",
      "training loss is:  2.3120126724243164\n",
      "Epoch: 62 | Step:  8\n",
      "training loss is:  2.4804625511169434\n",
      "Epoch: 62 | Step:  9\n",
      "training loss is:  2.314408540725708\n",
      "Epoch: 62 | Step:  10\n",
      "training loss is:  2.3140079975128174\n",
      "Epoch: 62 | Step:  11\n",
      "training loss is:  3.24845290184021\n",
      "Epoch: 62 | Step:  12\n",
      "training loss is:  2.6171023845672607\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.513476848602295\n",
      "MSE is: 4.6269941329956055\n",
      "\n",
      "\n",
      "Epoch: 63 | Step:  0\n",
      "training loss is:  3.112643241882324\n",
      "Epoch: 63 | Step:  1\n",
      "training loss is:  2.658550977706909\n",
      "Epoch: 63 | Step:  2\n",
      "training loss is:  2.5741682052612305\n",
      "Epoch: 63 | Step:  3\n",
      "training loss is:  2.4386610984802246\n",
      "Epoch: 63 | Step:  4\n",
      "training loss is:  2.3094630241394043\n",
      "Epoch: 63 | Step:  5\n",
      "training loss is:  2.2890915870666504\n",
      "Epoch: 63 | Step:  6\n",
      "training loss is:  2.092818260192871\n",
      "Epoch: 63 | Step:  7\n",
      "training loss is:  2.2962942123413086\n",
      "Epoch: 63 | Step:  8\n",
      "training loss is:  2.409069299697876\n",
      "Epoch: 63 | Step:  9\n",
      "training loss is:  2.4427199363708496\n",
      "Epoch: 63 | Step:  10\n",
      "training loss is:  2.6653831005096436\n",
      "Epoch: 63 | Step:  11\n",
      "training loss is:  2.6589365005493164\n",
      "Epoch: 63 | Step:  12\n",
      "training loss is:  2.8197896480560303\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5259029865264893\n",
      "MSE is: 4.714535713195801\n",
      "\n",
      "\n",
      "Epoch: 64 | Step:  0\n",
      "training loss is:  2.6681642532348633\n",
      "Epoch: 64 | Step:  1\n",
      "training loss is:  2.4425241947174072\n",
      "Epoch: 64 | Step:  2\n",
      "training loss is:  2.4072883129119873\n",
      "Epoch: 64 | Step:  3\n",
      "training loss is:  2.6481683254241943\n",
      "Epoch: 64 | Step:  4\n",
      "training loss is:  2.628176212310791\n",
      "Epoch: 64 | Step:  5\n",
      "training loss is:  2.934960126876831\n",
      "Epoch: 64 | Step:  6\n",
      "training loss is:  2.7587289810180664\n",
      "Epoch: 64 | Step:  7\n",
      "training loss is:  2.574660301208496\n",
      "Epoch: 64 | Step:  8\n",
      "training loss is:  2.5181844234466553\n",
      "Epoch: 64 | Step:  9\n",
      "training loss is:  2.0412299633026123\n",
      "Epoch: 64 | Step:  10\n",
      "training loss is:  2.4085538387298584\n",
      "Epoch: 64 | Step:  11\n",
      "training loss is:  2.368290424346924\n",
      "Epoch: 64 | Step:  12\n",
      "training loss is:  2.090397596359253\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.509899377822876\n",
      "MSE is: 4.639151573181152\n",
      "\n",
      "\n",
      "Epoch: 65 | Step:  0\n",
      "training loss is:  2.7061166763305664\n",
      "Epoch: 65 | Step:  1\n",
      "training loss is:  2.3450050354003906\n",
      "Epoch: 65 | Step:  2\n",
      "training loss is:  2.7974979877471924\n",
      "Epoch: 65 | Step:  3\n",
      "training loss is:  2.77666974067688\n",
      "Epoch: 65 | Step:  4\n",
      "training loss is:  2.688265800476074\n",
      "Epoch: 65 | Step:  5\n",
      "training loss is:  2.4696786403656006\n",
      "Epoch: 65 | Step:  6\n",
      "training loss is:  2.568539619445801\n",
      "Epoch: 65 | Step:  7\n",
      "training loss is:  2.3398630619049072\n",
      "Epoch: 65 | Step:  8\n",
      "training loss is:  2.346040964126587\n",
      "Epoch: 65 | Step:  9\n",
      "training loss is:  2.4998350143432617\n",
      "Epoch: 65 | Step:  10\n",
      "training loss is:  2.4430930614471436\n",
      "Epoch: 65 | Step:  11\n",
      "training loss is:  2.0698719024658203\n",
      "Epoch: 65 | Step:  12\n",
      "training loss is:  2.4942359924316406\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.64069664478302\n",
      "MSE is: 5.2621049880981445\n",
      "\n",
      "\n",
      "Epoch: 66 | Step:  0\n",
      "training loss is:  2.4906527996063232\n",
      "Epoch: 66 | Step:  1\n",
      "training loss is:  3.042025566101074\n",
      "Epoch: 66 | Step:  2\n",
      "training loss is:  2.44637131690979\n",
      "Epoch: 66 | Step:  3\n",
      "training loss is:  2.6395294666290283\n",
      "Epoch: 66 | Step:  4\n",
      "training loss is:  2.3006978034973145\n",
      "Epoch: 66 | Step:  5\n",
      "training loss is:  2.1320672035217285\n",
      "Epoch: 66 | Step:  6\n",
      "training loss is:  2.866149663925171\n",
      "Epoch: 66 | Step:  7\n",
      "training loss is:  2.5601649284362793\n",
      "Epoch: 66 | Step:  8\n",
      "training loss is:  2.7612736225128174\n",
      "Epoch: 66 | Step:  9\n",
      "training loss is:  2.4605395793914795\n",
      "Epoch: 66 | Step:  10\n",
      "training loss is:  2.6157641410827637\n",
      "Epoch: 66 | Step:  11\n",
      "training loss is:  2.4803991317749023\n",
      "Epoch: 66 | Step:  12\n",
      "training loss is:  2.059800148010254\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5247608423233032\n",
      "MSE is: 4.721781253814697\n",
      "\n",
      "\n",
      "Epoch: 67 | Step:  0\n",
      "training loss is:  2.5167508125305176\n",
      "Epoch: 67 | Step:  1\n",
      "training loss is:  2.407119035720825\n",
      "Epoch: 67 | Step:  2\n",
      "training loss is:  2.401369571685791\n",
      "Epoch: 67 | Step:  3\n",
      "training loss is:  1.9772690534591675\n",
      "Epoch: 67 | Step:  4\n",
      "training loss is:  2.4953505992889404\n",
      "Epoch: 67 | Step:  5\n",
      "training loss is:  2.3450586795806885\n",
      "Epoch: 67 | Step:  6\n",
      "training loss is:  2.7722721099853516\n",
      "Epoch: 67 | Step:  7\n",
      "training loss is:  2.6511969566345215\n",
      "Epoch: 67 | Step:  8\n",
      "training loss is:  2.481112480163574\n",
      "Epoch: 67 | Step:  9\n",
      "training loss is:  2.4538586139678955\n",
      "Epoch: 67 | Step:  10\n",
      "training loss is:  2.8115131855010986\n",
      "Epoch: 67 | Step:  11\n",
      "training loss is:  2.241469144821167\n",
      "Epoch: 67 | Step:  12\n",
      "training loss is:  2.250638484954834\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.532088041305542\n",
      "MSE is: 4.705902099609375\n",
      "\n",
      "\n",
      "Epoch: 68 | Step:  0\n",
      "training loss is:  2.364922523498535\n",
      "Epoch: 68 | Step:  1\n",
      "training loss is:  2.4348199367523193\n",
      "Epoch: 68 | Step:  2\n",
      "training loss is:  2.431004524230957\n",
      "Epoch: 68 | Step:  3\n",
      "training loss is:  2.431159257888794\n",
      "Epoch: 68 | Step:  4\n",
      "training loss is:  3.0837063789367676\n",
      "Epoch: 68 | Step:  5\n",
      "training loss is:  2.749936819076538\n",
      "Epoch: 68 | Step:  6\n",
      "training loss is:  2.814643144607544\n",
      "Epoch: 68 | Step:  7\n",
      "training loss is:  2.8760592937469482\n",
      "Epoch: 68 | Step:  8\n",
      "training loss is:  2.2918620109558105\n",
      "Epoch: 68 | Step:  9\n",
      "training loss is:  2.557152509689331\n",
      "Epoch: 68 | Step:  10\n",
      "training loss is:  2.8781046867370605\n",
      "Epoch: 68 | Step:  11\n",
      "training loss is:  1.9838205575942993\n",
      "Epoch: 68 | Step:  12\n",
      "training loss is:  2.1173295974731445\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5100936889648438\n",
      "MSE is: 4.654694080352783\n",
      "\n",
      "\n",
      "Epoch: 69 | Step:  0\n",
      "training loss is:  2.210354804992676\n",
      "Epoch: 69 | Step:  1\n",
      "training loss is:  2.5582923889160156\n",
      "Epoch: 69 | Step:  2\n",
      "training loss is:  2.600691556930542\n",
      "Epoch: 69 | Step:  3\n",
      "training loss is:  2.580019950866699\n",
      "Epoch: 69 | Step:  4\n",
      "training loss is:  2.193514585494995\n",
      "Epoch: 69 | Step:  5\n",
      "training loss is:  2.4834210872650146\n",
      "Epoch: 69 | Step:  6\n",
      "training loss is:  2.537827491760254\n",
      "Epoch: 69 | Step:  7\n",
      "training loss is:  2.3113677501678467\n",
      "Epoch: 69 | Step:  8\n",
      "training loss is:  2.3660247325897217\n",
      "Epoch: 69 | Step:  9\n",
      "training loss is:  2.544816493988037\n",
      "Epoch: 69 | Step:  10\n",
      "training loss is:  2.814314842224121\n",
      "Epoch: 69 | Step:  11\n",
      "training loss is:  2.8465723991394043\n",
      "Epoch: 69 | Step:  12\n",
      "training loss is:  2.572866916656494\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.46393620967865\n",
      "MSE is: 4.410342216491699\n",
      "\n",
      "\n",
      "Epoch: 70 | Step:  0\n",
      "training loss is:  2.4941976070404053\n",
      "Epoch: 70 | Step:  1\n",
      "training loss is:  2.303013324737549\n",
      "Epoch: 70 | Step:  2\n",
      "training loss is:  2.5120644569396973\n",
      "Epoch: 70 | Step:  3\n",
      "training loss is:  2.3354954719543457\n",
      "Epoch: 70 | Step:  4\n",
      "training loss is:  2.210949420928955\n",
      "Epoch: 70 | Step:  5\n",
      "training loss is:  2.223607063293457\n",
      "Epoch: 70 | Step:  6\n",
      "training loss is:  2.0521533489227295\n",
      "Epoch: 70 | Step:  7\n",
      "training loss is:  2.460015296936035\n",
      "Epoch: 70 | Step:  8\n",
      "training loss is:  2.8472676277160645\n",
      "Epoch: 70 | Step:  9\n",
      "training loss is:  2.4805846214294434\n",
      "Epoch: 70 | Step:  10\n",
      "training loss is:  3.4143576622009277\n",
      "Epoch: 70 | Step:  11\n",
      "training loss is:  2.871151924133301\n",
      "Epoch: 70 | Step:  12\n",
      "training loss is:  2.9671006202697754\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4061598777770996\n",
      "MSE is: 4.118982315063477\n",
      "\n",
      "\n",
      "Epoch: 71 | Step:  0\n",
      "training loss is:  2.8907835483551025\n",
      "Epoch: 71 | Step:  1\n",
      "training loss is:  2.8269922733306885\n",
      "Epoch: 71 | Step:  2\n",
      "training loss is:  2.837510108947754\n",
      "Epoch: 71 | Step:  3\n",
      "training loss is:  2.5552122592926025\n",
      "Epoch: 71 | Step:  4\n",
      "training loss is:  2.2662670612335205\n",
      "Epoch: 71 | Step:  5\n",
      "training loss is:  2.2940003871917725\n",
      "Epoch: 71 | Step:  6\n",
      "training loss is:  2.4184930324554443\n",
      "Epoch: 71 | Step:  7\n",
      "training loss is:  2.512147903442383\n",
      "Epoch: 71 | Step:  8\n",
      "training loss is:  2.1700387001037598\n",
      "Epoch: 71 | Step:  9\n",
      "training loss is:  2.212047815322876\n",
      "Epoch: 71 | Step:  10\n",
      "training loss is:  2.3421387672424316\n",
      "Epoch: 71 | Step:  11\n",
      "training loss is:  2.4512181282043457\n",
      "Epoch: 71 | Step:  12\n",
      "training loss is:  2.0528066158294678\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5586192607879639\n",
      "MSE is: 4.883748531341553\n",
      "\n",
      "\n",
      "Epoch: 72 | Step:  0\n",
      "training loss is:  2.2470831871032715\n",
      "Epoch: 72 | Step:  1\n",
      "training loss is:  2.820561170578003\n",
      "Epoch: 72 | Step:  2\n",
      "training loss is:  2.4912521839141846\n",
      "Epoch: 72 | Step:  3\n",
      "training loss is:  2.33860445022583\n",
      "Epoch: 72 | Step:  4\n",
      "training loss is:  2.671398639678955\n",
      "Epoch: 72 | Step:  5\n",
      "training loss is:  1.936811089515686\n",
      "Epoch: 72 | Step:  6\n",
      "training loss is:  2.265690565109253\n",
      "Epoch: 72 | Step:  7\n",
      "training loss is:  2.573113441467285\n",
      "Epoch: 72 | Step:  8\n",
      "training loss is:  2.241395950317383\n",
      "Epoch: 72 | Step:  9\n",
      "training loss is:  2.1494264602661133\n",
      "Epoch: 72 | Step:  10\n",
      "training loss is:  2.4920969009399414\n",
      "Epoch: 72 | Step:  11\n",
      "training loss is:  2.463164806365967\n",
      "Epoch: 72 | Step:  12\n",
      "training loss is:  2.114331007003784\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5422747135162354\n",
      "MSE is: 4.805583953857422\n",
      "\n",
      "\n",
      "Epoch: 73 | Step:  0\n",
      "training loss is:  2.2964022159576416\n",
      "Epoch: 73 | Step:  1\n",
      "training loss is:  2.2570526599884033\n",
      "Epoch: 73 | Step:  2\n",
      "training loss is:  2.6920969486236572\n",
      "Epoch: 73 | Step:  3\n",
      "training loss is:  2.5517776012420654\n",
      "Epoch: 73 | Step:  4\n",
      "training loss is:  2.3786792755126953\n",
      "Epoch: 73 | Step:  5\n",
      "training loss is:  2.32663893699646\n",
      "Epoch: 73 | Step:  6\n",
      "training loss is:  2.201336145401001\n",
      "Epoch: 73 | Step:  7\n",
      "training loss is:  2.7745699882507324\n",
      "Epoch: 73 | Step:  8\n",
      "training loss is:  2.3177967071533203\n",
      "Epoch: 73 | Step:  9\n",
      "training loss is:  2.7630844116210938\n",
      "Epoch: 73 | Step:  10\n",
      "training loss is:  2.5932013988494873\n",
      "Epoch: 73 | Step:  11\n",
      "training loss is:  2.487546920776367\n",
      "Epoch: 73 | Step:  12\n",
      "training loss is:  2.378455400466919\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5012931823730469\n",
      "MSE is: 4.581210136413574\n",
      "\n",
      "\n",
      "Epoch: 74 | Step:  0\n",
      "training loss is:  2.1932578086853027\n",
      "Epoch: 74 | Step:  1\n",
      "training loss is:  2.614366054534912\n",
      "Epoch: 74 | Step:  2\n",
      "training loss is:  2.3488993644714355\n",
      "Epoch: 74 | Step:  3\n",
      "training loss is:  2.7648990154266357\n",
      "Epoch: 74 | Step:  4\n",
      "training loss is:  2.2157068252563477\n",
      "Epoch: 74 | Step:  5\n",
      "training loss is:  2.481501817703247\n",
      "Epoch: 74 | Step:  6\n",
      "training loss is:  2.383906364440918\n",
      "Epoch: 74 | Step:  7\n",
      "training loss is:  2.482940435409546\n",
      "Epoch: 74 | Step:  8\n",
      "training loss is:  2.4639346599578857\n",
      "Epoch: 74 | Step:  9\n",
      "training loss is:  2.425053119659424\n",
      "Epoch: 74 | Step:  10\n",
      "training loss is:  2.3814144134521484\n",
      "Epoch: 74 | Step:  11\n",
      "training loss is:  2.566664218902588\n",
      "Epoch: 74 | Step:  12\n",
      "training loss is:  2.9915518760681152\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.6029839515686035\n",
      "MSE is: 5.062705039978027\n",
      "\n",
      "\n",
      "Epoch: 75 | Step:  0\n",
      "training loss is:  2.915823221206665\n",
      "Epoch: 75 | Step:  1\n",
      "training loss is:  2.6557564735412598\n",
      "Epoch: 75 | Step:  2\n",
      "training loss is:  2.2478995323181152\n",
      "Epoch: 75 | Step:  3\n",
      "training loss is:  2.478698492050171\n",
      "Epoch: 75 | Step:  4\n",
      "training loss is:  2.2623205184936523\n",
      "Epoch: 75 | Step:  5\n",
      "training loss is:  2.261094093322754\n",
      "Epoch: 75 | Step:  6\n",
      "training loss is:  2.5773518085479736\n",
      "Epoch: 75 | Step:  7\n",
      "training loss is:  2.1954665184020996\n",
      "Epoch: 75 | Step:  8\n",
      "training loss is:  2.8567287921905518\n",
      "Epoch: 75 | Step:  9\n",
      "training loss is:  2.4828758239746094\n",
      "Epoch: 75 | Step:  10\n",
      "training loss is:  2.7182838916778564\n",
      "Epoch: 75 | Step:  11\n",
      "training loss is:  2.8432679176330566\n",
      "Epoch: 75 | Step:  12\n",
      "training loss is:  2.978938102722168\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4396823644638062\n",
      "MSE is: 4.29240608215332\n",
      "\n",
      "\n",
      "Epoch: 76 | Step:  0\n",
      "training loss is:  2.2842273712158203\n",
      "Epoch: 76 | Step:  1\n",
      "training loss is:  2.5214834213256836\n",
      "Epoch: 76 | Step:  2\n",
      "training loss is:  2.5439934730529785\n",
      "Epoch: 76 | Step:  3\n",
      "training loss is:  2.0647976398468018\n",
      "Epoch: 76 | Step:  4\n",
      "training loss is:  2.4470717906951904\n",
      "Epoch: 76 | Step:  5\n",
      "training loss is:  2.392035961151123\n",
      "Epoch: 76 | Step:  6\n",
      "training loss is:  2.4716992378234863\n",
      "Epoch: 76 | Step:  7\n",
      "training loss is:  2.3938229084014893\n",
      "Epoch: 76 | Step:  8\n",
      "training loss is:  2.3995168209075928\n",
      "Epoch: 76 | Step:  9\n",
      "training loss is:  2.2983200550079346\n",
      "Epoch: 76 | Step:  10\n",
      "training loss is:  2.560258150100708\n",
      "Epoch: 76 | Step:  11\n",
      "training loss is:  2.462231159210205\n",
      "Epoch: 76 | Step:  12\n",
      "training loss is:  2.929202079772949\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5934679508209229\n",
      "MSE is: 5.035226345062256\n",
      "\n",
      "\n",
      "Epoch: 77 | Step:  0\n",
      "training loss is:  2.2477498054504395\n",
      "Epoch: 77 | Step:  1\n",
      "training loss is:  3.01385498046875\n",
      "Epoch: 77 | Step:  2\n",
      "training loss is:  2.7518038749694824\n",
      "Epoch: 77 | Step:  3\n",
      "training loss is:  2.53014874458313\n",
      "Epoch: 77 | Step:  4\n",
      "training loss is:  2.5879805088043213\n",
      "Epoch: 77 | Step:  5\n",
      "training loss is:  2.123577117919922\n",
      "Epoch: 77 | Step:  6\n",
      "training loss is:  2.437751531600952\n",
      "Epoch: 77 | Step:  7\n",
      "training loss is:  2.4056529998779297\n",
      "Epoch: 77 | Step:  8\n",
      "training loss is:  2.5862925052642822\n",
      "Epoch: 77 | Step:  9\n",
      "training loss is:  2.493650197982788\n",
      "Epoch: 77 | Step:  10\n",
      "training loss is:  2.294501781463623\n",
      "Epoch: 77 | Step:  11\n",
      "training loss is:  2.474020481109619\n",
      "Epoch: 77 | Step:  12\n",
      "training loss is:  3.199610710144043\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5919595956802368\n",
      "MSE is: 4.9299397468566895\n",
      "\n",
      "\n",
      "Epoch: 78 | Step:  0\n",
      "training loss is:  3.2798800468444824\n",
      "Epoch: 78 | Step:  1\n",
      "training loss is:  2.1727094650268555\n",
      "Epoch: 78 | Step:  2\n",
      "training loss is:  2.435192108154297\n",
      "Epoch: 78 | Step:  3\n",
      "training loss is:  2.5063517093658447\n",
      "Epoch: 78 | Step:  4\n",
      "training loss is:  2.563429832458496\n",
      "Epoch: 78 | Step:  5\n",
      "training loss is:  2.3065576553344727\n",
      "Epoch: 78 | Step:  6\n",
      "training loss is:  2.6345486640930176\n",
      "Epoch: 78 | Step:  7\n",
      "training loss is:  2.3756916522979736\n",
      "Epoch: 78 | Step:  8\n",
      "training loss is:  2.6403846740722656\n",
      "Epoch: 78 | Step:  9\n",
      "training loss is:  2.4485180377960205\n",
      "Epoch: 78 | Step:  10\n",
      "training loss is:  2.8343708515167236\n",
      "Epoch: 78 | Step:  11\n",
      "training loss is:  3.012918710708618\n",
      "Epoch: 78 | Step:  12\n",
      "training loss is:  2.780022144317627\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4927616119384766\n",
      "MSE is: 4.551352024078369\n",
      "\n",
      "\n",
      "Epoch: 79 | Step:  0\n",
      "training loss is:  2.299283027648926\n",
      "Epoch: 79 | Step:  1\n",
      "training loss is:  2.6691043376922607\n",
      "Epoch: 79 | Step:  2\n",
      "training loss is:  2.598282814025879\n",
      "Epoch: 79 | Step:  3\n",
      "training loss is:  2.183065414428711\n",
      "Epoch: 79 | Step:  4\n",
      "training loss is:  2.387159824371338\n",
      "Epoch: 79 | Step:  5\n",
      "training loss is:  2.405468702316284\n",
      "Epoch: 79 | Step:  6\n",
      "training loss is:  2.379230260848999\n",
      "Epoch: 79 | Step:  7\n",
      "training loss is:  2.4272806644439697\n",
      "Epoch: 79 | Step:  8\n",
      "training loss is:  2.467261791229248\n",
      "Epoch: 79 | Step:  9\n",
      "training loss is:  2.540541887283325\n",
      "Epoch: 79 | Step:  10\n",
      "training loss is:  2.552239179611206\n",
      "Epoch: 79 | Step:  11\n",
      "training loss is:  2.354806900024414\n",
      "Epoch: 79 | Step:  12\n",
      "training loss is:  2.102303981781006\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5209267139434814\n",
      "MSE is: 4.711322784423828\n",
      "\n",
      "\n",
      "Epoch: 80 | Step:  0\n",
      "training loss is:  2.497816562652588\n",
      "Epoch: 80 | Step:  1\n",
      "training loss is:  2.1821532249450684\n",
      "Epoch: 80 | Step:  2\n",
      "training loss is:  2.664949655532837\n",
      "Epoch: 80 | Step:  3\n",
      "training loss is:  2.7868599891662598\n",
      "Epoch: 80 | Step:  4\n",
      "training loss is:  2.5487895011901855\n",
      "Epoch: 80 | Step:  5\n",
      "training loss is:  2.946953058242798\n",
      "Epoch: 80 | Step:  6\n",
      "training loss is:  2.6553354263305664\n",
      "Epoch: 80 | Step:  7\n",
      "training loss is:  2.4139130115509033\n",
      "Epoch: 80 | Step:  8\n",
      "training loss is:  2.384692907333374\n",
      "Epoch: 80 | Step:  9\n",
      "training loss is:  2.8473448753356934\n",
      "Epoch: 80 | Step:  10\n",
      "training loss is:  2.194413661956787\n",
      "Epoch: 80 | Step:  11\n",
      "training loss is:  2.17219614982605\n",
      "Epoch: 80 | Step:  12\n",
      "training loss is:  2.400343418121338\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.532906413078308\n",
      "MSE is: 4.71954345703125\n",
      "\n",
      "\n",
      "Epoch: 81 | Step:  0\n",
      "training loss is:  2.253156900405884\n",
      "Epoch: 81 | Step:  1\n",
      "training loss is:  2.2877535820007324\n",
      "Epoch: 81 | Step:  2\n",
      "training loss is:  2.3454020023345947\n",
      "Epoch: 81 | Step:  3\n",
      "training loss is:  2.19364857673645\n",
      "Epoch: 81 | Step:  4\n",
      "training loss is:  2.623555898666382\n",
      "Epoch: 81 | Step:  5\n",
      "training loss is:  2.2595314979553223\n",
      "Epoch: 81 | Step:  6\n",
      "training loss is:  2.522000789642334\n",
      "Epoch: 81 | Step:  7\n",
      "training loss is:  2.4320685863494873\n",
      "Epoch: 81 | Step:  8\n",
      "training loss is:  2.56956148147583\n",
      "Epoch: 81 | Step:  9\n",
      "training loss is:  2.117332696914673\n",
      "Epoch: 81 | Step:  10\n",
      "training loss is:  2.7535595893859863\n",
      "Epoch: 81 | Step:  11\n",
      "training loss is:  2.4220809936523438\n",
      "Epoch: 81 | Step:  12\n",
      "training loss is:  2.126812696456909\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5463743209838867\n",
      "MSE is: 4.816829204559326\n",
      "\n",
      "\n",
      "Epoch: 82 | Step:  0\n",
      "training loss is:  2.7991161346435547\n",
      "Epoch: 82 | Step:  1\n",
      "training loss is:  2.5648980140686035\n",
      "Epoch: 82 | Step:  2\n",
      "training loss is:  2.508737564086914\n",
      "Epoch: 82 | Step:  3\n",
      "training loss is:  2.4989845752716064\n",
      "Epoch: 82 | Step:  4\n",
      "training loss is:  2.633531093597412\n",
      "Epoch: 82 | Step:  5\n",
      "training loss is:  2.605301856994629\n",
      "Epoch: 82 | Step:  6\n",
      "training loss is:  2.480102777481079\n",
      "Epoch: 82 | Step:  7\n",
      "training loss is:  2.2728817462921143\n",
      "Epoch: 82 | Step:  8\n",
      "training loss is:  2.3857650756835938\n",
      "Epoch: 82 | Step:  9\n",
      "training loss is:  2.91019606590271\n",
      "Epoch: 82 | Step:  10\n",
      "training loss is:  2.165126085281372\n",
      "Epoch: 82 | Step:  11\n",
      "training loss is:  2.639399766921997\n",
      "Epoch: 82 | Step:  12\n",
      "training loss is:  2.1593053340911865\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5014526844024658\n",
      "MSE is: 4.585544586181641\n",
      "\n",
      "\n",
      "Epoch: 83 | Step:  0\n",
      "training loss is:  2.5661404132843018\n",
      "Epoch: 83 | Step:  1\n",
      "training loss is:  2.2560713291168213\n",
      "Epoch: 83 | Step:  2\n",
      "training loss is:  2.118675947189331\n",
      "Epoch: 83 | Step:  3\n",
      "training loss is:  2.6304662227630615\n",
      "Epoch: 83 | Step:  4\n",
      "training loss is:  2.285444974899292\n",
      "Epoch: 83 | Step:  5\n",
      "training loss is:  2.3467631340026855\n",
      "Epoch: 83 | Step:  6\n",
      "training loss is:  2.0187718868255615\n",
      "Epoch: 83 | Step:  7\n",
      "training loss is:  2.7372798919677734\n",
      "Epoch: 83 | Step:  8\n",
      "training loss is:  2.4499351978302\n",
      "Epoch: 83 | Step:  9\n",
      "training loss is:  2.1836211681365967\n",
      "Epoch: 83 | Step:  10\n",
      "training loss is:  2.671513557434082\n",
      "Epoch: 83 | Step:  11\n",
      "training loss is:  2.3945112228393555\n",
      "Epoch: 83 | Step:  12\n",
      "training loss is:  2.421505928039551\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5761889219284058\n",
      "MSE is: 4.983298301696777\n",
      "\n",
      "\n",
      "Epoch: 84 | Step:  0\n",
      "training loss is:  2.2742955684661865\n",
      "Epoch: 84 | Step:  1\n",
      "training loss is:  2.4470860958099365\n",
      "Epoch: 84 | Step:  2\n",
      "training loss is:  2.94533634185791\n",
      "Epoch: 84 | Step:  3\n",
      "training loss is:  2.3117411136627197\n",
      "Epoch: 84 | Step:  4\n",
      "training loss is:  2.3559582233428955\n",
      "Epoch: 84 | Step:  5\n",
      "training loss is:  2.7072556018829346\n",
      "Epoch: 84 | Step:  6\n",
      "training loss is:  2.046814203262329\n",
      "Epoch: 84 | Step:  7\n",
      "training loss is:  2.269385814666748\n",
      "Epoch: 84 | Step:  8\n",
      "training loss is:  2.36222505569458\n",
      "Epoch: 84 | Step:  9\n",
      "training loss is:  2.4978928565979004\n",
      "Epoch: 84 | Step:  10\n",
      "training loss is:  2.52653169631958\n",
      "Epoch: 84 | Step:  11\n",
      "training loss is:  2.413370370864868\n",
      "Epoch: 84 | Step:  12\n",
      "training loss is:  2.1975433826446533\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.571019172668457\n",
      "MSE is: 4.961812973022461\n",
      "\n",
      "\n",
      "Epoch: 85 | Step:  0\n",
      "training loss is:  2.580554485321045\n",
      "Epoch: 85 | Step:  1\n",
      "training loss is:  2.5054426193237305\n",
      "Epoch: 85 | Step:  2\n",
      "training loss is:  2.3439090251922607\n",
      "Epoch: 85 | Step:  3\n",
      "training loss is:  2.4171624183654785\n",
      "Epoch: 85 | Step:  4\n",
      "training loss is:  2.361051559448242\n",
      "Epoch: 85 | Step:  5\n",
      "training loss is:  2.299793004989624\n",
      "Epoch: 85 | Step:  6\n",
      "training loss is:  2.6382734775543213\n",
      "Epoch: 85 | Step:  7\n",
      "training loss is:  2.2832937240600586\n",
      "Epoch: 85 | Step:  8\n",
      "training loss is:  2.588237762451172\n",
      "Epoch: 85 | Step:  9\n",
      "training loss is:  2.6632161140441895\n",
      "Epoch: 85 | Step:  10\n",
      "training loss is:  2.6055233478546143\n",
      "Epoch: 85 | Step:  11\n",
      "training loss is:  2.1609301567077637\n",
      "Epoch: 85 | Step:  12\n",
      "training loss is:  2.7226545810699463\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4863100051879883\n",
      "MSE is: 4.522430419921875\n",
      "\n",
      "\n",
      "Epoch: 86 | Step:  0\n",
      "training loss is:  2.396317958831787\n",
      "Epoch: 86 | Step:  1\n",
      "training loss is:  2.360093116760254\n",
      "Epoch: 86 | Step:  2\n",
      "training loss is:  2.4952826499938965\n",
      "Epoch: 86 | Step:  3\n",
      "training loss is:  2.3263823986053467\n",
      "Epoch: 86 | Step:  4\n",
      "training loss is:  2.912156343460083\n",
      "Epoch: 86 | Step:  5\n",
      "training loss is:  2.673506736755371\n",
      "Epoch: 86 | Step:  6\n",
      "training loss is:  2.521106719970703\n",
      "Epoch: 86 | Step:  7\n",
      "training loss is:  2.708634853363037\n",
      "Epoch: 86 | Step:  8\n",
      "training loss is:  2.4573636054992676\n",
      "Epoch: 86 | Step:  9\n",
      "training loss is:  2.5633625984191895\n",
      "Epoch: 86 | Step:  10\n",
      "training loss is:  2.515340805053711\n",
      "Epoch: 86 | Step:  11\n",
      "training loss is:  2.169370412826538\n",
      "Epoch: 86 | Step:  12\n",
      "training loss is:  2.2652225494384766\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.51241135597229\n",
      "MSE is: 4.6548004150390625\n",
      "\n",
      "\n",
      "Epoch: 87 | Step:  0\n",
      "training loss is:  2.4351158142089844\n",
      "Epoch: 87 | Step:  1\n",
      "training loss is:  2.2654991149902344\n",
      "Epoch: 87 | Step:  2\n",
      "training loss is:  2.6554648876190186\n",
      "Epoch: 87 | Step:  3\n",
      "training loss is:  2.6025123596191406\n",
      "Epoch: 87 | Step:  4\n",
      "training loss is:  2.424863815307617\n",
      "Epoch: 87 | Step:  5\n",
      "training loss is:  2.4851441383361816\n",
      "Epoch: 87 | Step:  6\n",
      "training loss is:  2.686037063598633\n",
      "Epoch: 87 | Step:  7\n",
      "training loss is:  2.4521279335021973\n",
      "Epoch: 87 | Step:  8\n",
      "training loss is:  2.3823325634002686\n",
      "Epoch: 87 | Step:  9\n",
      "training loss is:  2.408013343811035\n",
      "Epoch: 87 | Step:  10\n",
      "training loss is:  2.5815839767456055\n",
      "Epoch: 87 | Step:  11\n",
      "training loss is:  2.5978634357452393\n",
      "Epoch: 87 | Step:  12\n",
      "training loss is:  2.266083240509033\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.6484553813934326\n",
      "MSE is: 5.340561389923096\n",
      "\n",
      "\n",
      "Epoch: 88 | Step:  0\n",
      "training loss is:  2.854421615600586\n",
      "Epoch: 88 | Step:  1\n",
      "training loss is:  2.94233775138855\n",
      "Epoch: 88 | Step:  2\n",
      "training loss is:  2.9931466579437256\n",
      "Epoch: 88 | Step:  3\n",
      "training loss is:  2.3756563663482666\n",
      "Epoch: 88 | Step:  4\n",
      "training loss is:  2.470366954803467\n",
      "Epoch: 88 | Step:  5\n",
      "training loss is:  2.2477402687072754\n",
      "Epoch: 88 | Step:  6\n",
      "training loss is:  2.5246825218200684\n",
      "Epoch: 88 | Step:  7\n",
      "training loss is:  2.3135578632354736\n",
      "Epoch: 88 | Step:  8\n",
      "training loss is:  2.3619155883789062\n",
      "Epoch: 88 | Step:  9\n",
      "training loss is:  2.471893787384033\n",
      "Epoch: 88 | Step:  10\n",
      "training loss is:  2.590820550918579\n",
      "Epoch: 88 | Step:  11\n",
      "training loss is:  3.116746187210083\n",
      "Epoch: 88 | Step:  12\n",
      "training loss is:  3.0593671798706055\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4378198385238647\n",
      "MSE is: 4.235764026641846\n",
      "\n",
      "\n",
      "Epoch: 89 | Step:  0\n",
      "training loss is:  3.0310280323028564\n",
      "Epoch: 89 | Step:  1\n",
      "training loss is:  2.4715735912323\n",
      "Epoch: 89 | Step:  2\n",
      "training loss is:  2.4348461627960205\n",
      "Epoch: 89 | Step:  3\n",
      "training loss is:  2.316094160079956\n",
      "Epoch: 89 | Step:  4\n",
      "training loss is:  2.539858818054199\n",
      "Epoch: 89 | Step:  5\n",
      "training loss is:  2.701714277267456\n",
      "Epoch: 89 | Step:  6\n",
      "training loss is:  2.3753459453582764\n",
      "Epoch: 89 | Step:  7\n",
      "training loss is:  2.7096221446990967\n",
      "Epoch: 89 | Step:  8\n",
      "training loss is:  2.156679153442383\n",
      "Epoch: 89 | Step:  9\n",
      "training loss is:  2.3232405185699463\n",
      "Epoch: 89 | Step:  10\n",
      "training loss is:  2.202965497970581\n",
      "Epoch: 89 | Step:  11\n",
      "training loss is:  2.637087821960449\n",
      "Epoch: 89 | Step:  12\n",
      "training loss is:  2.535045862197876\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5858945846557617\n",
      "MSE is: 4.989022731781006\n",
      "\n",
      "\n",
      "Epoch: 90 | Step:  0\n",
      "training loss is:  2.715121269226074\n",
      "Epoch: 90 | Step:  1\n",
      "training loss is:  2.663740396499634\n",
      "Epoch: 90 | Step:  2\n",
      "training loss is:  2.3007237911224365\n",
      "Epoch: 90 | Step:  3\n",
      "training loss is:  2.4965178966522217\n",
      "Epoch: 90 | Step:  4\n",
      "training loss is:  2.130958080291748\n",
      "Epoch: 90 | Step:  5\n",
      "training loss is:  2.543414354324341\n",
      "Epoch: 90 | Step:  6\n",
      "training loss is:  2.3337104320526123\n",
      "Epoch: 90 | Step:  7\n",
      "training loss is:  2.46734619140625\n",
      "Epoch: 90 | Step:  8\n",
      "training loss is:  2.690487861633301\n",
      "Epoch: 90 | Step:  9\n",
      "training loss is:  2.9624617099761963\n",
      "Epoch: 90 | Step:  10\n",
      "training loss is:  2.6646833419799805\n",
      "Epoch: 90 | Step:  11\n",
      "training loss is:  2.737039566040039\n",
      "Epoch: 90 | Step:  12\n",
      "training loss is:  2.454869270324707\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5162005424499512\n",
      "MSE is: 4.675968647003174\n",
      "\n",
      "\n",
      "Epoch: 91 | Step:  0\n",
      "training loss is:  2.430311918258667\n",
      "Epoch: 91 | Step:  1\n",
      "training loss is:  2.0766820907592773\n",
      "Epoch: 91 | Step:  2\n",
      "training loss is:  2.1263816356658936\n",
      "Epoch: 91 | Step:  3\n",
      "training loss is:  2.5157954692840576\n",
      "Epoch: 91 | Step:  4\n",
      "training loss is:  2.3456666469573975\n",
      "Epoch: 91 | Step:  5\n",
      "training loss is:  2.6316018104553223\n",
      "Epoch: 91 | Step:  6\n",
      "training loss is:  2.5645477771759033\n",
      "Epoch: 91 | Step:  7\n",
      "training loss is:  2.147505760192871\n",
      "Epoch: 91 | Step:  8\n",
      "training loss is:  2.247190475463867\n",
      "Epoch: 91 | Step:  9\n",
      "training loss is:  2.497209072113037\n",
      "Epoch: 91 | Step:  10\n",
      "training loss is:  2.3708457946777344\n",
      "Epoch: 91 | Step:  11\n",
      "training loss is:  2.654531240463257\n",
      "Epoch: 91 | Step:  12\n",
      "training loss is:  2.8447482585906982\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.540429711341858\n",
      "MSE is: 4.789473056793213\n",
      "\n",
      "\n",
      "Epoch: 92 | Step:  0\n",
      "training loss is:  2.5750339031219482\n",
      "Epoch: 92 | Step:  1\n",
      "training loss is:  2.47004771232605\n",
      "Epoch: 92 | Step:  2\n",
      "training loss is:  2.3403830528259277\n",
      "Epoch: 92 | Step:  3\n",
      "training loss is:  2.5963892936706543\n",
      "Epoch: 92 | Step:  4\n",
      "training loss is:  2.303110122680664\n",
      "Epoch: 92 | Step:  5\n",
      "training loss is:  2.475093364715576\n",
      "Epoch: 92 | Step:  6\n",
      "training loss is:  2.3639938831329346\n",
      "Epoch: 92 | Step:  7\n",
      "training loss is:  2.29075288772583\n",
      "Epoch: 92 | Step:  8\n",
      "training loss is:  2.2801461219787598\n",
      "Epoch: 92 | Step:  9\n",
      "training loss is:  2.4478092193603516\n",
      "Epoch: 92 | Step:  10\n",
      "training loss is:  2.2415053844451904\n",
      "Epoch: 92 | Step:  11\n",
      "training loss is:  2.4439477920532227\n",
      "Epoch: 92 | Step:  12\n",
      "training loss is:  2.995234489440918\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.808465600013733\n",
      "MSE is: 5.958774089813232\n",
      "\n",
      "\n",
      "Epoch: 93 | Step:  0\n",
      "training loss is:  3.9499917030334473\n",
      "Epoch: 93 | Step:  1\n",
      "training loss is:  4.318489074707031\n",
      "Epoch: 93 | Step:  2\n",
      "training loss is:  2.5408639907836914\n",
      "Epoch: 93 | Step:  3\n",
      "training loss is:  2.515768527984619\n",
      "Epoch: 93 | Step:  4\n",
      "training loss is:  2.5818135738372803\n",
      "Epoch: 93 | Step:  5\n",
      "training loss is:  2.604248046875\n",
      "Epoch: 93 | Step:  6\n",
      "training loss is:  2.976468801498413\n",
      "Epoch: 93 | Step:  7\n",
      "training loss is:  2.483185052871704\n",
      "Epoch: 93 | Step:  8\n",
      "training loss is:  2.6688714027404785\n",
      "Epoch: 93 | Step:  9\n",
      "training loss is:  2.7663075923919678\n",
      "Epoch: 93 | Step:  10\n",
      "training loss is:  2.158874273300171\n",
      "Epoch: 93 | Step:  11\n",
      "training loss is:  2.168135643005371\n",
      "Epoch: 93 | Step:  12\n",
      "training loss is:  2.1779181957244873\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.551184058189392\n",
      "MSE is: 4.846652030944824\n",
      "\n",
      "\n",
      "Epoch: 94 | Step:  0\n",
      "training loss is:  2.586576223373413\n",
      "Epoch: 94 | Step:  1\n",
      "training loss is:  2.6577069759368896\n",
      "Epoch: 94 | Step:  2\n",
      "training loss is:  2.4499423503875732\n",
      "Epoch: 94 | Step:  3\n",
      "training loss is:  2.314589500427246\n",
      "Epoch: 94 | Step:  4\n",
      "training loss is:  2.8240139484405518\n",
      "Epoch: 94 | Step:  5\n",
      "training loss is:  2.823302984237671\n",
      "Epoch: 94 | Step:  6\n",
      "training loss is:  2.797559976577759\n",
      "Epoch: 94 | Step:  7\n",
      "training loss is:  2.718247175216675\n",
      "Epoch: 94 | Step:  8\n",
      "training loss is:  2.1562931537628174\n",
      "Epoch: 94 | Step:  9\n",
      "training loss is:  2.4698660373687744\n",
      "Epoch: 94 | Step:  10\n",
      "training loss is:  2.103069305419922\n",
      "Epoch: 94 | Step:  11\n",
      "training loss is:  2.253749132156372\n",
      "Epoch: 94 | Step:  12\n",
      "training loss is:  2.56074857711792\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5756467580795288\n",
      "MSE is: 4.954771518707275\n",
      "\n",
      "\n",
      "Epoch: 95 | Step:  0\n",
      "training loss is:  2.4754157066345215\n",
      "Epoch: 95 | Step:  1\n",
      "training loss is:  2.5009396076202393\n",
      "Epoch: 95 | Step:  2\n",
      "training loss is:  2.7022364139556885\n",
      "Epoch: 95 | Step:  3\n",
      "training loss is:  2.278920888900757\n",
      "Epoch: 95 | Step:  4\n",
      "training loss is:  2.1873388290405273\n",
      "Epoch: 95 | Step:  5\n",
      "training loss is:  2.5511105060577393\n",
      "Epoch: 95 | Step:  6\n",
      "training loss is:  2.330683946609497\n",
      "Epoch: 95 | Step:  7\n",
      "training loss is:  2.810469388961792\n",
      "Epoch: 95 | Step:  8\n",
      "training loss is:  2.247209072113037\n",
      "Epoch: 95 | Step:  9\n",
      "training loss is:  2.26780366897583\n",
      "Epoch: 95 | Step:  10\n",
      "training loss is:  2.6489882469177246\n",
      "Epoch: 95 | Step:  11\n",
      "training loss is:  2.860722541809082\n",
      "Epoch: 95 | Step:  12\n",
      "training loss is:  2.0854482650756836\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.4855530261993408\n",
      "MSE is: 4.513818264007568\n",
      "\n",
      "\n",
      "Epoch: 96 | Step:  0\n",
      "training loss is:  2.4256298542022705\n",
      "Epoch: 96 | Step:  1\n",
      "training loss is:  2.3786375522613525\n",
      "Epoch: 96 | Step:  2\n",
      "training loss is:  2.4353113174438477\n",
      "Epoch: 96 | Step:  3\n",
      "training loss is:  2.139646530151367\n",
      "Epoch: 96 | Step:  4\n",
      "training loss is:  2.162386178970337\n",
      "Epoch: 96 | Step:  5\n",
      "training loss is:  2.7112107276916504\n",
      "Epoch: 96 | Step:  6\n",
      "training loss is:  2.7759041786193848\n",
      "Epoch: 96 | Step:  7\n",
      "training loss is:  2.5031721591949463\n",
      "Epoch: 96 | Step:  8\n",
      "training loss is:  2.8836238384246826\n",
      "Epoch: 96 | Step:  9\n",
      "training loss is:  2.536531686782837\n",
      "Epoch: 96 | Step:  10\n",
      "training loss is:  2.417954683303833\n",
      "Epoch: 96 | Step:  11\n",
      "training loss is:  1.9639147520065308\n",
      "Epoch: 96 | Step:  12\n",
      "training loss is:  2.299788236618042\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5304750204086304\n",
      "MSE is: 4.751241207122803\n",
      "\n",
      "\n",
      "Epoch: 97 | Step:  0\n",
      "training loss is:  2.651134490966797\n",
      "Epoch: 97 | Step:  1\n",
      "training loss is:  2.3391640186309814\n",
      "Epoch: 97 | Step:  2\n",
      "training loss is:  2.5261335372924805\n",
      "Epoch: 97 | Step:  3\n",
      "training loss is:  2.80641508102417\n",
      "Epoch: 97 | Step:  4\n",
      "training loss is:  2.6443772315979004\n",
      "Epoch: 97 | Step:  5\n",
      "training loss is:  2.9304282665252686\n",
      "Epoch: 97 | Step:  6\n",
      "training loss is:  2.3134050369262695\n",
      "Epoch: 97 | Step:  7\n",
      "training loss is:  2.3657126426696777\n",
      "Epoch: 97 | Step:  8\n",
      "training loss is:  2.1134817600250244\n",
      "Epoch: 97 | Step:  9\n",
      "training loss is:  2.2542519569396973\n",
      "Epoch: 97 | Step:  10\n",
      "training loss is:  2.636709690093994\n",
      "Epoch: 97 | Step:  11\n",
      "training loss is:  2.367227077484131\n",
      "Epoch: 97 | Step:  12\n",
      "training loss is:  2.340576171875\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5138887166976929\n",
      "MSE is: 4.662208557128906\n",
      "\n",
      "\n",
      "Epoch: 98 | Step:  0\n",
      "training loss is:  2.150580883026123\n",
      "Epoch: 98 | Step:  1\n",
      "training loss is:  2.261460781097412\n",
      "Epoch: 98 | Step:  2\n",
      "training loss is:  2.557485818862915\n",
      "Epoch: 98 | Step:  3\n",
      "training loss is:  2.2722456455230713\n",
      "Epoch: 98 | Step:  4\n",
      "training loss is:  2.5041115283966064\n",
      "Epoch: 98 | Step:  5\n",
      "training loss is:  2.484637975692749\n",
      "Epoch: 98 | Step:  6\n",
      "training loss is:  2.5155510902404785\n",
      "Epoch: 98 | Step:  7\n",
      "training loss is:  2.7154154777526855\n",
      "Epoch: 98 | Step:  8\n",
      "training loss is:  2.8067989349365234\n",
      "Epoch: 98 | Step:  9\n",
      "training loss is:  2.2842509746551514\n",
      "Epoch: 98 | Step:  10\n",
      "training loss is:  2.570868968963623\n",
      "Epoch: 98 | Step:  11\n",
      "training loss is:  2.3973357677459717\n",
      "Epoch: 98 | Step:  12\n",
      "training loss is:  2.8266992568969727\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.457397222518921\n",
      "MSE is: 4.381292343139648\n",
      "\n",
      "\n",
      "Epoch: 99 | Step:  0\n",
      "training loss is:  2.369950532913208\n",
      "Epoch: 99 | Step:  1\n",
      "training loss is:  2.661545515060425\n",
      "Epoch: 99 | Step:  2\n",
      "training loss is:  2.3040049076080322\n",
      "Epoch: 99 | Step:  3\n",
      "training loss is:  2.3267993927001953\n",
      "Epoch: 99 | Step:  4\n",
      "training loss is:  2.3373894691467285\n",
      "Epoch: 99 | Step:  5\n",
      "training loss is:  2.077237129211426\n",
      "Epoch: 99 | Step:  6\n",
      "training loss is:  2.4701473712921143\n",
      "Epoch: 99 | Step:  7\n",
      "training loss is:  2.5398523807525635\n",
      "Epoch: 99 | Step:  8\n",
      "training loss is:  2.3951005935668945\n",
      "Epoch: 99 | Step:  9\n",
      "training loss is:  2.5166399478912354\n",
      "Epoch: 99 | Step:  10\n",
      "training loss is:  2.6463193893432617\n",
      "Epoch: 99 | Step:  11\n",
      "training loss is:  2.641890048980713\n",
      "Epoch: 99 | Step:  12\n",
      "training loss is:  2.6904022693634033\n",
      "<class 'torch.autograd.variable.Variable'>\n",
      "MAE is: 1.5128885507583618\n",
      "MSE is: 4.663056373596191\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#training:\n",
    "for epoch in range(100):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader):\n",
    "        print('Epoch:', epoch, '| Step: ', step)\n",
    "        \n",
    "        b_x = Variable(batch_x.float())\n",
    "        b_y = Variable(batch_y.float())\n",
    "        \n",
    "        prediction = net(b_x)\n",
    "        train_loss = loss_func(prediction, b_y)\n",
    "        print('training loss is: ',train_loss.data[0])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #validation loss:\n",
    "        if (step+1) % 13 ==0:\n",
    "            validation_output = net(x_test)\n",
    "            print(type(validation_output))\n",
    "            validation_loss_MAE = loss_func_MAe(validation_output, y_test)\n",
    "            validation_loss_MSE = loss_func(validation_output, y_test)\n",
    "            print('MAE is:',validation_loss_MAE.data[0])\n",
    "            print(\"MSE is:\",validation_loss_MSE.data[0] )\n",
    "            print('\\n')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2aeb12429ba8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_prediction_y = net(x_test).data.numpy()\n",
    "true_y = y_test.data.numpy()\n",
    "plt.scatter(true_y, final_prediction_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHg1JREFUeJzt3W2MXNV5B/D/s7ODGZOEdYrTlMGLjUrt4rrxigm15L7I\nNMIUiNlAEhOFSmk+uKmaNlDXkR2QMAmVnVqBSE3Vym3yCSexw8sGiloDtdNKVKbZze7iONgVryYD\nVTbBSxo8wePx0w8zd3xn9p5779y5M/fcO/+fhLKenb1zfLN+5sxznvMcUVUQEVF2DCU9ACIiihcD\nOxFRxjCwExFlDAM7EVHGMLATEWUMAzsRUcYwsBMRZQwDOxFRxjCwExFlzHASL3rJJZfo8uXLk3hp\nIqLUmpqa+qmqLg16XiKBffny5ZicnEzipYmIUktEXg3zPKZiiIgyhoGdiChjGNiJiDKGgZ2IKGMY\n2ImIMoaBnYgoYxIpdySibJiYLmPPwRN4fb6CS0cK2LZxJcbHikkPa+AxsBNRJBPTZex45Cgq1RoA\noDxfwY5HjgIAg3vCmIohokj2HDzRDOqOSrWGPQdPJDQicjCwE1Ekr89XOnqc+oeBnYgiuXSk0NHj\n1D8M7EQUybaNK1HI51oeK+Rz2LZxZUIjIgcXT4koEmeBlFUx9mFgJ6LIxseKDOQWYiqGiChjGNiJ\niDKGgZ2IKGMY2ImIMoaBnYgoYxjYiYgyhuWORBQJOzvai4GdiDrGzo52YyqGiDrGzo52Y2Anoo6x\ns6PdGNiJqGPs7Gg3BnYi6hg7O9qNi6dE1DF2drRbbIFdRHIAJgGUVfWmuK5LRHZiZ0d7xTlj/xyA\n5wG8J8ZrEtGAYX1892LJsYvIZQBuBPDPcVyPiAaTUx9fnq9Acb4+fmK6nPTQUiWuxdOvAvg8gHMx\nXY+IBhDr4+PRdWAXkZsA/ERVpwKet0VEJkVkcm5urtuXJaIMYn18POLIsa8HsElEbgBwIYD3iMiD\nqnq7+0mquhfAXgAolUoaw+sSUYp55dIvHSmg7BHEWR/fma5n7Kq6Q1UvU9XlAG4DcKg9qBMRuZly\n6ct/xTuAb1i1tL8DTDluUCKivjPl0o+8dMrz+YePM33biVg3KKnq9wB8L85rElH2mHLmNfXO0jLH\n3hnO2Imo70w58yHxfv7I4nwPR5M9DOxE1HemXjOLhr1D0nylylr2DjCwE1HfjY8VseuWNSiOFCAA\niiMF7LplDX5Z9d4KowpuVOoAm4ARUSK8es3sOXjCs9wROL9Rie0FgnHGTkSRTUyXsX73IazY/gTW\n7z7U9YzaK0XjxkXUcDhjJ6JIenHuqfNzWw/MelbIcKNSOAzsRBSJX1+XbtIlzs+63zSA8wd5sPtj\nMAZ2Ioqkl31dTAd5AIj9U0IWMbATUSS97uvitbi6fvehnnxK6JTtnxq4eEpEkSRx7qkN3R/T0DOe\ngZ2IIjHVonvNXOOqnjF9GujnomoaesYzFUNEkYU59zTO6pltG1caF1X7xYZPDUEY2Imop7qtnnHn\nsy8u5CE4Xwa5ZHEe93x4dV/z22noGc9UDBH1VDcz3PZ89nylitOutgOmFgS9lMTaQqcY2InIKI7c\nuLmTowRe12u275ZEbruTtYWkiBr6H/dSqVTSycnJvr8uEYXXnhsHgPyQ4F0XDmP+dBWXjhSwYdVS\nHD4+51v253WddoV8zjM4rtj+BIIilAB4efeNnf71UklEplS1FPQ85tiJyJPXbLl6TnHqdBVAfRH0\nwSMnm98rz1dw5/4Z3LF/BkXXhiLnOjkR1FSb/+tmyrmb8tntz3Gzvca8HxjYichTlCoPJ1yX5yvY\n9p1ZQIBqrf5oTRWFfM44c/d6Pa8qGLf23HYv+tekEQM7EXkKM1v2Uz23MIninrl7vV679tYCFxfy\nEEEzFdQ+G4+7f01aZ/8M7ETkKWi2HJXXzN2vqiRMrbwjzhrzNM/+WRVDRJ7aqz9GCnnjmaSdujA/\nhJFCPlJViV+ljuls1ChnpqZhh6kJZ+xEZNQ+Wx774pPNxdMgQwLkhqSZY3c7dbqKQj6HBzav7Wj2\nGzSLNhX5RSn+S8MOUxPO2IkotHmfoJ5vm87nRLD5g8tQNNSxR5n9Bs2i36p4j8/0uB8b+tJExcBO\nREbtaY+LC94pjZzIgsXS6jnF4eNzeGb7tTBlcMrzlY42PgXNouMMxmnYYWrCwE5Enrza07595uyC\nmXkhn/OsckHjZwAY3xCc54RtexsUuDesWur5fdPjbu1vYgCs32Fqwhw7EXny3KBUUyxZnMfiC4Zb\nSgD3HDxhLI28e+Io3j5z1ve1KtUath6YBeBfcRLU3fHw8TnPnzM97vDK3d+xf8bYZMz2MkgGdiLy\nZEp7nDpd9awjv2P/jOfzv/nsSXiUtC9QU8WOR45i8tU3jW0K/I7MW7/7kPHNJWjB09ST5tTp6oIS\nxzSUQTIVQ0Se/PLS7ScH+QW0MEHdUanWsO/IyZb0zx37Z7D23iebqZrxsSKe2X4tXt59I57Zfi0A\nNFNGUf4ugH/gb1/kTUMZJAM7EXnyWjxs14uA5vU+MF+pGvPwQR0gAe8c+8R0GWNffBLLQzQac79p\npKEMkoGdiDy1b1AyeX2+0pfzPivVGnY+dmzB42HaHrTn2Cemy9j20Gzomnxp/AyQjjJIBnYiWsCp\nELmzkTd/YPNaYz36pSOFvqUh5ivVljeRiemy75uOwymrdCpe7n38mOfGKRMFmn/HNJRBMrATUQuv\nMscdjxzFhlVLFwQ0aXy/m2ZhnXK/iex87FhgGsXh/vuEnam7OamWNBy0kZqqGNvLi4iyYudjxzwX\nBw8fn8OuW9bg3sePNQNj/4/pOR9gJ6bLmI+wozRImO6TnTQmS0IqAnsayouIssAvWDoBNew5o/mh\nhbtR4+AE2F6lfy7MD+HM2XMtY7ct1RKk61SMiCwTkcMi8iMROSYin4tjYG5pKC8iygK/f1NOLj1s\nG9+LFg1j0XD82V6nwqVX6Z+3z9QAQeTukzaIY8Z+FsBWVf2BiLwbwJSIPKWqP4rh2gDSUV5ElAV+\n/6a2bVzZXEw1EZxPz/QiTQIAD0+VF7xW3Ko1xUWLhjFzz3U9eoXe6vrtVFXfUNUfNL7+PwDPA4j1\nrS0N5UVEWWDqW75kcR7jY8XAvub9yLlXqjU8eOSk72vF0Te+nwvCcYv1c5KILAcwBuBZj+9tEZFJ\nEZmcm/Pv29AuDeVFRGk3MV3GL365sKdLPie458OrAUTra56Ec1p/M3JKNHNijvSm7/j9jO1iC+wi\n8i4ADwO4Q1V/3v59Vd2rqiVVLS1dGtxpzS0N5UVEabfn4AnPxc6LLhhu/luL0tc8KfOnq81Joan7\nJGD+lOH3M7aLpSpGRPKoB/V9qvpIHNdsZ3t5EVHamfLr7mDe7QHX/XRhfghbD8xGDtDFkUJqy6zj\nqIoRAF8H8Lyq3t/9kIgoCWHWssL0j7FFpXouVFAfKeQ9U70bVi313KjVj/YJ3YojFbMewB8DuFZE\nZhr/3RDDdYmoR7wOhA5ay3Jmr2HLHdOgkM9h56bVnqnew8fnUltmLZpAHqlUKunk5GTfX5eIFm74\nA+oBbtctawAs7HU+Plb0/Jm0KwakVlYYuj4KgJd332i8bi/TNyIypaqloOelYucpEcXHtOHvjv0z\nxmCXxZl6UMA1rSf4lVnbskueTcCIBozfJiRTHjlrmwHDpFSilFnbskueM3aiARNU2eL0PXfOMc2J\nJNLsq9eC3qxMx/D5zbxt2SXPwE4DL60lbVF5HQjdbr5SbbYESHM9t58wO9c7LbOOkr7pBaZiaKCZ\neo+noaQtKveGv0HVq53rtuySZ2CngWZLTrTfnAOhv7p5bWrq0rvltAjo5c51W3bJMxVDA82WnGhS\nvPLIp8+cjXTCkO1e3NWf7TU27JJnYKeBZktONEntgejuiaN48MjJBEcUP+cwaq+A67XGAtRPknLW\nGYak3lgsqPbdFgzsNNC8FhIHvXPoE8+9kfQQYuc+jNpd7VNTbenrXp6vYNtDs6jVFO5zopzeaGk5\nvY05dhpotuREbZLFNAxwPig7n9Ccap/2mp9qW1Bvl4Y1GM7YaeDZkBOl3suJxLZ71vY1GM7YiajF\nSMH/lKQ0EsRbj2/7GgwDOxG12LlpNfJxnC1nEUX4E5FyQ+IbGNOwBsPATkQtxseK2HzNsqSHEQt3\nKA87Y3/3omHcv3ltyycX533Obw3GqxVyUphjJ6IFDh/v7FxiGzlVL+1E/M9ufatSXbDu4i6JdBZO\n27/v1dVx8tU3cfj4XN/bVTCwE9GCWu60HH/nxzRDD5q4u/PnE9Nl3Pv4sZZKIa+SR9MO5n1HTraU\nUvarVJKpGKIB59UvZ5C9/c5ZTEyXm/fFq/yzveTRVCXT/h7Sr1JJztiJBlzWDtHwU8jnsGh4qLmj\n1Mt8pYodjxzFhfkh3/vifgPs5FNOP0olOWMnGnC212THJSeCXbesqVf95PwrZCrVWuBGLadNAeDd\n1dH0Cv0olWRgJxpwttdkx6Wmej63HUNJu7tNgdcO5k+uG02shS9TMUQDLszBG1lx5ReeQNWvX0CH\n3J92vHYwly5/byKHuDCwEw04J9D81YGZZrOrrIozqAPBn3aSalfBVAwRYXysiPs/vjYw90zn2bwD\nlTN2GniDduapF+ceVGsZn7LHJCeCW68OPxvv9+8YAzsNNNOOQcDufttxar8HFKymioenyihd/t7A\n35MkfseYiqGBNqhnnroNUh17nML8nkxMl7H1wGzff8cY2GmgDfqZp8Bg/V3j5nfvnJm6qbVBL+87\nAzsNNFNVw6DUdgOD9XeNm9+9C/ok1Mv7zsBOA81rx6DN1Q694HUPKFjQ70nQjHzDqqVxD6mJgZ0G\nGs88rd+DW68uGrfA00JhqmKCZuS9bI3MqhgaeDzztB5kWOgYXpiqmKAdvb3MsTOwZxRrs8PjveIC\nahROZYvpd2V8rIjJV9/Eg0dOen6/lzl2BvYMYm12eLxXdVk5XKPfgqpiHp7yPh6v1+s4seTYReR6\nETkhIi+IyPY4rknRsTY7vLjulU3nXUbBBdRo2mfd7t8Dr/p14Hz74F5OHLoO7CKSA/D3AP4IwFUA\nPiEiV3V7XYqOtdnhxXGvvE4g2vHIUauDe/sbEYDmIjKFI0DLrLv998BUv37O3T64R+KYsV8D4AVV\nfUlVzwD4NoCbY7guRcTa7PDiuFdp+4RkeiMCgGe2X5vs4FJCAHxy3WhLgA67gzctB20UAbzm+vOP\nG49RQlibHV4c9yptn5DS9kZkm+JIAQ9sXov7xte0PB7m/+/MHbQhIlsAbAGA0dHRfr3sQHKfnD7I\nlR5hxHGvTAuPtn5CCnojEsRywFAmCcyfaky/BzkRnFNN3UEbZQDLXH++rPFYC1XdC2AvAJRKJf7e\n9Bhrs8Pr9l551Svb/Akp6I2I/zjN/N6sTb8HSWx4iyMV830AV4rIChG5AMBtAB6L4bpEqZC23atB\n6ScuoHrL58T3zdr0ewCg7xVTXc/YVfWsiHwWwEEAOQDfUNVjXY+MKEXS9AnJL/00MV3G2++cTXiE\nyRIAI4vzeOt0FS0n6YX4KNP+e5DUPglRQ0lOL5VKJZ2cnOz76xKRGQ/cqOfDX9x1A9bvPuSZriqO\nFDqqHIrrOg4RmVLVUtDz2ASMiADwwA0AWHfFEgDmBeZOd+cmVTHFwE5EAOwtz+ynV35Wvwd+i6Sd\n5MiT2lPCwE5EAOwtz+wn583Nb5H03sfDLyEmtaeEgZ2IALBfDFBfH3VaLJicOl0Nfb2kKqbY3ZGI\nAJxvM7vvyMnAApCciLEXStq5K1fikETFFGfsRNQU5sCNQj7XXGTMqkq1BjEcKTVSyPd3MBEwsBNR\nU5gF1F23rMF/vfRmH0aTLFUgP9Qa3fNDgp2bVic0ovAY2ImoKWgBNSeCO/fPIKNZmBbFkQL2fOwD\nLfnxPR/7QCo2ojHHTkRN2zauxLaHZlGteUfurObV2zmVK+78uHOE4p37Z6xvrMfATkRN42NF7Hzs\nGOYr4Ss/0iSfE+Oblkg9/VJsBG2gXiHz+nwFI4vz+MUvz6J6rv6zth+hyFQMEbV4K6NBfaSQx56P\nfgA5w6ropRcX8MruG5tb/d2HkZw6XW0GdYfNPewZ2ImoRRY3KhXyOezctBrjY0WcM6STnIXjiemy\n8bxS08/YhoGdiFp4bVTKDwnyOUP9n+VGCvmWTUEXG8oVLx0pNBuhhV1LsPVNkDl2ImphausLAHfs\nn0lyaB0peixwTkyX8faZhW2J80P1XuudNEKz+TAVztiJKJTxsWJqDuFw2uK2L2zuOXjCc/H0XRcO\nY3ys6JtayecEI4V8Kg5T4YydiFr4HQ7hdfxbPieAYsHiYlL8ZtKmwD3f6P/id27pno+mo4Yd4Iyd\niNp4pSMq1Rp2Pnas+T2nsqQ4UsBFFwxbE9RzIr4z6aA2uqZujF/5eHqCOsDATkRtjLPaSrU5m62p\nNmfGfjXvptLCXggTgIPa6Kbt/FoTpmKIqIUpHdGuUq3hTp/FVEH/dqp6LZR68Tvv1f0cr+s4O09N\nP2cTBnYiauGVRzfxC9v9COkjhTxm7rmuo5+J0kY3qUOpo2IqhohaeKUjliyOr1VtnMmZfu2SNa07\n2LrzlDN2IlqgfVY7MV32bQ7WznQQhyDemXy/NggldSh1VJyxE1E4ISOyAFh3xRLPY/biTs/0a4NQ\nUodSR8XATkSB9hw8EbqkUQH84ORbuPXqYqwpnHZLFuf7lt9O6lDqqJiKIaJAnaYcKtUannjuDSy+\nYLijw5/DKuRzuOfD/TvJKEw1jU0Y2IkoUNgSSLdTp6uBQT1Kzn1I0FJb3l6GuGHVUhw+Phd7AE7i\nUOqoRBM4EaVUKunk5GTfX5eIgnnVawMIXQLpZlpEBerVNhtWLcWDR06Gvl4+J9j8wWXNwH1xIY+3\nz5z1XdQt5HOp3GTkRUSmVLUU9Dzm2ImoyanXdg6YcNdrOyWQAFpaCty+btR4PWeHqpsAuH3dKJ7Z\nfi3uG1+DRcPhw9DmDy7Dw1Pl5vjmK9XASh2byxJ7hakYImryq9f26pYI1N8MTLNuZ1a+78jJZspF\nATw8VUbp8vcCAM6FXJQtjhRw+Phcx58aAHvLEnuFgZ2ImqLUa5tmwwI0e5y3h273LDpMtY1TgeLX\nwsBPe1limtoDRMFUDBE1dVqvPTFdNi6qKuoLjqbvl+crvm8YXo24otSNt5clmtJNE9Pljq9tKwZ2\nImrqpF7bCZAm7fn4djkRY6B2ZvsvNw6XdmbT2zau9G1J4OT8/bozpq09QBRMxRBRUyf12n7HyLnf\nDExVMTXVZnql/RkKYOuBWdy5f6ZlDONjRePxfALgme3XBv4d09YeIAoGdiJqEbZe2y8QumfJRUMN\nfHGk4BuonTeE8nwF2x6abY7NdL2waRpTTb6t7QGi6CoVIyJ7ROS4iDwnIo+KyEhcAyMiu5kCoROw\nHUHpnTDnqFZrinsfPxbqekA9TbR+9yGs2P4E1u8+1JI/T1t7gCi6zbE/BeC3VPW3AfwPgB3dD4mI\n0iBsgAw6lcjrOl6cXaxB1wtaHM3KKUl+Ytt5KiIfAfBRVf1k0HO585QoG+IqG3Rfxy8ivbL7xsBr\nrd99yJj6CZODt1nYnadx5tg/DWC/z4C2ANgCAKOj5p1qRJQecfVPcV9n7b1Pep6jOlII1ylyEBZH\ngwQGdhF5GsD7Pb51l6p+t/GcuwCcBbDPdB1V3QtgL1CfsUcaLRElJo7ZeZhr7Ny0Gtu+M9uycSk/\nJNi5KVw3x0FYHA0SGNhV9UN+3xeRTwG4CcAfahIdxYio56Kc+enVdfHhqXLgNbptket1ZmvWFkeD\ndJVjF5HrAdwP4A9UdS7szzHHTpQuneat298IAHOL3l7kvrPaMqBfOfavAVgE4Cmp7y47oqqf6fKa\nRGSZTvPWXpuXTFPIMLnvTgN1mnqn90JXgV1Vfz2ugRCRvTrNW3eyUBmU+46SBhp07BVDRIE63dTj\n1wMm7DUcg9DbJW5sKUBEgTpd0DQtYN56dTHw2Lr2tIupO+QglS92ioGdiELpJG/t90Zw98RRfOvZ\n11Cer2DrgVlMvvom7htfA8A77WJadB2k8sVOMbATUU94vRHcPXG05bSlmmrzz/eNrzEuurYH90Er\nX+wUAzsR9YRXJcu3nn3N87nfevY13De+xpheUdTLIrNWvtgrDOxEFDtTJYtfb3bAXH3jHLzBYB4O\nq2KIKHamShYT55Ql0wlJCvPZqrQQAzsRxa7TipVP/M4yAPW8fLcbmUx92AcJAzsRxc7vEI7b1402\nZ+g5Edy+brRZFeM8p5NrOgbhkOqwmGMnotj5NeIaHyu2BPJOftaP30amQcvNM7ATUey66dAY9WfZ\nh/08BnYi6oluGnFF+Vn2YT+POXYiyoRBOKQ6LM7YiSgTuj2gI0sY2IkoMwa9D7uDqRgiooxhYCci\nyhgGdiKijGFgJyLKGAZ2IqKMYVUMEaWSV793VsTUMbATUeqY+r0DYHAHUzFElEJ+Db+IgZ2IUogN\nv/wxsBNR6pgaew1iwy8vDOxElDps+OWPi6dElDps+OWPgZ2IUokNv8yYiiEiyhgGdiKijGFgJyLK\nGAZ2IqKMYWAnIsqYWAK7iGwVERWRS+K4HhERRdd1YBeRZQCuA3Cy++EQEVG34pixPwDg8wA0hmsR\nEVGXugrsInIzgLKqzsY0HiIi6lLgzlMReRrA+z2+dReAL6CehgkkIlsAbAGA0dHRDoZIRESdENVo\nGRQRWQPg3wGcbjx0GYDXAVyjqv/r97OlUkknJycjvS4R0aASkSlVLQU9L3KvGFU9CuB9rhd8BUBJ\nVX8a9ZpElC08vi4ZbAJGRD3B4+uSE9sGJVVdztk6ETl4fF1yuPOUiHqCx9clh4GdiHqCx9clh4Gd\niHqCx9clh4unRNQTPL4uOQzsRNQzPL4uGUzFEBFlDAM7EVHGMLATEWUMAzsRUcYwsBMRZUzk7o5d\nvajIHIBXe/gSlwCwtb2BzWMD7B6fzWMD7B6fzWMD7B6fTWO7XFWXBj0pkcDeayIyGaa1ZRJsHhtg\n9/hsHhtg9/hsHhtg9/hsHpsJUzFERBnDwE5ElDFZDex7kx6AD5vHBtg9PpvHBtg9PpvHBtg9PpvH\n5imTOXYiokGW1Rk7EdHAylRgF5EvichzIjIrIodEZNT1vR0i8oKInBCRjQmMbY+IHG+M71ERGWk8\nvlxEKiIy0/jvH20ZW+N7id63xhg+JiLHROSciJRcj9tw7zzH1vhe4veubTw7RaTsul83WDCm6xv3\n5wUR2Z70eNqJyCsicrRxvyaTHk9oqpqZ/wC8x/X1XwL4euPrqwDMAlgEYAWAFwHk+jy26wAMN77+\nMoAvN75eDuCHCd8309gSv2+NcfwmgJUAvof6genO4zbcO9PYrLh3bWPdCeCvkxxD23hyjftyBYAL\nGvfrqqTH1TbGVwBckvQ4Ov0vUzN2Vf25648XAfhZ4+ubAXxbVd9R1ZcBvADgmj6P7UlVPdv44xEA\nl/Xz9f34jC3x+9YY3/OqauVBmT5js+LeWe4aAC+o6kuqegbAt1G/b9SlTAV2ABCRvxGR1wD8CYBd\njYeLAF5zPe3HjceS8mkA/+r684rGR73/EJHfS2pQDe6x2XbfvNh079xsvXd/0Ui5fUNEliQ8Flvv\nkZsCeFpEpkRkS9KDCSt1B22IyNMA3u/xrbtU9buqeheAu0RkB4AHAHzKlrE1nnMXgLMA9jW+9waA\nUVX9mYhcDWBCRFa3ffpIamx9E2Z8Hqy5d7bwGyuAfwDwJdSD1ZcAfAX1N3Iy+11VLYvI+wA8JSLH\nVfU/kx5UkNQFdlX9UMin7sP5mWcZwDLX9y5rPBaroLGJyKcA3ATgD7WRwFPVdwC80/h6SkReBPAb\nAGJdqIkyNvTpvoUZn+FnrLh3Bn27d25hxyoi/wTgX3o8nCCJ3KNOqGq58b8/EZFHUU8fWR/YM5WK\nEZErXX+8GcBM4+vHANwmIotEZAWAKwH8d5/Hdj2AzwPYpKqnXY8vFZFc4+srGmN7yYaxwYL75seG\ne+fDunsnIr/m+uNHAPwwqbE0fB/AlSKyQkQuAHAb6vfNCiJykYi82/ka9SKDpO9ZKKmbsQfYLSIr\nAdRQ/wf+ZwCgqsdE5ACAH6GeavhzVa31eWxfQ71C4ikRAYAjqvoZAL8P4IsiUgVwDsBnVPVNG8Zm\nyX2DiHwEwN8BWArgCRGZUdWNsODemcZmy71r87cishb1VMwrAP40ycGo6lkR+SyAg6hXyHxDVY8l\nOaY2vwrg0ca/iWEA31TVf0t2SOFw5ykRUcZkKhVDREQM7EREmcPATkSUMQzsREQZw8BORJQxDOxE\nRBnDwE5ElDEM7EREGfP/lhh12wsH2zMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2aeb122ec2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
